<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

# Method {#method}






## Study Design






## Data Simulation Procedure

All following analyses are based on mathematically simulated data sets that were generated for a previous study by @Schuster.2020. A detailed description of the simulation process can be found in the supplementary material of their article online ^[\url{https://doi.org/10.1016/j.invent.2020.100313}].\par

<!--
Estimated parameters and the simulation process will be described in the following sections.
-->

Data were simulated on the basis of an empirical clinical trial conducted with a 



<!--
// The Patient Health Questionnaire-9 [PHQ-9, @Kroenke.2001] was used to evaluate changes in the degree of depressive symptoms in an empirical study. All 9 items are scored on a 4-point Likert scale (0-3), resulting in a total score of 0-27, with higher scores indicating more severe depression. 

validation.
-->


### Sample



clinical sample undergoing treatment for depression


### Simulated Scenarios




Data sets for both diagnostic methods showed an overall effect size of Cohen´s _d_ between 0.88 (EMA) and 0.92 (questionnaire) for the symptom change from pre- to post timepoints. Their overall _treatment effect_ would therefore be considered large [@Cohen.2013], lying within the range of real empiric effect sizes reported in research on psychotherapy outcomes. For instance, a large meta-analysis of 115 studies conducted by @Cuijpers.2010 on the effectiveness of psychotherapy resulted in a mean effect size of Cohen´s _d_ = 0.68.
\par


\begin{table}[htb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Structure of the Questionnaire-Like Data Set}}
  \label{tab:quest-str}
  \begin{tabular}{@{}cc@{}}
  \toprule
  Variable & Description\\ \midrule
  ID & participant ID\\
  PRE1\_1 & assessment \#1 before treatment\\
  PRE1\_2 & assessment \#2 before treatment\\
  PRE1\_3 & assessment \#3 before treatment\\
  PRE1\_4 & assessment \#4 before treatment\\
  PRE1\_5 & assessment \#5 before treatment\\
  POST1\_1 & assessment \#1 after treatment\\
  POST1\_2 & assessment \#2 after treatment\\
  POST1\_3 & assessment \#3 after treatment\\
  POST1\_4 & assessment \#4 after treatment\\
  POST1\_5 & assessment \#5 after treatment\\
  PRE\_Mean & mean score of pre assessments\\
  POST\_Mean & mean score of post assessments\\
  ind.pretestSD & standard deviation of pre assessments\\
  ind.posttestSD & standard deviation of post assessments\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}


#### Questionnaire-Like Data

PHQ-9 [@Kroenke.2001]



Frequency of assessments





#### EMA-Like Data



Frequency of assessments







## Data Pre-Processing




### Extension of Individual Assessments

#### K-Nearest-Neighbor Search

In order to investigate the sensitivity and specificity of estimates obtained through single and short-interval assessment formats in comparison to each subject´s respective _true symptom levels_ -- defined by the score changes in their underlying structure of daily assessments -- it was necessary to extend the originally simulated assessment intervals. As both the questionnaire and EMA scenarios were first modeled for 5-fold intervals, they were extended for further analyses to obtain 30-fold pre- and post assessment intervals. This was achieved with the following approach (R code is provided in Appendix \@ref(r-knn-search)). \par

In both simulated data sets comprising _N_ = 100.000 participants each, subjects with equal interval means and standard deviations were matched using a k-nearest-neighbor (KNN) search algorithm. In particular, this was done using the k-dimensional tree algorithm within the function `get.knn()` from the R package _FNN_ [@Beygelzimer.2019]. This KNN-search method compares all cases to one another on one or more dimensions of interest by computing the Euclidian distances between them. \par

For instance, to compare two participants $p = (p_{1},p_{2})$ and $q = (q_{1},q_{2})$ regarding the symptom severity and variability within their baseline assessments, with $p_{1}$ and $q_{1}$ denoting the mean scores and $p_{2}$ and $q_{2}$ denoting the standard deviations of their respective baseline intervals, the Euclidian distance _d_ between them is given by Equation \@ref(eq:eucl-dist):

\begin{equation}
d(p,q) = \sqrt{(q_{1} - p_{1})^2 + (q_{2} - p_{2})^2} (\#eq:eucl-dist)
\end{equation}

Cases were matched separately by pre- and post-treatment intervals to ensure an appropriate balance between (1) within-interval similarity and (2) individual between-interval changes:

1. within-interval similarity between matched cases: cases need to have a similar mean score and fluctuation within the respective interval
2. individual between-interval changes: matched cases do not need to have similar score changes between their pre- and their post-treatment intervals

\par
By specifying the KNN-search function with _k_ = 5, it calculates the similarity of all cases to each other and matches each case with its 5 nearest neighbors, resulting in lists of 6 matched case IDs for each specific (observed) combination of interval means and standard deviations. In this way, cases with similar average symptom scores and similar _pre-_ and _post_ standard deviations were matched inside each data set (questionnaire and EMA). Thereby, only participants with both similar average score changes from _pre_ to _post_ and similar intra-individual variability were matched together. 


#### Generation of 30-fold Individual Assessment Intervals

The individual assessment intervals of these similar cases were then concatenated after one another in order to extend the number of simulated assessments from 5-fold to 30-fold intervals for each participant. In detail, for each combination of 6 perfect neighbors regarding the pre-treatment interval, the IDs of these neighbors were used to bind their 5-fold pre-treatment intervals together to obtain a table of cases with 30-fold pre-treatment intervals. Within this data set of matched pre-case IDs, cases were first sorted within each set of 6 matched IDs (i.e., within rows), then sorted by rows (i.e., by the lowest ID in each row), and then filtered to contain only unique combinations of matched case IDs. This was also done for all cases that were matched by their post-treatment intervals. \par

Finally, pre- and post-KNN lists were joined by the first, and therefore lowest, ID in each case row. Hence, the number of cases was further filtered to comprise only cases which contained both 6-fold pre- and 6-fold post-case IDs, i.e. only cases with 6 pre-nearest neighbors and 6 post-nearest neighbors, which could be linked together by their lowest ID. For instance, if the KNN search had found 6 cases that perfectly matched on their pre-treatment intervals, $IDs_{pre}$ = \{1; 63; 411; 482; 1072; 4315\}, and 6 cases that perfectly matched on their post-treatment intervals, $IDs_{post}$ = \{1; 28; 369; 472; 983; 2365\}, then these pre- and post-IDs would be joined together by the lowest ID that they had in common (i.e., 1): $IDs$ = \{\{1; 63; 411; 482; 1072; 4315\};\{1; 28; 369; 472; 983; 2365\}\}. Using this KNN-search information, the final 30-fold assessment intervals were created by concatenating the assessments of matched cases from the originally simulated questionnaire- and EMA-like data sets. Both the questionnaire-like and EMA-like samples were reduced by the extension process by about 92 %, resulting in sample sizes of _N_ = 8.240 (questionnaire) and _N_ = 8.087 (EMA). R code for this procedure is provided in the Appendices \@ref(r-knn-search) and \@ref(r-extension). \par

It should be noted that this strategy to extend assessment intervals, i.e. by stringing together 5-fold intervals from multiple different cases, was only considered appropriate because the originally simulated data presented no signs of autoregressive effects within individual intervals, i.e. neither systematic longitudinal effects between consecutive assessments (i.e., overall improvement or deterioration of symptoms within an interval) nor systematic variability (i.e., regression towards the mean or regression towards the tail). These assumptions can be confirmed, for instance, from the correlation matrix given in Section \@ref(reliability).


### Random Sampling of Assessments from the Intense-Assessment Intervals

In order to realistically simulate drawing arbitrary 5-fold (EMA-like) samples of assessments from each subject´s 30-fold intervals, the following approach was taken exclusively within the EMA data set (see the R code in Appendix \@ref(r-random-sampling)). \par

For each subject individually, 5-fold _windows_ of pre-treatment and post-treatment assessments were randomly drawn from their respective 30-fold intervals in order to create the scenario $EMA_{5.5-Window}$. This scenario simulates a study design in which participants are monitored via EMA on 5 consecutive days before and after receiving a treatment. Furthermore, for each subject individually, 5 single pre-treatment and post-treatment assessments were randomly drawn from their respective 30-fold intervals in order to create the scenario $EMA_{5.5-Days}$. This scenario simulates a study design in which participants are monitored via EMA on 5 arbitrary and not necessarily consecutive days before and after receiving a treatment.


### Exclusion of Cases Without Variance

Cases with no symptom variability (i.e. with constant scores) throughout one or both of their assessment intervals were excluded from all analyses. This criterion for exclusion was formulated because it was deemed improbable for participants to show no fluctuation in PHQ-9 scores over 5-fold, or even more improbable, over 30-fold assessments. Including these cases would also have affected the outcomes of all clinical change methods which incorporate individual standard deviations as estimates of within-subject fluctuations, e.g., by yielding infinite values (see Section \@ref(class-methods) below). _n_ = 14 cases were excluded from questionnaire scenarios and _n_ = 47 cases were excluded from EMA scenarios, resulting in final samples comprising _N_ = 8.226 participants with questionnaire assessments and _N_ = 8.040 participants with EMA assessments.


### Comparability of the Data Sets









## Reliability and Intra-Individual Autocorrelation {#reliability}



@Titov.2011 found an internal consistency of PHQ-9 scores of $\alpha$ = .74 (pre treatment) and $\alpha$ = .81 (post treatment).



### Paper-Pencil Scenario

#### Intense 5-fold Assessment Intervals

The correlation matrix of pre- and post assessments is shown in Table \@ref(tab:cor-fb5.5). The Fisher-z transformed average correlation coefficient of subsequent assessments was equal for pre- and post intervals, _r_ = .68. The average internal consistency Cronbach´s $\alpha$ between both pre- and post intervals was $\alpha$ = .84.



#### Intense 30-fold Assessment Intervals


#### Standard Single Pre-Post Assessments




### EMA Scenario




## Planned Statistical Analyses











## Clinical Interpretation of PHQ-9 Scores

[@Karin.2018b; @Kroenke.2001]
\par

The clinical interpretation categories of the PHQ-9 and their classification frequencies among both data sets are shown below in Table \@ref(tab:phq-int).

\begin{table}[htb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Clinical Interpretation of PHQ-9 Scores}}
  \label{tab:phq-int}
  \begin{tabular}{@{}ccc@{}}
  \toprule
  PHQ-9 Score & Classification & Interpretation\\ \midrule
  0-4 & 0 & Minimal or none\\
  5-9 & 1 & Mild\\
  10-14 & 2 & Moderate\\
  15-19 & 3 & Moderately severe\\
  20-27 & 4 & Severe\\
  \bottomrule
  \end{tabular}
  \end{threeparttable}
\end{table}





## Classification Methods {#class-methods}


Widespread classification methods will be explored regarding their convergent and divergent validity.



### Percentage Change



A detailed description of the Percentage Change method _PC_ (also known as Percentage Improvement, _PI_) is given in @Karin.2018.




\begin{equation}
PC = \Bigl(1 - \frac{\overline{x_{2}}} {\overline{x_{1}}}\Bigr) \cdot 100 (\#eq:pc)
\end{equation}

> $\overline{x_{2}}$ = mean of subject´s posttest scores, $\overline{x_{1}}$ = mean of subject´s pretest scores

\par

The _Percentage Change_ method results in an index that describes a subject´s post-treatment score as a proportion of his or her pre-treatment score. A positive result indicates that the post-treatment score is smaller than the pre-treatment score (i.e., improvement), while a negative result indicates a post-treatment score higher than the pre-treatment score (i.e., deterioration). When applied on a common scoring system for a psychological construct, i.e. including only non-negative scores, the resulting index can assume values smaller than or equal to 100. This is a consequence of the fact that a person can not reduce his or her scores by more than 100 %, as the lower bound of the scale itself is non-negative (most typically 0). But depending on the specific scale, it may well be possible that a subject can increase scores from pre- to post-treatment by more than 100 %, indicated by a post-treatment score greater than two times the size of the pre-treatment score. Hence, the negative limit of the index (i.e., the most extreme expression of deterioration) is not defined a priori, but rather scale- and data-specific, as it is determined by the maximum of the empirical distribution of pre-treatment scores in relation to the highest achievable score on the scale.
\par


The PC method is a _proportional_, and therefore individualized, approach to interpreting longitudinal change. This means that it does not assume score changes to be _linear_ in the population, but rather makes no assumptions about the nature of individual change. 



The assumption of _linear_ change, on the other hand, is inherent in all methods which include a fixed score difference that has to be achieved by participants in order to be regarded as meaningfully improved or deteriorated. 

This approach treats all individuals equally in that it does not take into account the individual symptom severity expressed at their baseline assessment. 

Subjects with a low symptom severity at baseline may not be able to show a score reduction $\geq$ the pre-defined meaningful difference, and hence could not be regarded as meaningfully improved, while subjects with a high symptom severity at baseline could pass the required score improvement and be regarded as meaningfully improved, even though their post-treatment score could still be within the clinical range of scores. 

The main advantage of this approach is _..._

\par
On the other hand, if a method inherently assumes _proportional_ change, it defines the absolute score difference to be regarded as meaningfully changed proportionally, in order to account for the influence of baseline severity. By setting proportional differences as cutoff criteria for classification categories, observed changes are evaluated individually in relation to baseline severity.

The main advantage of this approach is _..._

[see @Karin.2018]
\par


\begin{table}[htb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Percentage Change Interpretation of PHQ-9 Scores}}
  \label{tab:pc-int}
  \begin{tabular}{@{}cccc@{}}
  \toprule
  PC Criteria & Class. & Interpretation & Conventional Interpretation\\ \midrule
  PC $\leq$ -50 & -2 & Strong Deterioration & Deterioration\\
  -50 < PC $\leq$ -25 & -1 & Deterioration & No Change\\
  -25 < PC < 25 & 0 & No Change & No Change\\
  25 $\leq$ PC < 50 & 1 & Improvement & No Change\\
  PC $\geq$ 50 & 2 & Strong Improvement & Improvement\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}

An overview of the interpretation categories for the Percentage Change method is displayed in Table \@ref(tab:pc-int) above.



### Clinically Significant Improvement, Clinically Significant Change



<!--
> ``The original validation study of the PHQ-9 defined clinically significant improvement as a post-treatment score of <= 9 combined with improvement of 50%.´´ (McMillan, Gilbody, & Richards, 2010)
-->


\begin{table}[htb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Clinically Significant Change Interpretation of PHQ-9 Scores}}
  \label{tab:csi-int}
  \begin{tabular}{@{}ccc@{}}
  \toprule
  Clinically Significant Change Criteria & Classification & Interpretation\\ \midrule
  PHQ-9 Post-Score $\leq$ 9 \& PC $\geq$ 50 & -1 & Significant Improvement\\
  PHQ-9 Post-Score > 9 \& -50 < PC < 50 & 0 & No Significant Change\\
  PHQ-9 Post-Score > 9 \& PC $\leq$ -50 & 1 & Significant Deterioration\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}




### Reliable Change Index


The _Reliable Change Index (RCI)_ was first introduced by @Jacobson.1984 and @Jacobson.1991. It is defined as a	standardized difference score that determines whether a score difference is statistically significant, i.e. exceeds the error variance of the assessment method. Hence, it determines if the observed score difference can be attributed to treatment effects rather than to naturally occurring variance in the sample.
\par


<!--
-	When true changes are expected (e.g., as practice effects in neuropsychological tests), regression-based approaches can be used to correct obtained scores.  
-	Many normative studies in the literature that report RCIs and cutoff scores for significant change in specific tests and scales.  
-	``For instance, @Geiser.2000 concluded from their study on psychosomatic patients that RCIs were useful when monitoring the global scale of the Symptom-Checklist-90-R (SCL-90-R), but that it was also important to investigate specific cutoff scores for different groups of diagnoses. Furthermore, @Ekeroth.2014 investigated the concordance of CSIs/RCIs and diagnostic change (as assessed with the DSM-IV) in patients with eating disorders, and found that the calculated indices explained more variance in the measured psychopathological change than the diagnostic change according to DSM-IV.´´
-->
\par


#### Reliable Change [@Jacobson.1984; @Jacobson.1991]


Contemplating the sole reliance on statistical significance of tests, @Jacobson.1991 critized widespread research approaches for the following problems: (1) comparisons on a group level ignore intra-individual variability and change, and (2) significant group differences are not synonymous with clinical relevance.
\par
They 
\par
The RC Index is a standardised measure of the raw score difference between 2 assessments. It quantifies the extent by which the score difference exceeds the error variance of the assessment method. A significant RCI therefore indicates that the observed change exceeds the measurement error by an extent upon which it can be confidently assumed that it is not caused by error variance, but rather by other factors, such as an applied clinical treatment. The conventionally applied significance cutoff is $|RCI|>1.96$, given by the z score for 95 % confidence, i.e. a two-sided $\alpha$ probability <.05. \par



\begin{equation}
RCI = \frac{x_{2} - x_{1}}  {s_{diff}} (\#eq:rci-jt)
\end{equation}

\begin{equation}
s_{diff} = \sqrt{2 \cdot (S_{E})^2} (\#eq:rci-jt-sdiff)
\end{equation}

\begin{equation}
SE = s_{1} \cdot \sqrt{1 - r_{xx \text{´}}} (\#eq:rci-jt-se)
\end{equation}

> $x_{2}$ = subject´s posttest score, $x_{1}$ = subject´s pretest score, $s_{diff}$ = standard error of difference between test scores, $SE$ = standard error of measurement, $s_{1}$ = standard deviation of test scores at pretest, $r_{xx \text{´}}$ = reliability of the measure

\par

For instance, a significant RC index of $\pm 2$ would show that the score difference was equal to two standard deviations which were weighted by the reliability of the method.


\par

Furthermore, @Jacobson.1984 and @Jacobson.1991 offer an additional formula for the calculation of a significance cutoff in raw scores, given by the following formula.


\begin{equation}
\textit{significance cutoff} = 1.96 \cdot s_{diff} = 1.96 \cdot \sqrt{2 \cdot (s_{1} \cdot \sqrt{1 - r_{xx \text{´}}})^2} (\#eq:rci-jt-cut)
\end{equation}

> $\textit{significance cutoff}$ = (absolute) cutoff score for reliable change (95%-criterion)

\par

This formula defines the raw score that an individual would have to gain or lose in the respective scale to be recognized as reliably changed. It is also based on the whole sample´s characteristics.

\par
These estimates should be calculated using the standard deviation of either a control group, a normal population, or an experimental group at the baseline assessment. It also includes the test-retest reliability, usually obtained from a non-clinical sample, which is oftentimes available in the test manual or in published validation studies.

\par
Following from the assumption of normally distributed change scores, an individual RCI score could also be interpreted in the sense of percentage ranks, i.e. 
assuming normality, it is expected that _X_ % of participants getting the same treatment under the same conditions, would show an improvement/deterioration of at most the same extent.
\par

<!--
//According to @HintonBayre.2000, the RCIJT is appropriate when only pretest data and the test reliability are available and a true change in the construct, independently of treatment effects, is not expected. If normative retest data are available, he argues for the inclusion of the posttest variance. The absence of true change in the construct is a critical precondition because in many assessment contexts there are practice effects, regression toward the mean or divergence from the mean, and natural fluctuations in the construct. If these changes are expected independently of an intervention, they need to be taken into account as error variance [@Busch.2015].
-->


#### Defining an Individualized Reliable Change Index


The RCI(ind) is proposed as a mathematical adaptation of the originally defined RCI to repeated-measurement data including more that two timepoints, such as data from EMA procedures.
\par


\par
Adaptation Step from RCI(JT) to RCI(ind): The numerator in the formula is replaced by the mean interval difference (5xPre - 5xPost):


\begin{equation}
RCI_{Step} = \frac{\overline{x_{2}} - \overline{x_{1}}}  {s_{diff}} (\#eq:rci-step)
\end{equation}

> $\overline{x_{2}}$ = mean of subject´s posttest scores, $\overline{x_{1}}$ = mean of subject´s pretest scores

\par

RCI(ind) using the SD from the individual pre-interval


\begin{equation}
RCI_{\textit{ind, pre SD}} = \frac{\overline{x_{2}} - \overline{x_{1}}}  {SE_{D,pre}} (\#eq:rci-ind-presd)
\end{equation}

\begin{equation}
SE_{D,pre} = \sqrt{2 \cdot (s_{x} \cdot (1 - r_{xy})^2)} (\#eq:rci-ind-presd-se)
\end{equation}

\begin{equation}
\textit{significance cutoff} = 1.96 \cdot SE_{D,pre} = 1.96 \cdot \sqrt{2 \cdot (s_{x} \cdot (1 - r_{xy})^2)} (\#eq:rci-ind-presd-cut)
\end{equation}

> $\overline{x_{2}}$ = mean of subject´s posttest scores, $\overline{x_{1}}$ = mean of subject´s pretest scores, $SE_{D,pre}$ = standard error of difference between the test scores in the individual´s pre interval, $s_{x}$ = individual standard deviation of pretest time points, $r_{xy}$ = reliability (internal consistency Cronbach´s $\alpha$) of the measure, $\textit{significance cutoff}$ = (absolute) cutoff score for reliable change (95%-criterion)

\par


RCI(ind) using pooled SDs from both individual intervals


\begin{equation}
RCI_{\textit{ind, pooled SD}} = \frac{\overline{x_{2}} - \overline{x_{1}}}  {SE_{D}} (\#eq:rci-ind-pooledsd)
\end{equation}

\begin{equation}
SE_{D} = \sqrt{(s_{x}^2 + s_{y}^2) \cdot (1 - r_{xy})} (\#eq:rci-ind-pooledsd-se)
\end{equation}

\begin{equation}
\textit{significance cutoff} = 1.96 \cdot SE_{D} = 1.96 \cdot \sqrt{(s_{x}^2 + s_{y}^2) \cdot (1 - r_{xy})} (\#eq:rci-ind-pooledsd-cut)
\end{equation}

> $\overline{x_{2}}$ = mean of subject´s posttest scores, $\overline{x_{1}}$ = mean of subject´s pretest scores, $SE_{D}$ = pooled standard error of difference between the test scores, $s_{x}$ = individual standard deviation of pretest time points, $s_{y}$ = individual standard deviation of pretest time points, $r_{xy}$ = reliability (internal consistency Cronbach´s $\alpha$) of the measure, $\textit{significance cutoff}$ = (absolute) cutoff score for reliable change (95%-criterion)

\par





<!--
//rather simple adaptation of the formula with a few changes, but very different outcome, because now it´s not a group estimate anymore, but a standardized estimate for reliable change of a single person (-> individualized RCI)  
  //test scores aggregated into means for 2 assessment intervals (baseline and follow-up), so for the formula itself the number of timepoints in each interval doesn´t matter and the intervals don´t have to have equal numbers of single assessments  
//number of included timepoints, sampling frequency, sampling randomness and start- and end-points for assessment intervals would have to be decided construct- and context-specifically for each group of diagnoses and each study (just like it is now)  
  //mean difference standardized by dividing it by standard error of difference SED  
//here pooled SED, including the SDs of both intervals, but depending on results of following analyses, maybe I will only include the pretest SD of each participant, or instead the experimental or the control group´s SD in the denominator  
  //by including the individual standard deviation of a single subject instead of variability measures on the group or population level, the individual´s error term is neither understated nor inflated by the test scores of other people than the person of interest. Nevertheless, the variability of responses of a sample or population to the given test is also part of the reliability of the test ($r_{xy}$), which is included in the error estimate, too.  
  //Whereas in the most popular RCI formulas, rtt is used, in contexts where instable psychological constructs are measured over time, the internal consistency (Cronbach´s $\alpha$) is more appropriate for the following reasons: As @Cronbach.1947 described in his early review of reliability coefficients, the test-retest reliability can only be an accurate estimate of measurement precision if the measured construct is expected to be stable over time. By definition, the unique measurement variance can only be distinguished from the real construct variance over time if the construct in reality does not fluctuate between timepoints [@Maassen.2009; @Wyrwich.2004]. As stability is not expected for the constructs examined in (psycho-)therapy research and practice, but they are rather particularly analysed for changes over time, the test-retest reliability is not considered appropriate for the calculation of a reliable change index.


//RCIind > 1.96 indicates reliable deterioration, whereas RCIind < -1.96 indicates reliable improvement  
//subject´s mean difference of scores between both testing intervals, however large or small it may be in the individual case, is relativized not only by the unreliability (i.e. inconsistency) of the measurement instrument, but also by the subject´s own variability of responses to the instrument. As a consequence, a person with a relatively large mean difference in a measured construct, but also with much variability in single test scores over time, will be assigned a smaller index than another person with the same mean difference and less variability in test scores, because for the latter person there would logically be more confidence in the accuracy of the resulting difference in test scores than for the former one.  
//calculation of cutoff scores for significance of change in a specific test/scale etc.:
-->




### Confidence Interval Method (Edwards-Nunnally Method)


@Edwards.1978
[@Speer.1992]


\begin{equation}
\textit{EN Interval} = \bigl[ r_{xx} (X_{pre} - M_{pre}) + M_{pre} \bigr] \pm 2 \cdot S_{pre} \cdot \sqrt{1 - r_{xx}} (\#eq:en)
\end{equation}

> $r_{xx}$ = reliability of the measure, $X_{pre}$ = individual´s raw score at pre-treatment, $M_{pre}$ = mean of the sample at pre-treatment, $S_{pre}$ = standard deviation of the sample at pre-treatment

\par

Interpretation of PHQ-9 post-scores according to the Edwards-Nunnally-interval method:

\par


\begin{table}[htb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Edwards-Nunnally Method Change Interpretation of PHQ-9 Scores}}
  \label{tab:en-int}
  \begin{tabular}{@{}ccc@{}}
  \toprule
  Edwards-Nunnally Criteria & Classification & Interpretation\\ \midrule
  PHQ POST < [EN Interval] & -1 & Significant Improvement\\
  PHQ POST $\in$ [EN Interval] & 0 & No Significant Change\\
  PHQ POST > [EN Interval] & 1 & Significant Deterioration\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}




<!--
### Sudden Gains and Losses

> Sudden Gain/Loss-Classification between pre- and post intervals [using the R package _{suddengains}_; @Wiedemann.2020]

> 3 criteria for sudden change according to @Tang.1999:
  > significant absolute change: Über-/Unterschreitung eines definierten Cutoff-Scores (z.B. RCI), hier PHQ-9-Score <= 9
  > significant relative change: Reduktion/Zuwachs von Pre zu Post um >= 25%
  > significant change in relation to symptom fluctuation: 

\begin{equation}
M_{pre} - M_{post} > \text{critical value} \cdot \sqrt{\frac{\left(n_{pre} - 1 \right) \cdot SD^2_{pre} + \left(n_{post} - 1 \right) \cdot SD^2_{post}} {n_{pre} + n_{post} - 2}} (\#eq:sg)
\end{equation}

<br>
$M_{pre}$ = mean of the subject´s scores before a potential gain/loss, 
$M_{post}$ = mean of the subject´s scores after a potential gain/loss, 
$\text{critical value}$ = 2.776 = two-tailed t statistic for $\alpha$ = 0.05 and df = 4, 
$n_{pre}$ = number of measurement points before a potential gain/loss, 
$n_{post}$ = number of measurement points after a potential gain/loss, 
$SD^2_{pre}$ = standard deviation of the subject´s scores before a potential gain/loss, 
$SD^2_{post}$ = standard deviation of the subject´s scores after a potential gain/loss
<br>

```{r sg_int, echo=FALSE, results='asis'}
tibble(SG = c("PHQ-9 Post-Mean <= 9 & (Mean) PC >= 25 & Mean Diff. > (absolute) critical fluctuation", 
            "every other combination of conditions", 
            "PHQ-9 Post-Mean > 9 & (Mean) PC <= -25 & Mean Diff. > (absolute) critical fluctuation"),
       Classification = c(-1L,0L,1L),
       Interpretation = c("Sudden Gain", "No Sudden Change", "Sudden Loss")) %>% 
  apa_table(
    col.names = c("Sudden-Gain/Loss Criteria", "Classification", "Interpretation"),
    caption = "Sudden-Gain/Loss Interpretation of PHQ-9 Scores",
    #caption.short = "Sudden-Gain/Loss Interpretation",
    label = "sg-int",
    #longtable = TRUE,
    placement = "htb")
```
-->

<!--
### Personalized Advantage Index (PAI, DeRubeis)
### Hierarchical Linear Models (HLM)
multiwave-data approach using growth-curve modeling sensu @Speer.1992
### Regression-Based Methods
### Gulliksen-Lord-Novick Method [@Hsu.1989; @Hsu.1999]
### Time Series-Based Analyses [e.g., @Holmes.2016]
### Significant Change sensu @Christensen.1986
### RCI(indiv) sensu @Hageman.1993
### Method of 2 SEMs/2 SDs
### ANCOVAs
-->


## Analyses of Statistical Power, Sensitivity, and Specificity

Statistical power is the probability of a specific method to detect an effect, given that it really exists in the population. Hence, it defines the probability of finding _true positive_ results.

Methods for increasing statistical power in clinical trials include, for instance, (1) imputing missing data, (2) repeated assessments, (3) adjusting effects for covariates, or (4) computing linear mixed models (LMM) [@Schuster.2020]. 


<!--
- higher power through inclusion of more assessment occasions (sensu Spearman-Brown formula)
- e.g. @Schuster.2020: power increased by 6-92% from two- to five-fold assessment compared to single assessments
-->





\par \noindent
_Sensitivity_, also known as _recall_ or _true-positive rate_, is the probability of a given method to correctly identify positive cases. In the present study, positive cases are equivalent to true cases of meaningful change, and therefore include both _true_ improvement and deterioration.

\begin{equation}
Sensitivity = \frac{tp}{p} = \frac{tp}{tp + fn} (\#eq:sensitivity)
\end{equation}

\par
_Specificity_, also known as _selectivity_ or _true-negative rate_, is the probability of a given method to correctly identify negative cases. In the present study, positive cases are equivalent to true cases of no meaningful change.

\begin{equation}
Specificity = \frac{tn}{n} = \frac{tn}{tn + fp} (\#eq:specificity)
\end{equation}

\par \noindent

\begin{equation}
\alpha = \frac{fp}{fp + tn} (\#eq:alpha-error)
\end{equation}

\par \noindent

\begin{equation}
\beta = \frac{fn}{fn + tp} (\#eq:beta-error)
\end{equation}

\par \noindent

\begin{equation}
Power = 1 - \beta (\#eq:power)
\end{equation}

\par \noindent
Note that statistical power is equivalent to sensitivity, because:

\begin{equation}
Power = 1 - \beta = 1 - \frac{fn}{fn + tp} = \frac{fn + tp}{fn + tp} - \frac{fn}{fn + tp} = \frac{tp}{fn + tp} = Sensitivity (\#eq:power-sens-equiv)
\end{equation}

\par

<!--
<https://wiki.socr.umich.edu/index.php/SMHS_PowerSensitivitySpecificity>
Note that both (Type I ($\alpha$) and Type II ($\beta$)) errors are proportions in the range [0,1], so they represent error-rates. The reason they are listed in the corresponding cells is that they are directly proportionate to the numerical values of the FP and FN, respectively.
-->


<!--    !!! Classification Evaluation with >2 Classes
//siehe Data Mining/ida(1)/Folie 36-37:
- macro averaging
- class-weighted averaging
-->



## False-Positive Rate and Specificity in a Control Group

False-positive rates and the specificity of clinical change methods were investigated in questionnaire and EMA scenarios with overall within-subjects effect sizes of Cohen´s _d_ $\approx$ 0, representing the scores of a control group in a clinical trial. Although some participants in these simulated scenarios showed a substantial symptom improvement or deterioration, the overall pre-post symptom changes were closely distributed around 0, with the vast majority of cases showing no meaningful changes in absolute scores. The main advantage of using zero-effect data sets for this analysis is that the absence of a general treatment effect, along with equally distributed random positive and negative effects, enables the a-priori assumption that proportions of cases identified as changed should be minimal in the most specific calculation methods. The respective cases would only constitute false-positive classifications (i.e. both classifications of improvement and of deterioration), as the number of truly changed participants would be _p_ = _tp_ + _fn_ = 0, implying that cases of true change could neither be detected (i.e. _tp_ = 0) nor overlooked (i.e. _fn_ = 0) in these scenarios. Following from their definitions, classification sensitivity (see Equation \@ref(eq:sensitivity)) could not be calculated under these conditions, while the classification specificity (see Equation \@ref(eq:specificity)) could be appropriately estimated with regard to the known _ground truth_ of the whole sample consisting of only negative (i.e. non-changed) cases.



The _false-positive rate (FPR)_ is given by:
\begin{equation}
FPR = \frac{fp}{n} = \frac{fp}{fp + tn} (\#eq:fpr)
\end{equation}

\par
Hence, classification methods can be compared regarding their false-positive rates and their specificity (i.e. probability of true-positive classifications) on the basis of this data source.

\par
The resulting 





## Jackknife Resampling of Parameter Estimates

Exemplary R code for this bootstrapped analysis is provided in \@ref(r-jackknife).




