---
author: 'Stephan Bartholdy, B.Sc.'
date: "`r format(Sys.time(), '%B %Y')`"
institution: 'University of Salzburg'
advisor: 'Ao. Univ.--Prof. Dr. Anton--Rupert Laireiter'
altadvisor: 'Dr. Raphael Schuster'
department: 'Fachbereich Psychologie'
#title: 'Optimizing Statistical Power and Precision of Reliable Change in Clinical Trials by Means of Pre--Post EMA'
#title: 'Optimizing Statistical Power and Specificity of Clinically Significant Change by Means of Pre--Post Ecological Momentary Assessment'
#title: 'Estimating potential gains in precision for measures of clinically sigificant change by implementing EMA based on individual within-subjects variance'
#title: 'Optimizing precision in ...'
#   title: 'Clinical Significance in Times of ...'
#   title: 'Clinically Significant Change in Times of ...'
title: 'Reliable Change in Times of Ecological Momentary Assessment: Adaptation and Expectable Increases in Classification Accuracy'
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: 
  thesisdown::thesis_pdf: default
#  thesisdown::thesis_gitbook: default
#  thesisdown::thesis_word: default
#  thesisdown::thesis_epub: default
abstract: |
  `r if(knitr:::is_latex_output()) paste(readLines("00-abstract.Rmd"), collapse = '\n  ')`
zusammenfassung: |
  `r if(knitr:::is_latex_output()) paste(readLines("00-zusammenfassung.Rmd"), collapse = '\n  ')`
# If you'd rather include the preliminary content in files instead of inline
# like below, use a command like that for the abstract above.  Note that a tab is 
# needed on the line after the |.
acknowledgements: |
  I want to thank my supervisor, Ao. Univ.–Prof. Dr. Anton–Rupert Laireiter, for always offering his advice and expertise and allowing me to work as flexibly and as freely as I needed to.
  \par
  I also want to thank my second supervisor, Dr. Raphael Schuster, for his unlimited help and kind encouragement throughout the whole process of writing this thesis. His level of interest in the topic and his constructive feedback were inspiring to me and helped me very much.
  \par
  Apart from the excellent supervision by both of them, I´m thankful to the whole team of authors consisting of Raphael Schuster, Manuela Larissa Schreyer, Tim Kaiser, Thomas Berger, Jan Philipp Klein, Steffen Moritz, Anton-Rupert Laireiter, and Wolfgang Trutschnig for providing me with data which they generated for their own recent study on the statistical power of intense pre-post assessment approaches.
  \par
  On a side note, I want to acknowledge that this thesis was written using the _Salzburgthesisdown_ template ^[\url{https://github.com/irmingard/salzburgthesisdown}] by Veronika Priesner. Based on the _Thesisdown_ package ^[\url{https://github.com/ismayc/thesisdown}] [@Ismay.2020], this format allows for the preparation and formatting of theses using a combination of R code, Markdown and \LaTeX\ syntax.
  \par
  Finally, I want to thank my parents Laura and Frank, as well as my grandparents, for supporting me unconditionally throughout every step of my life and education. I dedicate this thesis, being the most difficult task of my life yet, to you, because without you I would not have been able to fulfill any of the dreams nor achieve any of the goals that I had in my life until today.
#preface: |
#  This is an example of a thesis setup to use the reed thesis document class
#  (for LaTeX) and the R bookdown package, in general.
bibliography: bib/thesis.bib
csl: csl/apa.csl
lot: true
lof: true
#space_between_paragraphs: true
header-includes:
- \usepackage{tikz}
---

<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!
If you'd prefer to not include a Dedication, for example, simply delete lines 17 and 18 above or add a # before them to comment them out.  If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.
-->

<!--
If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r include_packages, include=FALSE, echo=FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis.
#if(!require(devtools))
#  install.packages("devtools", repos = "http://cran.rstudio.com")
#if(!require(thesisdown))
#  devtools::install_github("ismayc/thesisdown")

# just in case:
#tinytex::tlmgr_update()
#update.packages(ask = FALSE, checkBuilt = TRUE)

pacman::p_load(rmarkdown,knitr,thesisdown,papaja,dplyr,tidyverse,haven,foreign,bootstrap,sjmisc,lattice,Rmisc,methods,devtools,psych,DescTools,summarytools,kableExtra,lubridate,timetk,overlapping,ggplot2,gghalves,plot.matrix,FNN,citr,raincloudplots)
#pacman::p_load(qgraph,bootnet,copula,reshape) #data generation

#opts_chunk$set(echo = FALSE, cache=FALSE)
#knitr::read_chunk("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/PP_Stichprobenvergleiche_d0.88.R")
```

<!-- You'll need to include the order that you'd like Rmd files to appear in the _bookdown.yml file for
PDF files and also delete the # before rmd_files: there.  You'll want to not include 00(two-hyphens)prelim.Rmd
and 00-abstract.Rmd since they are handled in the YAML above differently for the PDF version.
-->

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers
on chapters.
-->



<!--chapter:end:index.Rmd-->


# Introduction

The most common paradigm in clinical research is the pre-post design, in which a construct is repeatedly measured over time to evaluate changes in this construct and reach conclusions about the influences that led to the observed changes. The research process then includes statistical tests for the hypothesis, which, in most studies, would regard the efficacy or effectiveness of a therapy or a type of medication. The validity of the resulting inferences strongly depends on the operationalization of the construct, as well as on the methods used to evaluate changes in test scores [@Estrada.2018]. These approaches for evaluating clinically significant changes over time can vary strongly in their definitions and outcomes, and three of them will be explored in detail within this study (i.e. the Reliable Change Index, the Percentage Change method, and an Individualized Reliable Change Index).
\par
The present thesis aims to address two important problems which affect the validity of certain types of empirical findings in psychiatric research: (1) the limited precision of single-point assessment strategies for measuring psychological symptoms, and (2) the lack of assessment methods for clinically significant changes on an individual level.
\par
Starting with a theoretical introduction into both problems, the methodological analyses in this thesis will explore and test possible solutions for increasing the validity of research designs.

<!--chapter:end:01-introduction.Rmd-->


# Theoretical Background

Before diving into the empirical methodology of this thesis, some important theoretical concepts ought to be clarified.

<!------------------------------------------------------------------------------------------------------------->
## Assessment of Treatment Effectiveness on a Group Level vs. Individual Level

Digital mental health and precision medicine practices introduce new ways of data acquisition and analysis into the field of psychiatric research, which have the potential to advance the individualization of psychotherapy. With the rise of digitally assisted interventions in clinical research and practice and the popularization of machine learning algorithms in precision medicine (e.g., treatment outcome prediction models), an emphasis on validity and data quality is necessary to achieve the goals of individualization in person-centered treatments.
\par
In order to evaluate new treatments in psychiatric research, patients are commonly monitored over the course of a therapy. This enables researchers to observe changes in their symptoms over time, aiming to find effects which can be attributed to the applied treatment. Then, the crucial part of the analysis is determining from which perspective the observed changes should be analyzed and interpreted to evaluate potential treatment effects. While most treatment outcome studies report standardized, group-level measures of treatment efficacy, it is often overlooked that individualized measures could give much more valuable insights on within-subjects treatment effects [@Lambert.2013, p. 149].
\par
Regarding the strength of evidence in study designs in general, the \enquote{\textit{gold standard}} of empirical clinical research are randomized-controlled trials (RCTs). They are characterized by the following components: experimental conditions and control conditions, random assignment of participants to conditions, and commonly double blinding (i.e., neither participants nor experimenters know which group each participant is assigned to).
\par
These characteristics provide a necessary basis for researchers to be able to reach conclusions about the effects of an investigated treatment , but there are other methodological problems on a deeper level that need to be considered to ensure sufficiently valid conditions for the type of causal inferences that researchers aim for. Well-studied, but often overlooked problems of commonly applied research designs in RCTs include, i.a., (1) limited precision of the predominantly used single-point assessment strategies for measuring psychological symptoms, and (2) a lack of methods for evaluating clinically significant symptom changes on a truly individualized basis.
\par
1. The first problem is caused by the common practice in clinical studies to assess psychological symptoms, which are often fluctuating in nature, only on single-point occasions, e.g., pre- and post-treatment and on a later follow-up assessment. This practice introduces a certain amount of measurement error and, thus, imprecision [@Fisher.2018; @Pfeiffer.2015; @Schuster.2020].
2. The second problem is concerned with the specific methods used to determine the clinical effectiveness of a treatment. @Estrada.2018 introduced a useful terminology for the distinction between two types of approaches to clinically significant change: \textit{average-based change approaches (ABC)} and \textit{individual-based change approaches (IBC)}. \textit{ABCs} examine clinically relevant changes only on a group-level (i.e. between subjects), whereas \textit{IBCs} directly identify individuals who show meaningful changes (i.e. within subjects). Changes on a group-level are typically measured as average differences between the group´s pre-treatment and post-treatment distributions of test scores, implemented in statistical hypothesis tests, which result in effect sizes (e.g., Cohen´s $d$, Hedge´s $g$) and corresponding significance levels. Individual changes, on the other hand, are examined using a variety of different clinical significance methods, which may include standardized pre-post differences and standard errors or variations of linear regressions [@Anderson.2014; @Estrada.2018; @Ferrer.2014]. An example for an IBC approach is the popular Reliable Change Index [@Jacobson.1984; @Jacobson.1991], which determines individual treatment responses by evaluating within-subjects pre-post differences, while also relying on group-level variance estimates.


\par
The issues mentioned above could be minimized by applying individualized change criteria to ecological momentary assessment data, as part of the increasingly widespread paradigm of digital mental health studies.

<!------------------------------------------------------------------------------------------------------------->
## Ecological Momentary Assessment

Ecological Momentary Assessment (EMA), also known as Ambulatory Assessment [e.g., see @Fahrenberg.2007], Experience Sampling Method (ESM) [e.g., see @Csikszentmihalyi.2014b; @Vork.2019], or Real-Time Data Capture (RTDC) [e.g., see @Stone.2007], is the repeated assessment of self-reported symptoms, behavioral or physiological variables via short scales or questionnaires, commonly presented on mobile devices, in order to measure the construct directly in the subject´s natural environment [@EbnerPriemer.2009]. The method gained popularity as part of the broad concept of Digital Mental Health, which includes methods for the assessment of psychopathology as well as for treatment assistance with digital tools.
\par
EMA is especially suitable for accurately capturing psychological constructs with high intra-individual variability over time (e.g., depression, anxiety, or craving). Despite oftentimes high sampling frequencies, it can be applied efficiently, as it enables highly informative insights from data that is gathered at a minimal cost and effort. Longitudinal EMA designs typically consist of a small number of items that require minimal effort and time for the respondents to answer [@Rot.2012; @Shiffman.2008]. As these short self-reports can be presented in smartphone apps or computer programs, this approach forms a powerful, yet feasible opportunity to study the progression of affective states and behaviors on an individual level.
\par
This diagnostic format could be seen as a bridge between empirical research and clinical practice: Current research in psychiatry has a tendency towards producing group-level based evidence, while the actual treatment of patients is much more concerned with their individual psychopathology. Any practical work with them is in itself a study of processes within each person. EMA formats can benefit both fields by facilitating a deeper understanding of complex psychological processes. For instance, EMA is applied in psychiatric research and therapy, e.g., to capture mood instability in bipolar disorders [e.g., @Holmes.2016] or fluctuating symptoms of depression [e.g., @Armey.2015; @Silk.2011]. Through this form of repeated measurement, it is possible to assess relevant information at random or non-random times of the day or week (e.g., directly after panic attacks in patients with panic disorders, or every morning in depressive patients), while always embedded in the participant´s normal environment and everyday life, instead of in a laboratory, a clinic, or a counseling center. It therefore has the inherent advantage of eliminating lab-specific response tendencies, which is certainly also coupled with the disadvantages of introducing other, environment-specific sources of bias, and possibly the risk of a lower response rate than usually obtained in settings with personally given instructions.
\par
Assessing symptoms over time through EMA may capture more accurately each individual´s treatment responses, but to be able to interpret the course of multiple individual observations, it is important to consider that symptom trajectories show higher variability when captured with EMA than with classical questionnaire assessments. The presence of individual fluctuation between single assessments raises the essential question how to interpret these score changes, and furthermore, what to conclude about the effectiveness of the given treatment for a given individual on the basis of this data. Several questions need to be answered in the process of developing an analytic strategy.

<!------------------------------------------------------------------------------------------------------------->
## Methods for the Classification of Meaningful Change in Clinical Research

Psychiatric research and practice in any context that involves repeated testing of subjects regarding some measurable construct can benefit from being able to determine if a specific change in test scores over time could be attributed to measurement error alone or if it exceeds this error interval significantly and could therefore be attributed to another influence, e.g., an intervention. The application of this idea and other highly relevant clinical change methods will be explained in detail in Chapter \@ref(method).
\par
The broad term clinical significance comprises a variety of different approaches to assess how practically meaningful a change in symptom severity is. Quoting @Bauer.2004b, \enquote{clinically significant change refers to meaningful change in individual patient functioning during psychosocial or medical interventions}. There are many variations of the same broad concept of meaningful differences (e.g., the Clinically Significant Improvement _CSI_, Clinically Significant Difference _CSD_, the Minimal Detectable Change _MDC_, the Minimal Clinically Important Difference _MCID_, and the Minimal Important Difference _MID_) and there are many more approaches to calculating estimates which represent the extent of individual change relative to some error variance, as well as to other subjects. The variations of these individual criteria and formulas can be roughly divided into two approaches, which in practice are not always mutually exclusive: (1) _distribution-based_ methods (i.e. interpreting score changes in relation to the underlying distribution of test scores in a relevant sample) and (2) _anchor-based_ methods (i.e. involving external criteria as references for clinically meaningful change, e.g., a cutoff criterion of 2 between-subjects standard deviations for significance) [@Estrada.2018; @Haley.2006]. The clinical significance approaches included in this study´s analyses represent both of these categories separately and in combination.

<!------------------------------------------------------------------------------------------------------------->
## Estimating the Precision of Clinical Significance Methods {#theory-pss}

The clinical significance methods investigated in this study are both statistical tests and diagnostic criteria, which makes their performance dependent not only on their ability to detect existing meaningful changes, but also on their ability to detect cases without meaningful changes. To evaluate the performance of classification methods (e.g., bio-medical tests for medical conditions), different commonly used metrics can be calculated from their classifications, if reference classifications from another method are available for comparison. The classes resulting from the investigated method can then, for each class respectively, be categorized into true positive (TP), true negative (TN), false positive (FP), and false negative (FN) classifications (i.e., in a contingency table or confusion matrix). Naturally, in the optimal case, the _true_ classes of the observed object are known and can be used as a reference, resulting in the most precise and meaningful estimates for the performance of the classification method. The labels _positive_ and _negative_ are not necessary, but describe the most common binary classification problems. Regardless of the number of classes, there exist many performance measures which can be computed from the frequencies of TP, TN, FP, and FN cases. For the present study, the following quality measures will be relevant [e.g., see  @Berthold.2020, p. 107].
\newline
\par \noindent
_Sensitivity_, also known as _recall_ or _true-positive rate_, is the probability of a given method to correctly identify positive cases.
\begin{equation}
Sensitivity = \frac{TP}{P} = \frac{TP}{TP + FN} (\#eq:sensitivity)
\end{equation}

\par \noindent
_Specificity_, also known as _selectivity_ or _true-negative rate_, is the probability of a given method to correctly identify negative cases.
\begin{equation}
Specificity = \frac{TN}{N} = \frac{TN}{TN + FP} (\#eq:specificity)
\end{equation}

<!------------------------------------------------------------------------------------------------------------->
## Purpose of the Study

The present thesis forms an in-depth investigation of the theoretically expected increases in sensitivity and specificity of diagnostic approaches in clinical trials through the use of multiple baseline and follow-up assessments, as practically implemented in ecological momentary assessment.
\par
Furthermore, this study is concerned with the comparison of currently used techniques for determining meaningful change in longitudinal clinical trials, which follow either a single-point approach or an intense-assessment approach to measuring psychopathology. An individualized adaptation of the Reliable Change Index, the RCI\textsubscript{ind,pre-SD}, which implements within-subjects rather than between-subjects variability, is proposed and tested. Agreement and differences between these estimates for the clinical significance of symptom changes will be examined in order to evaluate the psychometric quality of their classifications.
\par
For the purpose of investigating the above-mentioned research questions in combination, an exploratory simulation study will be conducted, in which the included clinical change methods will be compared for both a classical questionnaire format and an EMA format of a short scale for depressive symptoms.

<!--chapter:end:02-theoretical-background.Rmd-->

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

# Method {#method}

The present study constitutes an investigation of classification outcomes from different methods in simulated scenarios of a psychiatric trial with depressive patients.

<!------------------------------------------------------------------------------------------------------------->
## Study Design

The methodology follows a repeated-measures design with simulated patient data. Data were generated on the basis of various empirical data sets, ensuring the validity of findings. Simulation studies are characterized by relying on data that is generated and sampled pseudo-randomly on the basis of known distributions of the respective variables of interest. They can be used to empirically investigate the performance of statistical methods under specific conditions and allow to draw conclusions about them independently of context-specific influences that would otherwise be present in studies with real-world data [@Morris.2019].

<!------------------------------------------------------------------------------------------------------------->
## Data Simulation Procedure

All following analyses are based on mathematically simulated data sets that were generated for a previous study by @Schuster.2020. A detailed description of the simulation process can be found in the supplementary material of their article online ^[\url{https://doi.org/10.1016/j.invent.2020.100313}]. Estimated parameters and the data-generation process will be described in the following sections.
\par
Data were simulated for two essential scenarios, questionnaire and EMA, on the basis of empirical trials that were conducted with clinical samples of patients with diagnosed depression. As described in sections 2 and 3 of the appendix of the original article, the dependence structure between subsequent assessments was simulated using copulas (i.e. a Frank copula for control conditions and a Clayton copula for treatment conditions). While @Schuster.2020 generated and analyzed both \enquote{actual trial data} (scenario 2), which were informed by the EVIDENT trial [@Klein.2016], and realistic, synthetic data without a specific basis (scenarios 1 and 3), only the latter category was used in this thesis. The difference between them is that the so-called \enquote{actual trial data} scenario has differing pairwise correlations between subsequent assessments ($r_{tt}$ between .46 and .69), while the dependence structure of the other scenarios was set constantly as the average empirically found inter-correlations between assessments (i.e. $r_{tt}$ = .4 between subsequent EMA assessments and $r_{tt}$ = .7 between subsequent questionnaire assessments). As a special form, one simulation also included varying parameters accounting for potential systematic bias, such as a learning effect in EMA data (e.g., participant reactivity to repeated measurements.
\par
All simulated scenarios were based on data sets implementing the Patient Health Questionnaire-9 [PHQ-9, @Kroenke.2001], which is commonly used to evaluate changes in the degree of self-reported depressive symptoms. In this short scale, all 9 items are scored on a 4-point Likert scale (0-3), resulting in a total score between 0 and 27, with higher scores indicating more severe depressive mood. In practice, the PHQ-9 is being used in both assessment modalities, i.e. for questionnaire assessments and EMA.

<!------------------------------------------------------------------------------------------------------------->
### Simulated Scenarios

To give an overview, the characteristics of all investigated trial scenarios are summarized in Table \@ref(tab:data-structure). The simulation process leading to this data is described in Appendix \@ref(pre-pro).
\newline
\par
\textbf{Questionnaire Scenarios} include treatment conditions within intense 30-fold assessment intervals  (PP\textsubscript{30.30}), intense 5-fold random-window assessment intervals (PP\textsubscript{5.5-Window}), and single pre-post assessments (PP\textsubscript{1.1}), as well as no-treatment (control) conditions within intense 5-fold assessment intervals (PP\textsubscript{5.5}) and single pre-post assessments (PP\textsubscript{1.1}).
\par
\textbf{EMA Scenarios} include treatment conditions within intense 30-fold assessment intervals (EMA\textsubscript{30.30}), intense 5-fold random-window assessment intervals (EMA\textsubscript{5.5-Window}), and intense 5-fold random-day assessment intervals (EMA\textsubscript{5.5-Days}), as well as no-treatment (control) conditions within intense 5-fold assessment intervals (EMA\textsubscript{5.5}) and single pre-post assessments (EMA\textsubscript{1.1}).
\par

\begin{table}[thb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Final Data Structure of the Simulated Scenarios, Including Standard-Questionnaire and EMA Scenarios With Three Different Sampling Frequencies Each Under Treatment Conditions, and Standard-Questionnaire and EMA Scenarios With Two Different Sampling Frequencies Each Under No-Treatment (Control) Conditions}}
  \label{tab:data-structure}
  \begin{tabular}{@{}ccccccc@{}}
  \toprule
  Condition & Scenario & $N$ & $d$ & Frequency & $r_{tt}$ & $\alpha$\\
  \midrule
  Treat & Standard Questionnaire & 8180 & 0.89 & 30-30       & 0.65 & 0.98\\
        &                        &      &     & 5-5 Window  & 0.66 & \textbf{0.87}\\
        &                        &      &     & 1-1         & 0.10 & 0.17\\
  \midrule
        & EMA & 8040 & 0.88 & 30-30      & 0.31 & 0.93\\
        &     &       &     & 5-5 Window & 0.31 & \textbf{0.62}\\
        &     &       &     & 5-5 Days   & 0.30 & 0.70\\
  \midrule
  No-Treat & Standard Questionnaire & 99810 & 0.00 & 5-5 & 0.66 & \textbf{0.83}\\
           &                        &       &      & 1-1 & 0.36 & 0.53\\
  \midrule
            & EMA & 99964 & 0.00  & 5-5  & 0.33 & \textbf{0.51}\\
            &     &       &       & 1-1  & 0.17 & 0.29\\
  \bottomrule
  \end{tabular}
  \begin{tablenotes}[para]
  \normalsize{\textit{Note.} Treat = treatment condition, No-Treat = no-treatment (control) condition, $d$ = effect size Cohen´s \textit{d} for the mean difference between the first pre- and the first post-assessment, $r_{tt}$ = average correlation between subsequent assessments (i.e. test-retest reliability), $\alpha$ = internal consistency Cronbach´s $\alpha$ between assessments, highlighting in bold font indicates which $\alpha$ estimate was implemented in Reliable Change Indices for each scenario respectively}
  \end{tablenotes}
\end{threeparttable}
\end{table}

Treatment-condition data sets for both diagnostic methods showed an overall effect size of Cohen´s _d_ between 0.88 (EMA) and 0.89 (standard questionnaire) for the symptom change from pre- to post-treatment assessments. Their overall treatment effect would therefore be considered large [@Cohen.2013], while lying within the range of real empiric effect sizes reported in research on psychiatric outcomes. No-treatment scenarios with _d_ = 0, on the other hand, were used in separate analyses to investigate how precise the included clinical significance methods recognized cases without meaningful changes (i.e. specificity), and in turn, how many cases were falsely classified as clinically meaningful changes (i.e. false positives).
\par
The correlation matrices of pre- and post-treatment assessments are given in Appendix \@ref(app-corrmats). The pairwise correlation coefficients between subsequent assessments, $r_{tt}$, were roughly equal both within and between pre- and post-treatment intervals for all included scenarios (pooled estimates of $r_{tt}$ are given in Table \@ref(tab:data-structure)). In detail, in treatment scenarios, the average correlation between subsequent assessments was $r_{tt}$ = .64-.65 in PP\textsubscript{30.30}, $r_{tt}$ = .65-.66 in PP\textsubscript{5.5-Window}, $r_{tt}$ = .10 in PP\textsubscript{1.1}, $r_{tt}$ = .31 in EMA\textsubscript{30.30}, $r_{tt}$ = .31 in EMA\textsubscript{5.5-Window}, and $r_{tt}$ = .29-.30 in EMA\textsubscript{5.5-Days}; and in no-treatment scenarios: $r_{tt}$ = .66-.67 in PP\textsubscript{5.5}, $r_{tt}$ = .36 in PP\textsubscript{1.1}, $r_{tt}$ = .33 in EMA\textsubscript{5.5}, and $r_{tt}$ = .17 in EMA\textsubscript{1.1}.
\par
As indicated in Table \@ref(tab:data-structure), the internal consistency Cronbach´s $\alpha$, was implemented to calculate some of the clinical significance methods (see Chapter \@ref(def-rci-ind)). It varied strongly between different assessment frequencies, which is expected, as $\alpha$ typically increases with the number of assessments. To control its differential effects on the classification accuracy of methods in these scenarios, $\alpha$ was taken from the 5-fold random-window scenario for use in calculations within each standard-questionnaire and EMA modality, e.g., $\alpha$ = .87 in PP\textsubscript{5.5-Window} for all three treatment-condition questionnaire scenarios and $\alpha$ = .62 in EMA\textsubscript{5.5-Window} for all three treatment-condition EMA scenarios.
\par
For a comparison to previous studies using the PHQ-9, for instance, @Titov.2011 reported a roughly similar internal consistency of $\alpha$ = .74 (before treatment) and $\alpha$ = .81 (after treatment), while other studies reported higher empirical estimates of $\alpha$, e.g., $\alpha$ = .87 in a study by @Hepner.2009 [see also @Adewuya.2006; @Kroenke.2001; @Kroenke.2010; @Lamers.2008].

<!------------------------------------------------------------------------------------------------------------->
## Clinical Interpretation of PHQ-9 Scores {#phq-int}

Within the PHQ-9 [@Kroenke.2001], depressive mood is evaluated as a sum score of its 9 items. The items of the PHQ-9 correspond to the diagnostic criteria of major depressive disorder in the DSM-IV [@AmericanPsychiatricAssociation.1995]. The severity of depressive symptoms is operationalized by the items retrospectively asking for the patient´s self-report on the frequency of experienced symptoms over the past 2 weeks. Each item scores on a Likert scale between 0 and 3, where 0 corresponds to \textit{\enquote{Not at all}}, 1 to \textit{\enquote{Several days}}, 2 to \textit{\enquote{More than half the days}}, and 3 to \textit{\enquote{Nearly every day}}. The total score of these subjective ratings therefore represents the severity of depressive mood based on the number of experienced symptoms and the frequency with which they were perceived over a period of 2 weeks.
\newline
\par

\begin{table}[bht]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Clinical Interpretation of Depressive Symptom Levels on the PHQ-9 Scale}}
  \label{tab:phq-int}
  \begin{tabular}{@{}ccc@{}}
  \toprule
  PHQ-9 Score & Classification & Depression Severity\\ 
  \midrule
  0--4 & 0 & Minimal or none\\
  5--9 & 1 & Mild\\
  10--14 & 2 & Moderate\\
  15--19 & 3 & Moderately severe\\
  20--27 & 4 & Severe\\
  \bottomrule
  \end{tabular}
  \end{threeparttable}
\end{table}

It is important to note that, although the items are originally formulated retrospectively, they are also commonly adapted to a daily EMA strategy by instructing participants to rate the specified symptoms on their intensity on the present day, instead of on their frequency over the past 2 weeks. Because of the intended daily-assessment structure underlying the simulated data sets, precisely this item variation of the PHQ-9 is assumed for the present study.
\par
The clinical interpretation categories of PHQ-9 Total scores are displayed in Table \@ref(tab:phq-int) [see @Karin.2018b; @Kroenke.2001]. The documentation of the PHQ-9 explicitly discourages diagnosing depressive disorders solely on the basis of the questionnaire. The interpretation categories given in Table \@ref(tab:phq-int) therefore do not exactly correspond to diagnostic levels of depression (with regards to classification systems, such as ICD-10 or DSM-5), but to scale-specific recommendations for interpretation.

<!------------------------------------------------------------------------------------------------------------->
## Classification Methods for Clinically Significant Change {#class-methods}

The following classification methods will be evaluated regarding their accuracy and agreement with each other:

+ the Percentage Change Method (PC),
+ the Reliable Change Index (RCI);
  + Reliable Change Index by Jacobson \& Truax (RCI\textsubscript{JT});
  + Repeated-Assessment Individualized Reliable Change Index with individual pre-treatment standard deviation (RCI\textsubscript{ind,pre-SD}).


\par \noindent
Their respective criteria for clinically significant change are listed in Table \@ref(tab:csi-int).

\begin{table}[h]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Clinically Significant Change Interpretation of PHQ-9 Scores, According to Percentage Change and Reliable Change Criteria}}
  \label{tab:csi-int}
  \small
  \begin{tabular}{@{}lcccc@{}}
  \toprule
  \multicolumn{1}{c}{Method} & PHQ-9 (Post) & Index Value & Classification & Interpretation \\
  \midrule
  \multirow{3}{*}{CSI\textsubscript{PC}} & $\leq$ 9 & PC $\geq$ 50 & -1 & Significant Improvement \\
 & > 9 & -50 < PC < 50 & 0 & No Significant Change \\
 & > 9 & PC $\leq$ -50 & 1 & Significant Deterioration \\
 \midrule
 \multirow{3}{*}{CSI\textsubscript{RCI}} & $\leq$ 9 & RCI < -1.96 & -1 & Significant Improvement \\
 & > 9 & -1.96 $\leq$ RCI $\leq$ 1.96 & 0 & No Significant Change \\
 & > 9 & RCI > 1.96 & 1 & Significant Deterioration \\
 \bottomrule
 \end{tabular}
\end{threeparttable}
\end{table}

<!------------------------------------------------------------------------------------------------------------->
### Percentage Change

The Percentage Change method (\textit{PC}), also known as Percentage Improvement method (\textit{PI}), describes longitudinal changes in test scores proportionally on an individual level. It results in an index that describes a subject´s post-treatment score as a proportion of his or her pre-treatment score. A positive result indicates that the post-treatment score is smaller than the pre-treatment score (i.e. improvement), while a negative result indicates a post-treatment score higher than the pre-treatment score (i.e. deterioration). The formula to calculate a PC index is given in Equation \@ref(eq:pc). Note that it can be applied equally as simple to single assessments as to average scores calculated from assessment intervals.

\begin{equation}
PC = \Bigl(1 - \frac{\overline{x_{2}}} {\overline{x_{1}}}\Bigr) \cdot 100 (\#eq:pc)
\end{equation}

> _Note._ $\overline{x_{2}}$ = mean of subject´s posttest scores, $\overline{x_{1}}$ = mean of subject´s pretest scores

\par \noindent
When applied on a common scoring system for a psychological construct, i.e. including only non-negative scores, the resulting index can assume values smaller than or equal to 100. This is a consequence of the fact that a person can not reduce his or her scores by more than 100%, as the lower bound of the scale itself is non-negative (most typically 0). But depending on the specific scale, it may well be possible that a subject can increase scores from pre- to post-treatment by more than 100%, indicated by a post-treatment score greater than two times the size of the pre-treatment score. Hence, the negative limit of the index (i.e. the most extreme expression of deterioration) is not defined a priori, but rather scale- and data-specific, as it is determined by the maximum of the empirical distribution of pre-treatment scores in relation to the highest achievable score on the scale.
\par
Percentage Change (Percentage Improvement) rates are commonly reported in psycho-pharmacological studies, mostly involving cutoff criteria of 25 or 50% [@Hiller.2012]. Furthermore, particularly for clinical trials on depressive disorders, a large body of research generally recommends using a criterion of $\geq$ 50% improvement to indicate significant treatment response [e.g., see @Bandelow.2006; @Frank.1991; @Hiller.2012; @Lecrubier.2002; @McMillan.2010; @Nierenberg.2001; @Rush.2006].
\par
Following the Clinical Significance method with a two-fold criterion, consisting of (1) proportional change significance in terms of Percentage Change and (2) clinical significance on a scale-specific cutoff score, subjects in the present study are evaluated according to the criteria given in Table \@ref(tab:csi-int). The listed criteria were adopted from the original validation study of the PHQ-9, which defined clinically significant improvement as (1) percentage improvement of PC $\geq$ 50%, combined with (2) a post-treatment score $\leq$ 9 [@McMillan.2010].

<!------------------------------------------------------------------------------------------------------------->
### Reliable Change Index

The Reliable Change Index (\textit{RCI}) was first introduced by @Jacobson.1984 and @Jacobson.1991. It is defined as a	standardized difference score that determines whether a score change is statistically significant, i.e. substantially exceeds the error variance of the assessment method. Hence, it determines if the observed score difference can be attributed to treatment effects rather than to naturally occurring variance in the sample.

<!------------------------------------------------------------------------------------------------------------->
#### Reliable Change [@Jacobson.1984; @Jacobson.1991]

Contemplating the sole reliance on statistical significance of tests, @Jacobson.1991 criticized widespread research approaches for the following problems: (1) comparisons on a group level ignore intra-individual variability and change, and (2) statistically significant group differences are therefore not synonymous with clinical relevance. To address these issues, the authors formulated a two-fold approach to evaluating clinically significant change, consisting of both statistical reliability (i.e. the RC index) and clinical significance in terms of symptom severity scores [@Jacobson.1984].
\par
The RC Index is a standardized measure of the raw score difference between two assessments. It quantifies the extent by which the score difference exceeds the error variance of the assessment method. A significant RCI therefore indicates that the observed change exceeds the measurement error by an extent upon which it can be confidently assumed that it is not caused by error variance, but rather by other factors, such as an applied clinical treatment. The conventionally applied significance cutoff is $|RCI|>1.96$, derived from the z score for 95% confidence, i.e. a two-sided $\alpha$ probability <.05. An $RCI > 1.96$ indicates statistically reliable deterioration, an $RCI < -1.96$ indicates reliable improvement, and $-1.96 \geq RCI \leq 1.96$ indicates no reliable change.

\begin{equation}
RCI = \frac{x_{2} - x_{1}}  {s_{diff}} (\#eq:rci-jt)
\end{equation}

\begin{equation}
s_{diff} = \sqrt{2 \cdot (SE)^2} (\#eq:rci-jt-sdiff)
\end{equation}

\begin{equation}
SE = s_{1} \cdot \sqrt{1 - r_{xx \text{´}}} (\#eq:rci-jt-se)
\end{equation}

> _Note._ $x_{2}$ = subject´s posttest score, $x_{1}$ = subject´s pretest score, $s_{diff}$ = standard error of difference between test scores, $SE$ = standard error of measurement, $s_{1}$ = standard deviation of test scores at pretest, $r_{xx \text{´}}$ = reliability of the measure

\par \noindent
For instance, a significant RC index of $\pm 2$ would show that the score difference was equal to two standard deviations, weighted by the reliability of the method. Furthermore, @Jacobson.1984 and @Jacobson.1991 offer an additional formula for the calculation of a significance cutoff given in raw scores:

\begin{equation}
\textit{significance cutoff} = 1.96 \cdot s_{diff} = 1.96 \cdot \sqrt{2 \cdot (s_{1} \cdot \sqrt{1 - r_{xx \text{´}}})^2} (\#eq:rci-jt-cut)
\end{equation}

> _Note._ $\textit{significance cutoff}$ = (absolute) cutoff score for reliable change (95%-criterion)

\par \noindent
This formula defines the raw score that an individual would have to gain or lose in the given test to be recognized as reliably changed. It is also based on the whole sample´s characteristics. The estimates should be calculated using the standard deviation of either a control group, a normal population, or an experimental group at the baseline assessment (for an adaptation using within-subjects variability, see the proposed RCI\textsubscript{ind,pre-SD} in the following section). It also includes the test-retest reliability, which is oftentimes available in the test documentation or in published validation studies.
\par
Following from the assumption of normally distributed change scores, an individual RCI score could also be interpreted in the sense of percentage ranks, i.e.: Assuming normality, it is expected that $X \%$ of participants getting the same treatment under the same conditions would show an improvement or deterioration of at most the same extent.
\par
Regarding the recommended use cases of the RCI\textsubscript{JT}, there is a general consensus between the original authors and following studies. For instance, @HintonBayre.2000 argues that the RCI\textsubscript{JT} is appropriate when only pretest data and the test reliability are available and a true change in the construct, independently of treatment effects, is not expected. If normative retest data are available, he argues for the inclusion of the post-treatment variance. The absence of true change in the construct is a critical precondition, because in many assessment contexts there are practice effects, regression toward the mean or divergence from the mean, and natural fluctuations in the construct. If these changes are expected independently of an intervention, they need to be taken into account as error variance [@Busch.2015]. When true changes are expected additionally to the effects of experimental interventions, e.g., in the form of practice effects or spontaneous remission, especially regression-based calculation approaches can be used to correct the obtained scores (e.g., hierarchical linear models).

<!------------------------------------------------------------------------------------------------------------->
#### Defining an Individualized Reliable Change Index {#def-rci-ind}

The Individualized Reliable Change Index, \textit{RCI\textsubscript{ind}}, is proposed as an adaptation of the originally defined RCI to repeated-measurement data including more than two timepoints, such as data from EMA procedures. RCI\textsubscript{ind} scores are standardized estimates for the reliable change of a single person, as they are based on each individual´s variance, instead of relying on group-level variance estimates.
\par
The proposed individualized formula, the RCI\textsubscript{ind,pre-SD}, is adapted to include more than two single assessments in its numerator: The originally included score difference between two single scores is replaced by the average difference between a pre-treatment and a post-treatment assessment interval. Through averaging, the number of assessments in each interval can vary between subjects and missing data could be imputed or, after careful consideration, even ignored. It also includes a subject-level standard error, $SE_{D, pre}$. This standard error is calculated using the reliability of the assessment method and the standard deviation of each individual´s scores throughout the pre-treatment interval. In this way, the individual´s pre-post difference is relativized by their own measurement error, which includes both within-subject fluctuation of scores and the reliability (i.e. consistency) of the method. Similar to the RCI\textsubscript{JT}, an individual cutoff score for significant change can also be calculated easily.

\begin{equation}
RCI_{\textit{ind,pre-SD}} = \frac{\overline{x_{2}} - \overline{x_{1}}}  {SE_{D,pre}} (\#eq:rci-ind-presd)
\end{equation}

\begin{equation}
SE_{D,pre} = \sqrt{2 \cdot (s_{x} \cdot (1 - r_{xy})^2)} (\#eq:rci-ind-presd-se)
\end{equation}

\begin{equation}
\textit{significance cutoff} = 1.96 \cdot SE_{D,pre} = 1.96 \cdot \sqrt{2 \cdot (s_{x} \cdot (1 - r_{xy})^2)} (\#eq:rci-ind-presd-cut)
\end{equation}

> _Note._ $\overline{x_{2}}$ = mean of subject´s posttest scores, $\overline{x_{1}}$ = mean of subject´s pretest scores, $SE_{D,pre}$ = standard error of difference between the test scores in the individual´s pre interval, $s_{x}$ = individual standard deviation of pretest time points, $r_{xy}$ = reliability (internal consistency Cronbach´s $\alpha$) of the measure, $\textit{significance cutoff}$ = (absolute) cutoff score for reliable change (95%-criterion)

\par \noindent
Significance cutoff scores for the individualized RCI give the absolute scale points that an individual would have to gain or lose on the respective scale to be classified as reliably changed. However, contrary to the RCI\textsubscript{JT}, the cutoff score in Equation \@ref(eq:rci-ind-presd-se) is calculated individually on each subject. Thus, it is not assumed that there exists a universal cutoff score to decide clinically significant change for all participants in a sample. Rather, each subject would need to pass a personally defined range of points in either direction to be considered reliably deteriorated or improved.
\par
The RCI\textsubscript{ind,pre-SD} formula is a simple adaptation of the original RC approach by @Jacobson.1984 and @Jacobson.1991, but, resulting from a few changes, the proposed estimate is interpreted differently, specifically because (1) it is calculated over assessment intervals rather than single assessments and (2) it is no longer based on a group-level estimate of variance in a single assessment, but on the individual´s score fluctuation over whole intervals. The inclusion of a subject's individual standard deviation(s), rather than group- or population-level estimates of variability, neither understates the individual error term nor inflates it through the influence of test scores of subjects other than the individual of interest. Nonetheless, the variability of a sample or population's responses to the given test is still considered informative for the individual, and is nevertheless a part of the test's reliability, i.e. included in the measurement error.
\par
For the estimate of reliability in the standard error, similar to the RCI\textsubscript{JT}, it is recommended to use the internal consistency Cronbach´s $\alpha$ instead of the test-retest reliability $r_{tt}$. While $r_{tt}$ is used in some common RCI approaches, in contexts where unstable psychological constructs are measured over time, the internal consistency Cronbach's $\alpha$ is more appropriate for the following reasons: As @Cronbach.1947 described in his review of reliability coefficients, the test-retest reliability can be an accurate estimate of measurement accuracy only if the measured construct is expected to be stable over time. By definition, measurement variance in a single test score can only be distinguished from real construct variance over time if the construct does not vary between assessments in reality [@Maassen.2009; @Wyrwich.2004]. Since constructs examined in clinical research and practice are not expected to be stable, but are examined specifically for changes over time, the test-retest reliability is not considered suitable for calculating a reliable index of change. This issue has been addressed in early studies, but is still often not taken into account in clinical research. For instance, @Martinovich.1996 and @Tingey.1996 similarly recommend using the internal consistency instead of the retest reliability.
\par
A person´s average difference in test scores between the two assessment intervals, however large or small it may be, is relativized not only by the reliability (i.e. consistency) of the measurement instrument, but also by the person's own variability in responses to the instrument. Consequently, a person with a relatively large mean difference in a measured construct, but also with much variability in individual test scores over time, will be assigned a smaller RCI than another person with the same mean difference and less variability in test scores. This is because there is a reasonably higher confidence in the accuracy of the resulting difference in test scores for the latter person than for the former.
\par
Following the Clinical Significance method with a two-fold criterion, consisting of (1) statistical significance on a Reliable Change Index and (2) clinical significance on a scale-specific cutoff score, subjects in the present study are evaluated according to the criteria given in Table \@ref(tab:csi-int). It should be noted that the originally introduced 3-class interpretation of RCIs can also be extended to better specify different levels of improvement, as suggested by @Lambert.2009: Patients could be classified as _recovered_ (if the passed both criteria), _improved_ (if they passed only the RCI criterion in the positive direction), _unchanged_ (if they did not pass the RCI criterion), or _deteriorated_ (if they passed the RCI criterion in the negative direction).

<!------------------------------------------------------------------------------------------------------------->
## Analyses of Sensitivity and Specificity

After defining the statistical terms of interest in Chapter \@ref(theory-pss), their precise implementations regarding the research question and data ought to be clarified shortly.
\par
In the present study, positive cases are equivalent to true cases of meaningful change, and therefore include both _true_ improvement and deterioration. Negative cases are equivalent to true cases of no meaningful change. In order to apply the definition of sensitivity (see Equation \@ref(eq:sensitivity)) to the three classes of change interpretation, a class-weighted average sensitivity can be calculated in the following way:

\begin{equation}
\begin{aligned}
Sensitivity_{\textit{cwa}} = \frac{1}{n} \sum_{k=1}^{c} tp^{(k)} = \frac{tp^{(det)}} {tp^{(det)} + fn^{(det)}} + \frac{tp^{(\textit{nc})}} {tp^{(\textit{nc})} + fn^{(\textit{nc})}} + \frac{tp^{(imp)}} {tp^{(imp)} + fn^{(imp)}} (\#eq:sensitivity-cwa)
\end{aligned}
\end{equation}

\par \noindent
Equally, the class-weighted average specificity is calculated in the following way:

\begin{equation}
\begin{aligned}
Specificity_{\textit{cwa}} = \frac{1}{n} \sum_{k=1}^{c} tn^{(k)} = \frac{tn^{(det)}} {tn^{(det)} + fp^{(det)}} + \frac{tn^{(\textit{nc})}} {tn^{(\textit{nc})} + fp^{(\textit{nc})}} + \frac{tn^{(imp)}} {tn^{(imp)} + fp^{(imp)}} (\#eq:specificity-cwa)
\end{aligned}
\end{equation}

> _Note._ $c$ = number of classes $k$ (i.e. 3: deteriorated; not changed; improved); $n$ = total number of cases; $tp^{k}$ = proportion of true positive cases in class k; $fn^{k}$ = proportion of false negative cases in class k; $tn^{k}$ = proportion of true negative cases in class k; $fp^{k}$ = proportion of false positive cases in class k

<!------------------------------------------------------------------------------------------------------------->
## False-Positive Rate and Specificity in a Control Group

False-positive rates and the specificity of clinical change methods were investigated in questionnaire and EMA scenarios with overall within-subjects effect sizes of Cohen´s _d_ $\approx$ 0, representing the scores of a control group in a clinical trial. The characteristics of these data sets are summarized in Table \@ref(tab:data-structure). Although some participants in these simulated scenarios showed a substantial symptom improvement or deterioration, the overall pre-post symptom changes were closely distributed around 0, with the vast majority of cases showing no meaningful changes in absolute scores. The main advantage of using zero-effect data sets for this analysis is that the absence of a general treatment effect, along with equally distributed random positive and negative effects, enables the a-priori assumption that proportions of cases identified as changed should be minimal in the most specific calculation methods. The respective cases would only constitute false-positive classifications (i.e. both classifications of improvement and of deterioration), as the number of truly changed participants would be _P_ = _TP_ + _FN_ = 0, implying that cases of true change could neither be detected (i.e. _TP_ = 0) nor overlooked (i.e. _FN_ = 0) in these scenarios. Following from their definitions, classification sensitivity (see Equation \@ref(eq:sensitivity)) could not be calculated under these conditions, while the classification specificity (see Equation \@ref(eq:specificity)) could be appropriately estimated with regard to the known _ground truth_ of the whole sample consisting of only negative (i.e. non-changed) cases.
\newline
\par \noindent
The _false-positive rate (FPR)_ is given by:
\begin{equation}
FPR = \frac{FP}{N} = \frac{FP}{FP + TN} (\#eq:fpr)
\end{equation}

\par \noindent
Hence, classification methods can be compared regarding their false-positive rates and their specificity (i.e. probability of true-positive classifications) on the basis of this data.

<!--chapter:end:03-method.Rmd-->

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!--
```{r, include=FALSE, cache=FALSE, eval=FALSE}
#knitr::purl("PP_Stichprobenvergleiche_d0.92.Rmd", output = "PP_Stichprobenvergleiche_d0.92.R", documentation = 2)

#knitr::read_chunk("PP_Stichprobenvergleiche_d0.92.R")
```

```{r echo=FALSE}
load("data/cor_07_k20/PP_30.30.RData")
PP_30.30 = PP_30.30 %>%
  as_tibble()

load("data/cor_07_k20/PP_5.5.RData")
PP_5.5 = PP_5.5 %>% 
  dplyr::rename(ID_orig = ID) %>% 
  filter(ID_orig %in% PP_30.30$ID1_PRE)

load("data/cor_07_k20/PP_1.1.RData")

pre_30mzp = c("PRE1_1","PRE1_2","PRE1_3","PRE1_4","PRE1_5",
            "PRE1_6","PRE1_7","PRE1_8","PRE1_9","PRE1_10",
            "PRE1_11","PRE1_12","PRE1_13","PRE1_14","PRE1_15",
            "PRE1_16","PRE1_17","PRE1_18","PRE1_19","PRE1_20",
            "PRE1_21","PRE1_22","PRE1_23","PRE1_24","PRE1_25",
            "PRE1_26","PRE1_27","PRE1_28","PRE1_29","PRE1_30")

post_30mzp = c("POST1_1","POST1_2","POST1_3","POST1_4","POST1_5",
             "POST1_6","POST1_7","POST1_8","POST1_9","POST1_10",
             "POST1_11","POST1_12","POST1_13","POST1_14","POST1_15",
             "POST1_16","POST1_17","POST1_18","POST1_19","POST1_20",
             "POST1_21","POST1_22","POST1_23","POST1_24","POST1_25",
             "POST1_26","POST1_27","POST1_28","POST1_29","POST1_30")

pre_5mzp = c("PRE1_1","PRE1_2","PRE1_3","PRE1_4","PRE1_5")
post_5mzp = c("POST1_1","POST1_2","POST1_3","POST1_4","POST1_5")

PP_5.5$PRE_Mean = apply(PP_5.5[pre_5mzp], 1, mean)
PP_5.5$POST_Mean = apply(PP_5.5[post_5mzp], 1, mean)
PP_5.5$MeanDiff = PP_5.5$PRE_Mean - PP_5.5$POST_Mean
PP_5.5$ind.pretestSD = apply(PP_5.5[pre_5mzp], 1, sd)
PP_5.5$ind.posttestSD = apply(PP_5.5[post_5mzp], 1, sd)

PP_1.1$Diff = as.numeric(PP_1.1$PRE - PP_1.1$POST)

# Ausschluss von Personen ohne Varianz in min. einem MZP-Intervall

PP_5.5 = PP_5.5 %>% 
  filter(ind.pretestSD != 0 & ind.posttestSD != 0)

PP_30.30 = PP_30.30 %>% 
  filter(ind.pretestSD != 0 & ind.posttestSD != 0)


PP_5.5 = PP_5.5 %>% 
  filter(ID_orig %in% PP_30.30$ID1_PRE)

PP_30.30 = PP_30.30 %>% 
  filter(ID1_PRE %in% PP_5.5$ID_orig)

PP_1.1 = PP_1.1 %>% 
  filter(ID_orig %in% PP_5.5$ID_orig & ID_orig %in% PP_30.30$ID1_PRE)


PP_5.5 = PP_5.5 %>% 
  add_column(., .before = "ID_orig", ID = 1:nrow(.))

PP_30.30 = PP_30.30 %>% 
  add_column(., .before = "ID1_PRE", ID = 1:nrow(.))

PP_1.1 = PP_1.1 %>% 
  add_column(., .before = "ID_orig", ID = 1:nrow(.))
```
-->


# Results {#results}

All steps of data preparation and statistical analyses were performed using the statistical programming language R [@RCoreTeam.2020]. A complete list of additionally loaded packages is provided in Appendix \@ref(session-info).

## Clinical Significance Under Treatment Conditions {#res-treat}

### Pre--Post Differences in Symptom Scores

The first step of analyses was concerned with the question if the respective variants of questionnaire and EMA scenarios were sufficiently comparable between each other. Their similarity was necessary for the purpose of comparing the course of clinical symptoms of the same participants between different assessment frequencies. The simulation and pre-processing methods described in the previous chapter and in Appendix \@ref(pre-pro) ensured that scenarios with different questionnaire and EMA frequencies were linked to comprise the same participants. Although all questionnaire data sets had an identical sample (\textit{N} = 8180), and equally all EMA data sets had an identical sample (\textit{N} = 8040), it was furthermore necessary that they had respectively similar overall effect sizes between pre- and post-treatment intervals of symptom scores. The data sets were judged comparable if they showed similar overall pre- and post-treatment levels of depression with similar average standard deviations.

<!------------------------------------------------------------------------------------------------------------->
#### Questionnaire Scenarios

The within-subjects pre-post treatment effect was equal among all questionnaire scenarios (PP\textsubscript{5.5-Window}, PP\textsubscript{30.30}, and PP\textsubscript{1.1}), Cohen´s $d = 0.89$.^[The sample-level effect size was calculated between the first pre-treatment and the first post-treatment PHQ-9 assessment.] As shown in Figure \@ref(fig:k20-pp-datasets-pre-post-boxplots), the data set PP\textsubscript{5.5-Window} had average pre-treatment interval depression levels of $\overline{x_1} = 10.33$ ($s_{x_1} = 1.90$) and post-treatment levels of $\overline{x_2} = 7.10$ ($s_{x_2} = 2.33$); PP\textsubscript{30.30} had average pre-treatment interval depression levels of $\overline{x_1} = 10.33$ ($s_{x_1} = 1.87$) and post-treatment levels of $\overline{x_2} = 7.09$ ($s_{x_2} = 2.31$); and PP\textsubscript{1.1} had average pre-treatment single-assessment depression levels of $\overline{x_1} = 10.37$ ($s_{x_1} = 3.25$) and post-treatment levels of $\overline{x_2} = 7.12$ ($s_{x_2} = 4.00$). For a more detailed overview of within-subjects treatment effects within the three questionnaire scenarios, see Figure \@ref(fig:k20-pp-55-3030-11-pre-post-plot) in Appendix \@ref(distr-plots).

\begin{figure}[thb]
\caption{\textit{Box Plots for PHQ-9 Score Distributions of 5-Fold (PP\textsubscript{5.5-Window}) and 30-Fold (PP\textsubscript{30.30}) Pre- and Post-Interval Mean Scores and Single Pre-Post Scores (PP\textsubscript{1.1}) in a Simulated Standard-Questionnaire Scenario}}\label{fig:k20-pp-datasets-pre-post-boxplots}
\includegraphics[width=0.75\linewidth]{data/Plots/k20_PP-Datasets_Pre-Post_Boxplots} \hfill{}
\end{figure}

<!------------------------------------------------------------------------------------------------------------->
#### EMA Scenarios

The within-subjects pre-post treatment effect was equal for all EMA scenarios (EMA\textsubscript{30.30}, EMA\textsubscript{5.5-Window}, and EMA\textsubscript{5.5-Days}), Cohen´s $d = 0.88$. As shown in Figure \@ref(fig:k20-ema-datasets-pre-post-boxplots), the data set EMA\textsubscript{30.30} had average pre-treatment interval depression levels of $\overline{x_1} = 10.31$ ($s_{x_1} = 2.51$) and post-treatment levels of $\overline{x_2} = 7.12$ ($s_{x_2} = 3.12$); EMA\textsubscript{5.5-Window} had average pre-treatment interval depression levels of $\overline{x_1} = 10.32$ ($s_{x_1} = 2.58$) and post-treatment levels of $\overline{x_2} = 7.10$ ($s_{x_2} = 3.18$); and EMA\textsubscript{5.5-Days} had average pre-treatment interval depression levels of $\overline{x_1} = 10.31$ ($s_{x_1} = 2.42$) and post-treatment levels of $\overline{x_2} = 7.11$ ($s_{x_2} = 2.98$). Compared to the 30-fold questionnaire scenario PP\textsubscript{30.30}, the standard deviations of pre- and post-treatment interval averages, $s_{x_1}$ and $s_{x_2}$, were higher in the 30-fold EMA scenario EMA\textsubscript{30.30}. This difference was also present between both 5-fold EMA scenarios (EMA\textsubscript{5.5-Window} and EMA\textsubscript{5.5-Days}) and the 5-fold questionnaire scenario (PP\textsubscript{5.5-Window}). Hence, there was generally more variation in within-subject interval-wise average depression levels in EMA scenarios than in questionnaire scenarios with corresponding assessment frequencies. For a more detailed overview of within-subjects treatment effects within the three EMA scenarios, see Figure \@ref(fig:k20-ema-3030-55w-55d-pre-post-plot) in Appendix \@ref(distr-plots).

\begin{figure}[htb]
\caption{\textit{Box Plots for PHQ-9 Score Distributions of 30-Fold (EMA\textsubscript{30.30}), 5-Fold Random Window (EMA\textsubscript{5.5-Window}), and 5-Fold Random Days (EMA\textsubscript{5.5-Days}) Pre-Treatment and Post-Treatment Interval Mean Scores in a Simulated EMA Scenario}}\label{fig:k20-ema-datasets-pre-post-boxplots}
\includegraphics[width=0.75\linewidth]{data/Plots/k20_EMA-Datasets_Pre-Post_Boxplots} \hfill{}
\end{figure}

<!------------------------------------------------------------------------------------------------------------->
### Comparison of Classification Methods

In this section, results of applying the investigated classification methods under treatment conditions are summarized and compared with each other in terms of their sensitivity and specificity. Summary and evaluation tables are given for questionnaire and EMA scenarios.

\begin{table}[thb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Evaluation of Performances Within Classification Methods Between Different Assessment Frequencies of Questionnaire and EMA Scenarios in Reference to their Respective 30-Fold Scenarios}}
  \label{tab:pp-ema-senspec-wmbf}
  \footnotesize
  \begin{tabular}{@{}c|c|cccc|c|cccc@{}}
  \toprule
  & \multicolumn{5}{c|}{Questionnaire} & \multicolumn{5}{c}{EMA} \\
  \midrule
  Method & \multicolumn{1}{c|}{Freq.} & \multicolumn{1}{c}{Sens.} & \multicolumn{1}{c}{Spec.} & \multicolumn{1}{c}{Acc.} & \multicolumn{1}{c|}{$\kappa$} & \multicolumn{1}{c|}{Freq.} & \multicolumn{1}{c}{Sens.} & \multicolumn{1}{c}{Spec.} & \multicolumn{1}{c}{Acc.} & \multicolumn{1}{c}{$\kappa$} \\
  \midrule
  PC & 30.30 (BL) & 1 & 1 & 1 & 1 & 30.30 (BL) & 1 & 1 & 1 & 1 \\
     & 5.5 Window & 0.89 & 0.95 & 0.92 & 0.83 & 5.5 Window & 0.82 & 0.92 & 0.90 & 0.74 \\
     & 1.1 & 0.71 & 0.84 & 0.72 & 0.47 & 5.5 Days & 0.79 & 0.90 & 0.85 & 0.64 \\
  \midrule
  CSI\textsubscript{PC} & 30.30 (BL) & 1 & 1 & 1 & 1 & 30.30 (BL) & 1 & 1 & 1 & 1 \\
                        & 5.5 Window & 0.90 & 0.95 & 0.94 & 0.83 & 5.5 Window & 0.80 & 0.91 & 0.91 & 0.71 \\
                        & 1.1 & 0.69 & 0.84 & 0.77 & 0.47 & 5.5 Days & 0.80 & 0.88 & 0.87 & 0.61 \\
  \midrule
  RCI & 30.30 (BL) & 1    & 1    & 1    & 1    & 30.30 (BL)  & 1    & 1    & 1    & 1 \\
      & 5.5 Window & 0.84 & 0.93 & 0.88 & 0.77 & 5.5 Window  & 0.79 & 0.90 & 0.87 & 0.71 \\
      & 1.1        & 0.59\textsuperscript{\tiny{JT}} & 0.81\textsuperscript{\tiny{JT}} & 0.66\textsuperscript{\tiny{JT}} & 0.39\textsuperscript{\tiny{JT}} & 5.5 Days    & 0.75 & 0.88 & 0.81 & 0.60 \\
  \midrule
  CSI\textsubscript{RCI} & 30.30 (BL) & 1 & 1 & 1 & 1 & 30.30 (BL) & 1 & 1 & 1 & 1 \\
                         & 5.5 Window & 0.87 & 0.94 & 0.91 & 0.83 & 5.5 Window & 0.82 & 0.90 & 0.89 & 0.72 \\
                         & 1.1 & 0.63\textsuperscript{\tiny{JT}} & 0.82\textsuperscript{\tiny{JT}} & 0.73\textsuperscript{\tiny{JT}} & 0.49\textsuperscript{\tiny{JT}} & 5.5 Days & 0.75 & 0.88 & 0.84 & 0.61 \\
  \bottomrule
  \end{tabular}
  \begin{tablenotes}[para]
  \normalsize{\textit{Note.} \textit{N} = 8.180, (BL) = baseline for performance evaluation, PC and CSI\textsubscript{PC} refer to mean percentage change in multiple-assessment and to percentage change in single-assessment scenarios; CSI\textsubscript{RCI} refers to the individualized CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} in multiple-assessment and to the CSI\textsubscript{RCI\textsubscript{\tiny{JT}}} (highlighted with \textsuperscript{JT}) in single-assessment scenarios; Freq. = assessment frequency; Sens. = sensitivity; Spec. = specificity; Acc. = accuracy (percentage agreement with reference method); $\kappa$ = Cohen´s $\kappa$}
  \end{tablenotes}
\end{threeparttable}
\end{table}

\par
Table \@ref(tab:pp-ema-senspec-wmbf) shows the results of within-method/between-frequencies comparisons of classification outcomes for both questionnaire and EMA scenarios. With the respective 30-fold scenario as a reference for each method, classification performances were compared between different 5-fold and single-point assessment scenarios.
\par
Within questionnaire assessments, accuracy levels across all four methods were consistently higher in 5-fold Random Window than in Single-Point scenarios. The increases in accuracy ranged between 17--22% (largest increase for RCI), while increases in sensitivity ranged between 18--25% (largest increase for RCI) and increases in specificity ranged between 11--12%. All four methods reached only moderate levels of accuracy (.66--.77) when applied in a Single-Point scenario, but high levels of accuracy (.88--.94) in a 5-fold Random Window scenario.
\par
Similarly, within EMA assessments, accuracy levels across all four methods were also consistently higher in 5-fold Random Window than in 5-fold Random Days scenarios. The increases in accuracy ranged between 4--6%, while increases in sensitivity ranged between 0--7% (no increase for CSI\textsubscript{PC} and largest increase for CSI\textsubscript{RCI}) and increases in specificity ranged between 2--3%. Overall, despite small advantages of EMA\textsubscript{5.5 Window} over EMA\textsubscript{5.5 Days}, all methods resulted in high levels of agreement (accuracy between .81--.91) in comparison to their 30-fold reference scenario.

<!------------------------------------------------------------------------------------------------------------->
#### Questionnaire Scenarios

From the perspective of Clinical Significance criteria, i.e. based partially on the interpretation of PHQ-9 pre- and post-treatment levels of depression (see Table \@ref(tab:csi-int)), the baseline distribution of severity levels among participants -- i.e. symptomatic pre-treatment levels of depression (PHQ-9 $\geq$ 9) in 55--60% of participants in questionnaire scenarios -- implies that only these respective proportions of each sample would be able to show clinically significant improvement after the treatment. Simultaneously, the baseline distributions imply that between 40--45% of participants in questionnaire scenarios would theoretically be able to deteriorate significantly over the course of the treatment.
\par
For a summary of classification results in standard- and intense-questionnaire scenarios, see Table \@ref(tab:pp-class). The summary table displays the absolute and relative frequencies of classification categories (i.e., sig. deteriorated, not sig. changed, and sig. improved) which resulted from applying the investigated clinical significance methods within questionnaire scenarios. To interpret the classification performances, it is important to note that the (Mean) PC and the RCI methods did not include an external, symptom-level criterion, while the CSI\textsubscript{PC} and the CSI\textsubscript{RCI} methods did include an external cutoff score.

\begin{table}[ht]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Classifications Resulting From Clinical Change Methods in Questionnaire Scenarios}}
  \label{tab:pp-class}
  \small
  \begin{tabular}{@{}ccccc@{}}
  \toprule
  Frequency & Method & \text{deteriorated (\%)} & \text{no sig. change (\%)} & \text{improved (\%)}\\
  \midrule
  30.30 & CSI\textsubscript{PC} (BL) & \textbf{181 (2.2 \%)} & \textbf{6275 (76.7 \%)} & \textbf{1724 (21.1 \%)}\\
      & Mean PC & \text{224 (2.7 \%)} & \text{5417 (66.2 \%)} & \text{2539 (31.0 \%)}\\
      & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & \text{352 (4.3 \%)} & \text{4839 (59.2 \%)} & \text{2989 (36.5 \%)}\\
      & RCI\textsubscript{ind,pre-SD} & \text{737 (9.0 \%)} & \text{2182 (26.7 \%)} & \text{5261 (64.3 \%)}\\
  \midrule
  5.5 Window & CSI\textsubscript{PC} & \text{226 (2.8 \%)} & \text{6157 (75.3 \%)} & \text{1797 (22.0 \%)}\\
             & Mean PC & \text{302 (3.7 \%)} & \text{5226 (63.9 \%)} & \text{2652	(32.4 \%)}\\
             & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & \text{378 (4.6 \%)} & \text{4845 (59.2 \%)} & \text{2957 (36.1 \%)}\\
             & RCI\textsubscript{ind,pre-SD} & \text{828 (10.1 \%)} & \text{2204 (26.9 \%)} & \text{5148 (62.9 \%)}\\
  \midrule
  1.1 & CSI\textsubscript{PC} & \text{457 (5.6 \%)} & \text{5463 (66.8 \%)} & \text{2260 (27.6 \%)}\\
      & PC & \text{729 (8.9 \%)} & \text{4327 (52.9 \%)} & \text{3124 (38.2 \%)}\\
      & CSI\textsubscript{RCI\textsubscript{JT}} & \text{487 (6.0 \%)} & \text{4613 (56.4 \%)} & \text{3080 (37.7 \%)}\\
      & RCI\textsubscript{JT} & \text{749 (9.2 \%)} & \text{3360 (41.1 \%)} & \text{4071 (49.8 \%)}\\
  \bottomrule
  \end{tabular}
  \begin{tablenotes}[para]
  \normalsize{\textit{Note.} \textit{N} = 8.180; (BL) = baseline for performance evaluation (highlighted in bold font)}
  \end{tablenotes}
\end{threeparttable}
\end{table}

\par
Consistently across all assessment frequencies and methods, except for the RCI\textsubscript{ind,pre-SD} method, the biggest proportion of participants was classified as not changed and the second biggest proportion as improved, whereas the RCI\textsubscript{ind,pre-SD} method classified the biggest proportion of the sample as improved and the second biggest proportion as not changed. As the CSI\textsubscript{PC} method in the 30-fold assessment scenario was defined as the reference for classifications, the other methods needed to yield as similar outcomes as possible to be considered precise. The ground truth for questionnaire scenarios was therefore given as a distribution of 181 (2.2 %) deteriorated cases, 6275 (76.7 %) cases with no significant change, and 1724 (21.1 %) improved cases.
\par
Results from the performance evaluation of classification methods in single- and intense-assessment questionnaire scenarios are summarized in Table \@ref(tab:pp-senspec). Note that specificity levels will also be examined separately under no-treatment conditions in chapter \@ref(res-no-treat).

\begin{table}[thb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Performance of Classification Methods in Questionnaire Scenarios in Reference to the Clinical Significance Method}}
  \label{tab:pp-senspec}
  \begin{tabular}{@{}cccccc@{}}
  \toprule
  Frequency & Method & Sensitivity & Specificity & Accuracy & Kappa\\
  \midrule
  30.30 & CSI\textsubscript{PC} (BL) & 1 & 1 & 1 & 1\\
        & Mean PC & 0.95 & 0.95 & 0.90 & 0.75\\
        & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & 0.92 & 0.92 & 0.82 & 0.62\\
        & RCI\textsubscript{ind,pre-SD} & 0.70 & 0.75 & 0.50 & 0.24\\
  \midrule
  5.5 Window & CSI\textsubscript{PC} & 0.90 & 0.95 & 0.94 & 0.83\\
             & Mean PC & 0.87 & 0.91 & 0.84 & 0.63\\
             & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & 0.87 & 0.91 & 0.81 & 0.58\\
             & RCI\textsubscript{ind,pre-SD} & 0.69 & 0.75 & 0.50 & 0.24\\
  \midrule
  1.1 & CSI\textsubscript{PC} & 0.69 & 0.84 & 0.77 & 0.47\\
      & PC & 0.70 & 0.81 & 0.67 & 0.35\\
      & CSI\textsubscript{RCI\textsubscript{JT}} & 0.68 & 0.83 & 0.71 & 0.40\\
      & RCI\textsubscript{JT} & 0.67 & 0.80 & 0.59 & 0.30\\
  \bottomrule
  \end{tabular}
  \begin{tablenotes}[para]
  \normalsize{\textit{Note.} (BL) = baseline for performance evaluation, Accuracy = percentage agreement with reference method; Kappa = Cohen´s $\kappa$}
  \end{tablenotes}
\end{threeparttable}
\end{table}

\par
Considering the CSI\textsubscript{PC} among different sampling frequencies, classifications were highly similar between the 5-fold Random Window and the 30-fold scenario (accuracy = .94), while the classification accuracy was considerably lower in the single-point scenario (.77). Similarly high sensitivity levels were generally achieved by all methods in the 5-fold Random Window and the 30-fold scenario, except for the RCI\textsubscript{ind,pre-SD} method, which resulted in considerably lower sensitivity levels.
\par
For both 5-fold Random Window and 30-fold questionnaire scenarios, sensitivity and specificity levels >.90 were achieved by the Mean PC, the CSI\textsubscript{RCI\textsubscript{ind,pre-SD}}, and the CSI\textsubscript{PC} methods, indicating good classification performances with accuracy levels (i.e. percentage agreement) between 81--94 %. Hence, these methods were able to correctly identify both significantly changed and not changed symptom trajectories considerably well.
\par
Within the single-point questionnaire scenario, however, all computed methods resulted in moderate performances at best. Specificity levels were consistently higher than sensitivity levels, with the highest sensitivity of .70 resulting from the PC method. Thus, none of the CSI\textsubscript{PC}, PC, CSI\textsubscript{RCI\textsubscript{JT}}, and RCI\textsubscript{JT} methods were able to identify deteriorated, not changed, and improved cases reasonably well within the single-point questionnaire scenario.
\par
There was a consistent advantage of the CSI\textsubscript{PC} over the PC method (increase in accuracy of 10%), as well as an advantage of the CSI\textsubscript{RCI\textsubscript{pre-SD}} over the RCI\textsubscript{ind,pre-SD} (increase in accuracy of 31--32%), and of the CSI\textsubscript{RCI\textsubscript{JT}} over the RCI\textsubscript{JT} (increase in accuracy of 12%), indicating that the inclusion of a cutoff score for clinical significance strongly increased the accuracy of each of the methods.

<!------------------------------------------------------------------------------------------------------------->
#### EMA Scenarios

From the perspective of Clinical Significance criteria, i.e. based partially on the interpretation of PHQ-9 pre- and post-treatment levels of depression (see Table \@ref(tab:csi-int)), the baseline distribution of severity levels among participants -- i.e. symptomatic pre-treatment levels of depression (PHQ-9 $\geq$ 9) in 58--60% of participants in EMA scenarios -- implies that only these respective proportions of the sample would be able to show clinically significant improvement after the treatment. Simultaneously, the baseline distributions imply that between 40--42% of participants in EMA scenarios would theoretically be able to deteriorate significantly over the course of the treatment.
\par
For a summary of classification results in EMA scenarios, see Table \@ref(tab:ema-class). The summary table displays the absolute and relative frequencies of classification categories (i.e., sig. deteriorated, not sig. changed, and sig. improved) which resulted from applying the investigated clinical significance methods within EMA scenarios. To interpret the classification performances, it is important to note that the (Mean) PC and the RCI methods did not include an external, symptom-level criterion, while the CSI\textsubscript{PC} and the CSI\textsubscript{RCI} methods did include an external cutoff score.

\begin{table}[htb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Classifications Resulting From Clinical Change Methods in EMA Scenarios}}
  \label{tab:ema-class}
  \small
  \begin{tabular}{@{}ccccc@{}}
  \toprule
  Frequency & Method & \text{deteriorated (\%)} & \text{no sig. change (\%)} & \text{improved (\%)}\\
  \midrule
  30.30 & CSI\textsubscript{PC} (BL) & \textbf{59 (0.7 \%)} & \textbf{6523 (81.1 \%)} & \textbf{1458 (18.1 \%)}\\
        & Mean PC & \text{73 (0.9 \%)} & \text{6068 (75.5 \%)} & \text{1899 (23.6 \%)}\\
        & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & \text{54 (0.7 \%)} & \text{5945 (73.9 \%)} & \text{2041 (25.4 \%)}\\
        & RCI\textsubscript{ind,pre-SD} & \text{82 (1.0 \%)} & \text{5382 (66.9 \%)} & \text{2576 (32.0 \%)}\\
  \midrule
  5.5 Window & CSI\textsubscript{PC} & \text{99 (1.2 \%)} & \text{6359 (79.1 \%)} & \text{1582 (19.7 \%)}\\
             & Mean PC & \text{123 (1.5 \%)} & \text{5858 (72.9 \%)} & \text{2059 (25.6 \%)}\\
             & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & \text{87 (1.1 \%)} & \text{5899 (73.4 \%)} & \text{2054 (25.5 \%)}\\
             & RCI\textsubscript{ind,pre-SD} & \text{127 (1.6 \%)} & \text{5283 (65.7 \%)} & \text{2630 (32.7 \%)}\\
  \midrule
  5.5 Days & CSI\textsubscript{PC} & \text{183 (2.3 \%)} & \text{6169 (76.7 \%)} & \text{1688 (21.0 \%)}\\
           & Mean PC & \text{225 (2.8 \%)} & \text{5559 (69.1 \%)} & \text{2256 (28.1 \%)}\\
           & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & \text{152 (1.9 \%)} & \text{5550 (69.0 \%)} & \text{2338 (29.1 \%)}\\
           & RCI\textsubscript{ind,pre-SD} & \text{224 (2.8 \%)} & \text{4783 (59.5 \%)} & \text{3033 (37.7 \%)}\\
  \bottomrule
  \end{tabular}
  \begin{tablenotes}[para]
  \normalsize{\textit{Note.} \textit{N} = 8.040; (BL) = baseline for performance evaluation (highlighted in bold font)}
  \end{tablenotes}
\end{threeparttable}
\end{table}

\par
Consistently across all assessment frequencies and methods, the biggest proportion of participants was classified as not changed and the second biggest proportion as improved. As the CSI\textsubscript{PC} method in the 30-fold assessment scenario was defined as the reference for classifications, the other methods needed to yield as similar outcomes as possible to be considered precise. The ground truth for EMA scenarios was therefore given as a distribution of 59 (0.7 %) deteriorated cases, 6523 (81.1 %) cases with no significant change, and 1458 (18.1 %) improved cases. Overall, the CSI\textsubscript{PC} method yielded fairly similar distributions among the three sampling frequencies.
\par
Results from the performance evaluation of classification methods in different EMA scenarios are summarized in Table \@ref(tab:ema-senspec). Note that specificity levels will also be examined separately under no-treatment conditions in chapter \@ref(res-no-treat).

\begin{table}[htb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Performance of Classification Methods in EMA Scenarios in Reference to the Clinical Significance Method}}
  \label{tab:ema-senspec}
  \begin{tabular}{@{}cccccc@{}}
  \toprule
  Frequency & Method & Sensitivity & Specificity & Accuracy & Kappa\\
  \midrule
  30.30 & CSI\textsubscript{PC} (BL) & 1 & 1 & 1 & 1\\
        & Mean PC & 0.98 & 0.98 & 0.94 & 0.84\\
        & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & 0.70 & 0.90 & 0.86 & 0.61\\
        & RCI\textsubscript{ind,pre-SD} & 0.68 & 0.87 & 0.79 & 0.48\\
  \midrule
  5.5 Window & CSI\textsubscript{PC} & 0.80 & 0.91 & 0.91 & 0.71\\
             & Mean PC & 0.80 & 0.90 & 0.86 & 0.62\\
             & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & 0.67 & 0.85 & 0.82 & 0.50\\
             & RCI\textsubscript{ind,pre-SD} & 0.66 & 0.84 & 0.76 & 0.41\\
  \midrule
  5.5 Days & CSI\textsubscript{PC} & 0.80 & 0.88 & 0.87 & 0.61\\
           & Mean PC & 0.82 & 0.88 & 0.82 & 0.53\\
           & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & 0.70 & 0.84 & 0.78 & 0.44\\
           & RCI\textsubscript{ind,pre-SD} & 0.69 & 0.82 & 0.71 & 0.35\\
  \bottomrule
  \end{tabular}
  \begin{tablenotes}[para]
  \normalsize{\textit{Note.} Accuracy = percentage agreement with reference method; Kappa = Cohen´s $\kappa$}
  \end{tablenotes}
\end{threeparttable}
\end{table}

\par
Considering the CSI\textsubscript{PC} among different sampling frequencies, classifications were relatively similar between the 30-fold and both 5-fold scenarios. Overall, a comparison of performance metrics within each method between the three sampling frequencies shows that for every method, its specificity, accuracy, and Cohen´s $\kappa$ indicated that the agreement of classifications with the 30-fold reference scenario was the highest within the 30-fold scenario, followed by the 5-fold Random Window scenario and the 5-fold Random Days scenario. In contrast, the sensitivity levels of the Mean PC, the CSI\textsubscript{RCI\textsubscript{ind,pre-SD}}, and the RCI\textsubscript{ind,pre-SD} methods did not consistently follow this rank order, although the differences were relatively small. However, considering all performance metrics, the examined clinical significance methods calculated for random assessment windows were generally more representative of participants´ _true_ change classifications given by the CSI\textsubscript{PC} (calculated on their individual 30-fold sample of assessments). This appears to indicate an accuracy advantage of implementing intervals of subsequent days over random days for assessing depressive symptoms.
\par
Specificity levels were consistently higher than sensitivity levels. The highest sensitivity levels were achieved by the Mean PC method in the 30-fold scenario (sensitivity = .98, specificity = .98, $\kappa$ = .84), followed by almost similarly high levels from the CSI\textsubscript{PC} method (sensitivity = .80, specificity = .91, $\kappa$ = .71) and the Mean PC method (sensitivity = .80, specificity = .90, $\kappa$ = .62) in the 5-fold Random Window scenario and in the 5-fold Random Days scenario. Within the 5-fold Random Days scenario, considerably good performances were also only achieved by the CSI\textsubscript{PC} method (sensitivity = .80, specificity = .88, $\kappa$ = .61) and the related Mean PC method (sensitivity = .82, specificity = .88, $\kappa$ = .53).
\par
In both 5-fold scenarios, the CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} and the RCI\textsubscript{ind,pre-SD} methods achieved specificity levels >.80, but much lower sensitivity levels between .66 and .70. Regarding the performance of the proposed individualized RCI formula, the results showed a consistently small advantage of the CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} over the RCI\textsubscript{ind,pre-SD}, throughout all EMA scenarios, indicating that the inclusion of a cutoff score for clinical significance increased the accuracy of the RCI\textsubscript{ind,pre-SD} by 6--7%. For the PC method, the inclusion of a cutoff score for clinical significance (i.e. CSI\textsubscript{PC}) increased its accuracy by 5%.


<!------------------------------------------------------------------------------------------------------------->
## Clinical Significance Under No-Treatment Conditions {#res-no-treat}

### False--Positive Rate and Specificity in a Control Group

<!------------------------------------------------------------------------------------------------------------->
#### Questionnaire Scenarios

The within-subjects pre-post treatment effect was equal among both no-treatment questionnaire scenarios (PP\textsubscript{5.5} and PP\textsubscript{1.1}), Cohen´s $d = 0.00$.^[The sample-level effect size was calculated between the first pre-treatment and the first post-treatment PHQ-9 assessment.] The data set PP\textsubscript{5.5} had average pre-treatment interval depression levels of $\overline{x_1} = 10.40$ ($s_{x_1} = 2.66$) and post-treatment levels of $\overline{x_2} = 10.39$ ($s_{x_2} = 2.65$), and PP\textsubscript{1.1} had average pre-treatment single-assessment depression levels of $\overline{x_1} = 10.38$ ($s_{x_1} = 3.45$) and post-treatment levels of $\overline{x_2} = 10.39$ ($s_{x_2} = 3.46$).
\par

\begin{figure}[th]
\caption{\textit{PHQ-9 Score Distributions of (1) 5-Fold Individual Pre- and Post-Treatment Interval Mean Scores and (2) Single Individual Pre- and Post-Treatment Scores of a No-Treatment Control Group in a Simulated Standard-Questionnaire Scenario}}\label{fig:k62-pp-55-11-pre-post-plot}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k62_PP_5.5_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k62_PP_1.1_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\end{figure}

Figure \@ref(fig:k62-pp-55-11-pre-post-plot) gives a more complete overview over within-subjects treatment effects observed in both questionnaire scenarios, with individual score changes depicted by thin gray lines, the overall average pre-post effect given by the bold black line, and pre- and post-treatment score distributions depicted as density and box plots. The left plot in Figure \@ref(fig:k62-pp-55-11-pre-post-plot) displays individual changes for participants in the 5-fold questionnaire scenario between pre- and post-treatment intervals. The right plot in Figure \@ref(fig:k62-pp-55-11-pre-post-plot) displays individual changes for participants in the single-assessment questionnaire scenario between their pre- and post-treatment assessments.

\begin{table}[htb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Specificity of Classification Methods for a No-Treatment Control Group in Simulated Questionnaire Scenarios}}
  \label{tab:pp-fpr-spec}
  \begin{tabular}{@{}ccccc@{}}
  \toprule
  Frequency & Method & False Positives & True Negatives & Specificity\\
  \midrule
  5.5 & Mean PC & 14291 & 85519 & 0.86\\
      & CSI\textsubscript{PC} & 11811 & 87999 & 0.88\\
      & RCI\textsubscript{ind,pre-SD} & 48316 & 51494 & 0.52\\
      & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & 26020 & 73790 & 0.74\\
  \midrule
  1.1 & PC & 21029 & 78777 & 0.79\\
      & CSI\textsubscript{PC} & 13953 & 85857 & 0.86\\
      & RCI\textsubscript{JT} & 33364 & 66446 & 0.67\\
      & CSI\textsubscript{RCI\textsubscript{JT}} & 21376 & 78434 & 0.79\\
  \bottomrule
  \end{tabular}
  \begin{tablenotes}[para]
  \normalsize{\textit{Note.} \textit{N} = 99.810}
  \end{tablenotes}
\end{threeparttable}
\end{table}

\par
Results of the analyses of specificity across all clinical significance methods in both no-treatment questionnaire scenarios are given in Table \@ref(tab:pp-fpr-spec). It should be noted that, in contrast to the simulated treatment scenarios in chapter \@ref(res-treat), the present simulation of no-treatment conditions did not require selecting a reference method to serve as the ground truth for classifications. Instead, specificity levels were calculated from false-positive and true-negative rates, based on the fact that no treatment effects were present in these scenarios and the assumption that therefore all cases classified as \textit{deteriorated} or \textit{improved} were false-positive judgments.
\par
In both scenarios, the highest specificity levels were achieved by the CSI\textsubscript{PC} method (.86--.88) and the Mean PC method (.79--.86), while the lowest specificity levels were achieved by the RCI\textsubscript{ind,pre-SD} method. Surprisingly, the RCI\textsubscript{JT} and the CSI\textsubscript{RCI\textsubscript{JT}} methods resulted in higher specificity levels within the single-point scenario (.67 and .79, respectively) than the RCI\textsubscript{ind,pre-SD} and the CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} methods within the 5-fold scenario (.52 and .74, respectively).
\par
Especially considering that the PP\textsubscript{1.1} scenario only included two single assessments, the specificity of the CSI\textsubscript{PC} method can be considered high (.86), closely followed by the PC method and the CSI\textsubscript{RCI\textsubscript{JT}} method with specificity = .79. In conclusion, particularly the CSI\textsubscript{PC} and the Mean PC methods were able to correctly identify symptom trajectories without clinically significant changes considerably well.

<!------------------------------------------------------------------------------------------------------------->
#### EMA Scenarios

The within-subjects pre-post treatment effect was equal among both no-treatment EMA scenarios (EMA\textsubscript{5.5} and EMA\textsubscript{1.1}), Cohen´s $d = 0.00$.^[The sample-level effect size was calculated between the first pre-treatment and the first post-treatment PHQ-9 assessment.] The data set EMA\textsubscript{5.5} had average pre-treatment interval depression levels of $\overline{x_1} = 10.40$ ($s_{x_1} = 2.01$) and post-treatment levels of $\overline{x_2} = 10.40$ ($s_{x_2} = 2.01$), and EMA\textsubscript{1.1} had average pre-treatment single-assessment depression levels of $\overline{x_1} = 10.40$ ($s_{x_1} = 3.45$) and post-treatment levels of $\overline{x_2} = 10.39$ ($s_{x_2} = 3.45$).
\par

\begin{figure}[ht]
\caption{\textit{PHQ-9 Score Distributions of (1) 5-Fold Individual Pre- and Post-Treatment Interval Mean Scores and (2) Single Individual Pre- and Post-Treatment Scores of a No-Treatment Control Group in a Simulated EMA Scenario}}\label{fig:k62-ema-55-11-pre-post-plot}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k62_EMA_5.5_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k62_EMA_1.1_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\end{figure}

Figure \@ref(fig:k62-ema-55-11-pre-post-plot) gives a more complete overview over within-subjects treatment effects observed in both EMA scenarios, with individual score changes depicted by thin gray lines, the overall average pre-post effect given by the bold black line, and pre- and post-treatment score distributions depicted as density and box plots. The left plot in Figure \@ref(fig:k62-ema-55-11-pre-post-plot) displays individual changes for participants in the 5-fold EMA scenario between pre- and post-treatment intervals. The right plot in Figure \@ref(fig:k62-ema-55-11-pre-post-plot) displays individual changes for participants in the single-assessment EMA scenario between their pre- and post-treatment assessments.

\begin{table}[htb]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Specificity of Classification Methods for a No-Treatment Control Group in Simulated EMA Scenarios}}
  \label{tab:ema-fpr-spec}
  \begin{tabular}{@{}ccccc@{}}
  \toprule
  Frequency & Method & False Positives & True Negatives & Specificity\\
  \midrule
  5.5 & Mean PC & 7896 & 92068 & 0.92\\
      & CSI\textsubscript{PC} & 6975 & 92989 & 0.93\\
      & RCI\textsubscript{ind,pre-SD} & 10921 & 89043 & 0.89\\
      & CSI\textsubscript{RCI\textsubscript{ind,pre-SD}} & 7816 & 92148 & 0.92\\
  \midrule
  1.1 & PC & 26644 & 73315 & 0.73\\
      & CSI\textsubscript{PC} & 6975 & 92989 & 0.93\\
      & RCI\textsubscript{JT} & 13600 & 86364 & 0.86\\
      & CSI\textsubscript{RCI\textsubscript{JT}} & 11236 & 88728 & 0.88\\
  \bottomrule
  \end{tabular}
  \begin{tablenotes}[para]
  \normalsize{\textit{Note.} \textit{N} = 99.964}
  \end{tablenotes}
\end{threeparttable}
\end{table}

\par
Results of the analyses of specificity across all clinical significance methods in both no-treatment EMA scenarios are given in Table \@ref(tab:ema-fpr-spec). It should be noted that, in contrast to the simulated treatment scenarios in chapter \@ref(res-treat), the present simulation of no-treatment conditions did not require selecting a reference method to serve as the ground truth for classifications. Instead, specificity levels were calculated from false-positive and true-negative rates, based on the fact that no treatment effects were present in these scenarios and the assumption that therefore all cases classified as \textit{deteriorated} or \textit{improved} were false-positive judgments.
\par
In the EMA\textsubscript{5.5} scenario, all methods achieved similarly high specificity levels between .89--.93. The CSI\textsubscript{PC} method achieved the highest specificity level in the EMA\textsubscript{1.1} scenario (.93), followed by the CSI\textsubscript{RCI\textsubscript{JT}} method with specificity = .88, while the PC method resulted in the lowest specificity level of .73.
\par
In conclusion, all examined methods achieved high specificity levels in the 5-fold no-treatment EMA scenario and, in the single-point EMA scenario, particularly the CSI\textsubscript{PC} method was similarly able to correctly identify symptom trajectories without clinically significant changes with a high precision.


<!--chapter:end:04-results.Rmd-->


# Discussion

The present thesis aimed to investigate possible increases in precision of research designs in clinical trials through the use of ecological momentary assessment instead of single-point questionnaire assessments, as well through the use of psychometrically valid classification methods for determining meaningful symptom changes. An exploratory simulation study was conducted, in which a selection of clinical significance methods was compared for both a classical questionnaire format and an EMA format of the PHQ-9 scale for depressive symptoms. The following methods were evaluated: Percentage Change (PC), the Reliable Change Index RCI\textsubscript{JT} [@Jacobson.1984; @Jacobson.1991], and the Individualized Reliable Change Index RCI\textsubscript{ind,pre-SD} introduced in this thesis, as well as their Clinical Significance variants CSI, which included an additional cutoff criterion for determining significant changes.
\par
In this chapter, the results presented in the previous section will be summarized and interpreted. Furthermore, strengths and limitations of this study will be discussed and a final conclusion will be given.

<!------------------------------------------------------------------------------------------------------------->
## Discussion of Results

<!-- Increase in precision in % für 5.5>1.1 (v.a. impr. + det.) + Aufwand (5 Tage) vs. Benefit (accuracy), geeignet für Forschung + Praxis -->
The investigated increases in classification precision resulting from the implementation of multiple daily rather than single-point assessments were consistently found for both questionnaire and EMA formats, but were most pronounced in standard-questionnaire scenarios: By choosing 5-fold assessment intervals rather than two single questionnaire assessments, the accuracy of clinical significance methods was increased by between 17--22%. In particular, the sensitivity was increased by between 18--25% and the specificity by between 11--12%. All of the investigated methods especially benefited from assessment intervals in questionnaire scenarios, where they improved from moderate to high levels of accuracy in determining significantly improved and deteriorated symptom changes. For EMA scenarios, the classification accuracy was increased by between 4--6%, the sensitivity by between 0--7%, and the specificity by between 2--3%, through applying 5-fold intervals of subsequent days rather than 5-fold random-day assessments. Overall, these considerable increases in precision could justify the additional effort of implementing 5-fold assessment intervals over single-point assessments in clinical research and practice.
\par
<!-- Treatment Conditions: PP scenarios -->
In 30-fold, 5-fold Random Window, and single-point questionnaire scenarios under treatment conditions, the methods with the highest sensitivity and specificity were the CSI\textsubscript{PC}, the Mean PC, the CSI\textsubscript{RCI\textsubscript{ind,pre-SD}}, and the RCI\textsubscript{ind,pre-SD} methods. Comparing only 5-fold and 30-fold scenarios, all methods (i.e. Mean PC, RCI\textsubscript{ind,pre-SD}, and CSI\textsubscript{RCI\textsubscript{ind,pre-SD}}) reached similarly high sensitivity and specificity levels >.90.
\par
In a single-point standard-questionnaire scenario, specificity levels were consistently higher than sensitivity levels, indicating that all examined methods were better able to identify negative cases (i.e., correctly identifying when each of the clinical change categories \textit{is not} present) than positive cases (i.e., correctly identifying when each of the clinical change categories \textit{is} present).
\par
<!-- Treatment Conditions: EMA scenarios -->
In 30-fold, 5-fold Random Window, and 5-fold Random Days EMA scenarios under treatment conditions, the methods with the highest sensitivity and specificity were the CSI\textsubscript{PC}, the Mean PC, and the RCI\textsubscript{ind,pre-SD} methods, respectively. Contrary to expectations, there were no large differences between classifications calculated from 30 vs. 5 pre- and post-treatment assessments, except in the Mean PC method, which dropped in sensitivity and specificity from the 30-fold to both 5-fold scenarios. Across all methods and sampling frequencies, specificity levels were consistently higher than sensitivity levels. In both 5-fold Random Window and Random Days scenarios, considerably good performances were only achieved by the CSI\textsubscript{PC} and the Mean PC methods.
\par
<!-- Random Windows > Random Days -->
Classifications in an EMA scenario with randomly selected windows (i.e., comprised of 5 subsequent days pre- and post-treatment) were generally more similar to the \enquote{true} classifications from their full 30-fold assessment interval than the classifications calculated over 5-fold randomly selected sets of days. This result indicates an advantage of study designs with regular daily assessments over designs with randomly selected assessment days. This suggestion seems especially reasonable when a kind of treatment is provided, because its expected effects could presumably be captured most reliably when other systematic, time-sensitive influences are minimized. Similarly, when studying the effects of a therapy over time, random and treatment-independent influences could be detected and statistically controlled more easily in time series with evenly spaced assessments than with random intervals between them.
\par
<!-- Sensitivity + Specificity: CSI_PC > RCI_ind_pre-SD > ... -->
Regarding the examined clinical significance methods under treatment conditions in general, sensitivity and specificity analyses revealed the strongest performances throughout different sampling frequencies resulting from the Clinical Significance approach with a Percentage Change criterion (CSI\textsubscript{PC}), followed by the Clinical Significance approach with an Individualized Reliable Change Index (RCI\textsubscript{ind,pre-SD}).
\newline
\par
<!-- No-Treatment Conditions: FPR + Specificity -->
<!-- Specificity: EMA>PP, Spec. in EMA-No-Treat very good, in PP for EN very low -->
When considering the performance of classification methods, the category of _no significant change_ is equally as important as the directed change categories, because in some contexts, in which deterioration in symptoms is expected over time, the _no change_ category can serve as evidence of therapy effects [@Estrada.2018]. In order to specifically investigate the ability of different approaches to identify these cases, they were also implemented in \enquote{no-treatment} data sets, which simulated waitlist control groups.
\par
Within no-treatment questionnaire scenarios, the highest specificity levels were achieved by the CSI\textsubscript{PC} (in PP\textsubscript{1.1 No-Treat} even higher than in PP\textsubscript{1.1 Treat}), followed by the PC method and the RCI\textsubscript{JT} method. With the lowest specificity in no-treatment questionnaire scenarios, and therefore the highest number of patients falsely determined as significantly changed, the RCI\textsubscript{ind,pre-SD} method generally appeared to be too imprecise for standard-questionnaire studies.
\par
Under no-treatment conditions in EMA scenarios, the overall ability of methods to identify true-negative cases could be considered very good. In the 5-fold scenario, all methods achieved specificity levels >.90, while the CSI\textsubscript{PC} method also did in the single-point scenario. Calculated for only two assessments, the RCI\textsubscript{JT} method also reached acceptable specificity levels >.80. There was an interesting difference between specificity levels in questionnaire and EMA scenarios: Although both were separately investigated in 5-fold and single-point assessment frequencies, every method, except for the PC method, achieved higher specificity levels in EMA scenarios than in standard-questionnaire scenarios. This difference suggests that the PC method performed better (i.e. yielded less false-positive classifications) when calculated with the lower between-assessment intercorrelations present in EMA data.
\par
The (Mean) PC and RCI\textsubscript{ind,pre-SD} methods are proportional and therefore individualized estimates, which are characterized by not making implicit assumptions about the nature of individual change. The RCI\textsubscript{JT} method, on the other hand, has an inherent assumption of linear change in the sense that it judges on the basis of a fixed, sample-level score difference that has to be achieved by participants in order to be regarded as meaningfully improved or deteriorated. This approach treats all individuals equally in that it does not take into account the individual symptom severity expressed at their baseline assessment. Subjects with a low symptom severity at baseline may not be able to show a score reduction $\geq$ the pre-defined meaningful difference, and hence could not be regarded as meaningfully improved, while subjects with a high symptom severity at baseline could pass the required score improvement and be regarded as meaningfully improved, even though their post-treatment score could still be within the clinical range of scores. In contrast, if a method inherently assumes proportional change, it defines the absolute score difference to be regarded as meaningfully changed proportionally, in order to account for the influence of baseline severity. By setting proportional differences as cutoff criteria for classification categories, observed changes are evaluated individually in relation to baseline severity [@Karin.2018].
\par
These characteristics may explain why individualized methods generally resulted in higher sensitivity and specificity levels than the linearly calculated (i.e. more baseline-dependent) RCI\textsubscript{JT}, especially in repeated-assessment scenarios. This is in line with previous research showing that proportional estimates were better apt to model treatment effects, with much higher sensitivity and specificity levels and a lower baseline dependency than linear estimates [@Karin.2018].
\par
<!-- Precision: EMA > PP, R. Window > R. Days, Specificity: EMA > PP -->
Regarding the achieved precision, results of this study suggest a clear superiority of repeated-assessment over single-point assessment approaches, as well as an advantage of random-window over random-day assessment intervals in EMA scenarios. This resulting advantage of EMA over retrospective standard-questionnaire formats is in line with previous research [e.g., @Vork.2019]. Furthermore, specificity ratings were altogether better in EMA scenarios than in standard-questionnaire scenarios, suggesting that true-negative cases (i.e. individuals with no meaningful symptom changes) are generally better detected in EMA data than in questionnaire data.

<!------------------------------------------------------------------------------------------------------------->
## Strengths and Limitations

<!-- Strength -->
The biggest strength of the present study is the empirical basis of the simulated trial data. The generation of questionnaire and EMA data sets in scenarios with different effect sizes and sampling frequencies, which were prepared to comprise respectively identical samples of participants, allowed for comparing individual changes in depressive symptoms across these assessment designs. Questionnaire and EMA scenarios also showed sufficiently similar reliabilities, pre- and post-treatment levels of depressive symptoms, and effect sizes between different scenarios to be comparable without introducing significant systematic biases. A particular emphasis was put on within-subjects comparisons in order to maximize the validity and practicability of findings regarding the sensitivity and specificity of classification methods. In combination with the validated and externally anchored reference method for clinically significant change (i.e. Clinical Significance with Percentage Change criteria and external cutoff scores), these preconditions were created to ensure a high external validity of the results reported in this study.
\par
<!-- Limitation: low baseline levels of depression -->
One notable limitation of the study regards other characteristics of the simulated data sets. Although based on empirically gathered data from clinical samples, on average, the simulated baseline-interval scores were arguably low and mainly corresponded to mild and moderate levels of depression. The PHQ-9 scale has a maximum score of 27 points, but the data used for the analyses in this study did only reach a maximum of 25 points. Therefore, for instance, it would have been possible to add a constant score of 2 points to every single assessment, if the intention would have been to correct the data sets to represent more severe levels of depression. This overall correction would not have had any impact on the effect size (Cohen´s _d_), the underlying covariance matrix of assessments, test-retest reliabilities, the internal consistency Cronbach´s $\alpha$, or the proportional clinical change methods (Percentage Change and Individualized Reliable Change Index). It would only have affected the proportions of cases that were identified by the Clinical Significance method as moved from the clinical to the non-clinical population, or vice versa. This is because the standard definition of clinically significant change in PHQ-9 scores includes the 50 % change criterion, as well as passing the cutoff score of 9 points defining the border between the clinical and the non-clinical distribution [see @McMillan.2010]. However, following from the comparative approach in this study, a constant-value correction would not have altered the measures of interest in this methodological comparison, and neither the conclusions that are drawn from its results. This example is intended to emphasize the generalizability of conclusions regarding the sensitivity and specificity of the analyzed methods in comparison to each other: Assuming that the simulated data sets realistically represent empirically observed clinical trial data, the resulting differences in agreement between methods would be the same, regardless of the symptom-severity levels.
\par
<!-- Standardfehler-Problem:
ws-SD < bs-SD, dadurch weniger Veränderung nötig, um sich nach RCI_ind rel. zu verändern als nach RCI_JT (wegen größerem SE im Nenner beim RCI_JT) -->
Another notable limitation concerns the formula for the newly introduced RCI\textsubscript{ind,pre-SD}, which implements the within-subjects standard deviation to determine the standard error of measurement individually, rather than the between-subjects standard deviation. Although this adaptation is intended, it makes comparisons between both approaches difficult, because the within-subjects variability is often smaller than between-subjects variability. In this case, the RCI\textsubscript{ind,pre-SD} results in more liberal estimates than the RCI\textsubscript{JT} and therefore more subjects being classified as significantly changed. This should be noted when applying the RCI\textsubscript{ind,pre-SD}, especially when comparing it with other methods.
\par
<!-- Strength: PHQ-9 clear clinical interpretation categories + based on DSM-IV criteria
+ Limitation: PHQ-9 adaptation -->
A strength concerning the empirical basis of this study is the use of the PHQ-9 for simulated scenarios, as it can be considered a highly reliable and valid (i.e. its items correspond to the DSM-IV criteria of Major Depressive Disorder) scale, which is extensively studied and applied under both standard-questionnaire and repeated-measurement conditions. However, the use of a special variant of the PHQ-9, which was adapted to daily assessments in EMA scenarios, may confound the originally assessed frequency of depressive symptoms in the past two weeks with the now assessed severity of symptoms on the respective day. This limitation may affect the generalizability of the results in this study.
\par
<!-- Limitation: CSI_PC as Reference -->
Another important limitation concerns the reference standard that was used in parts of the analyses to evaluate the included calculation methods. The clinical significance criteria defined by @McMillan.2010, namely a percentage change of $\geq$ 50 % and passing a given cutoff score, were selected for their empirical basis and adaptation to the simulated questionnaire. Besides this recommended definition of the CS method for the PHQ-9, it would also have been reasonable to implement the @Jacobson.1991 definition, which would consist of the same cutoff score and a Reliable Change Index in the place of percentage change. In this study, the preference for the method including a PC criterion automatically led to higher sensitivity and specificity scores of the PC method, as it was, by definition, very closely related to the reference method of Clinically Significant Change. More importantly, despite the limitations that follow from this specific choice, imperfect reference classifications, unfortunately, are a general problem in almost every context where new assessment methods are evaluated against established ones.
\par
<!-- Future Research: Compare Methods by diff. Reliabilities -->
A potential next step for future research may be comparing the included classification methods for varying levels of reliability, as was done, e.g., by @Atkins.2005. In a simulation study with a similar data structure, pairwise agreement results between clinical significance methods could be computed for a range of empirically relevant reliabilities. Results from the study by @Atkins.2005 showed that for all pairwise comparisons, the agreement between methods increased with higher reliabilities, with Cohen´s Kappa >.90 for a theoretical reliability of _r_ = .95. This extreme result indicates a high dependence of methods on reliability scores: By using very reliable assessment methods, it seems to become less relevant which classification method is applied, as they produce increasingly similar results for high reliabilities. Although reliability estimates as high as $\geq$.90 are not common in many fields of research and also depend on which measure of reliability is used (e.g., Cronbach´s $\alpha$ or $r_{tt}$), high reliabilities are certainly often reported for outcome measures in psychotherapy research.
\par
<!-- Future Research: Compare Methods by diff. Effect Sizes -->
Analogously, the agreement between classification methods could be examined for a range of different overall effect sizes. This simulation would then give deeper insights into important questions such as whether methods also converge in their agreement for increasing effect sizes, or if there are methods which show high sensitivity and specificity over a wide range of effect sizes.

<!------------------------------------------------------------------------------------------------------------->
## Conclusion

<!-- why problem was important -->
The present thesis addressed two important methodological issues which have been prevalent in psychiatric trials for decades and arguably may have contributed to the replication crisis in subdivisions of psychology. The first problem is that the single-point assessment paradigm, which is predominantly used for measuring psychological symptoms over time, has a limited statistical precision to accurately measure the often highly dynamic expression of these symptoms. The second problem of interest is a lack of individual-level methods for evaluating clinically significant symptom changes in repeated-measurement studies. Various useful and widely used methods were introduced in the literature, but most of them were only formulated for either group-level comparisons or individual single-point pre-post assessment scenarios. The purpose of this explorative study was to describe and test ways to improve the psychometric quality of RCTs by conducting ecological momentary assessment studies and implementing individualized criteria for clinically meaningful change.
\par
<!-- ## Implications of Findings -->
On the basis of this research, characteristics like the intensity, frequency, and duration of psychological interventions could be closely tailored to each patient individually, according to their symptom severity and fluctuation over time. Digital mental health tools allow for the precise evaluation of treatment effects within subjects and between subjects in a variety of easily applicable ways [e.g., see @Bauer.2004b]. For instance, within-patient effect sizes and cutoff scores for significant changes may be reported in clinical studies and meta-analyses, in order to increase the transparency and practical benefits of methods. It would also be possible to monitor and analyze symptom changes in EMA apps by incorporating clinical significance methods in their algorithms, which could identify or even predict meaningful changes on the basis of markers for remission or deterioration. Some already widely used routine monitoring tools already offer patient feedback, therapy evaluation, and easy visualization of symptom changes for patients and practitioners. But in addition, individual symptom changes could also be relativized with internal and external factors (e.g., sleep, medication, occupational concerns), to distinguish random noise from treatment effects.
\par
<!-- Raphaels 6 Punkte: 
1. 5.5 > 1.1 in % increase for imp + det -> 5.5 should be used
2. for CSIs, use the baseline-independent, individualized RCI\textsubscript{ind,pre-SD}, or (Mean) PC method
3. highest performance can be expected from PC -> so no need for RCI(ind)?
4. Especially negative effects should be assessed by PC\textsubscript{5.5} or RCI\textsubscript{5.5} (here RCI\textsubscript{5.5} was most liberal, which can be helpful in research on negative events, when an unbiased indicator is desired)
5. accuracy of questionnaire < EMA, despite lower $r_{tt}$ -> indicates that EMA can serve as substitute for questionnaire or even outperform it (+ increase in %)
6. this also implies that changes in psychological states could be assessed more reliably by EMA than by questionnaires (even with equal assessment frequencies)
-->
<!-- Recommendations: EMA + CSI_PC/CSI_RCI_ind_pre-SD -->
There are many approaches to increase the sensitivity and specificity of assessment strategies. In particular, the results of the present study lead to the recommendation for treatment outcome research to implement repeated-measurement designs with short and reliable symptom scales via EMA and to evaluate treatment effects following the Clinical Significance approach with a validated external cutoff criterion and either (1) a Percentage Change criterion of PC $\geq$ 50% or (2) the proposed Individualized Reliable Change Index RCI\textsubscript{ind,pre-SD} (including the individual pre-treatment standard deviation).



<!--chapter:end:05-discussion.Rmd-->


\backmatter

<!-- 
If you'd like to change the name of the bibliography to something else,
delete "References" and replace it.
-->

# References
<!--
This manually sets the header for this unnumbered chapter.
-->
\markboth{References}{References}
<!-- \hypertarget{refs}{} ? -->

<!--
To remove the indentation of the first entry.
-->
\noindent

<!--
To create a hanging indent and spacing between entries.  These three lines may need to be removed for styles that don't require the hanging indent.
-->

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}

::: {#refs}
:::

<!--
This is just for testing with more citations for the bibliography at the end.  Add other entries into the list here if you'd like them to appear in the bibliography even if they weren't explicitly cited in the document.

---
nocite: | 
  @angel2000, @angel2001, @angel2002a
...
-->

<!--chapter:end:06-references.Rmd-->


`r if(knitr:::is_latex_output()) '\\appendix'`
`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'`

<!-- weil in References.Rmd ein Einzug eingestellt wurde -->
\setlength{\parindent}{0in}
\setlength{\leftskip}{0in}
\setlength{\parskip}{0pt}

# Appendix

\markboth{Appendix}{Appendix}

<!------------------------------------------------------------------------------------------------------------->
## Appendix A: Pairwise Correlations Between Assessments {#app-corrmats}

The first appendix includes tables displaying correlation coefficients for pairwise comparisons between single PHQ-9 assessments. Thus, the following correlation matrices give estimates of the retest-reliability $r_{tt}$ for all possible combinations of assessments in the respective dataset. For the sake of readability, they are only included for 5-fold questionnaire and EMA scenarios, but not for 30-fold scenarios. Pairwise correlations between consecutive assessments are displayed in bold font on the diagonals.

<!------------------------------------------------------------------------------------------------------------->
### Questionnaire Scenarios {#rel-corrmats-pp}

<!--
```{r results="asis", echo=FALSE, eval=FALSE}
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/PP_5.5_KorMat.RData")
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/PP_30.30_KorMat.RData")

PP_5.5_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Standard-Questionnaire Scenario",
    label = "cor-pp-55",
    #longtable = TRUE,
    placement = "htb")

PP_30.30_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 30-Fold Standard-Questionnaire Scenario",
    label = "cor-pp-3030",
    #longtable = TRUE,
    placement = "htb")
```
-->

\begin{table}[H]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Window Standard-Questionnaire Scenario}}
  \label{tab:cor-pp-55}
  \tiny
  \begin{tabular}{@{}ccccccccccc@{}}
  \toprule
  & \multicolumn{1}{c}{PRE\_1} & \multicolumn{1}{c}{PRE\_2} & \multicolumn{1}{c}{PRE\_3} & \multicolumn{1}{c}{PRE\_4} & \multicolumn{1}{c}{PRE\_5} & \multicolumn{1}{c}{POST\_1} & \multicolumn{1}{c}{POST\_2} & \multicolumn{1}{c}{POST\_3} & \multicolumn{1}{c}{POST\_4} & \multicolumn{1}{c}{POST\_5}\\
  \midrule
  PRE\_1  & 1 & \textbf{0.65} & 0.53 & 0.50 &	0.55 & 0.11 & 0.11 & 0.12 & 0.12 & 0.12\\
  PRE\_2  & 0.65 & 1 & \textbf{0.65} & 0.52 &	0.51 & 0.10 & 0.12 & 0.11 & 0.12 & 0.12\\
  PRE\_3  & 0.53 & 0.65 &	1 &	\textbf{0.64} & 0.54 & 0.10 & 0.10 & 0.12 & 0.12 & 0.11\\
  PRE\_4  & 0.50 & 0.52 &	0.64 & 1 & \textbf{0.64} & 0.09	& 0.09 & 0.11	& 0.10 & 0.10\\
  PRE\_5  & 0.55 & 0.51	& 0.54 & 0.64 & 1 &	\textbf{0.09} & 0.08 & 0.11 & 0.11 & 0.10\\
  POST\_1 & 0.11 & 0.10	& 0.10 & 0.09 & 0.09 & 1 & \textbf{0.65} & 0.53	& 0.51 & 0.53\\
  POST\_2 & 0.11 & 0.12	& 0.10 & 0.09 & 0.08 & 0.65 & 1 & \textbf{0.65} & 0.52 & 0.52\\
  POST\_3 & 0.12 & 0.11	& 0.12 & 0.11 & 0.11 & 0.53 & 0.65 & 1 & \textbf{0.63} & 0.52\\
  POST\_4 & 0.12 & 0.12	& 0.12 & 0.10 & 0.11 & 0.51 & 0.52 & 0.63 & 1 & \textbf{0.65}\\
  POST\_5 & 0.12 & 0.12	& 0.11 & 0.10 & 0.10 & 0.53 & 0.52 & 0.52 & 0.65 & 1\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}

<!------------------------------------------------------------------------------------------------------------->
### EMA Scenarios {#rel-corrmats-ema}

<!--
```{r results="asis", echo=FALSE, eval=FALSE}
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/EMA_5.5_KorMat.RData")
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/EMA_30.30_KorMat.RData")
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/EMA_5.5_Window_KorMat.RData")
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/EMA_5.5_Days_KorMat.RData")

EMA_5.5_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold EMA Scenario",
    label = "cor-ema-55",
    #longtable = TRUE,
    placement = "htb")

PP_30.30_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 30-Fold EMA Scenario",
    label = "cor-ema-3030",
    #longtable = TRUE,
    placement = "htb")

EMA_5.5_Window_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Window EMA Scenario",
    label = "cor-ema-55-win",
    #longtable = TRUE,
    placement = "htb")

EMA_5.5_Days_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Days EMA Scenario",
    label = "cor-ema-55-days",
    #longtable = TRUE,
    placement = "htb")
```
-->

\begin{table}[H]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Window EMA Scenario}}
  \label{tab:cor-ema-55-win}
  \tiny
  \begin{tabular}{@{}ccccccccccc@{}}
  \toprule
  & \multicolumn{1}{c}{PRE\_1} & \multicolumn{1}{c}{PRE\_2} & \multicolumn{1}{c}{PRE\_3} & \multicolumn{1}{c}{PRE\_4} & \multicolumn{1}{c}{PRE\_5} & \multicolumn{1}{c}{POST\_1} & \multicolumn{1}{c}{POST\_2} & \multicolumn{1}{c}{POST\_3} & \multicolumn{1}{c}{POST\_4} & \multicolumn{1}{c}{POST\_5}\\
  \midrule
  PRE\_1 & 1.00 & \textbf{0.32} & 0.18 & 0.24 & 0.30 & 0.02 & 0.00 & 0.02 & 0.03 & 0.01\\
  PRE\_2 & 0.32 & 1.00 & \textbf{0.30} & 0.19 & 0.22 & 0.03 & 0.02 & 0.01 & 0.02 & 0.02\\
  PRE\_3 & 0.18 & 0.30 & 1.00 & \textbf{0.31} & 0.17 & 0.02 & 0.02 & 0.02 & 0.02 & 0.02\\
  PRE\_4 & 0.24 & 0.19 & 0.31 & 1.00 & \textbf{0.31} & 0.01 & 0.03 & 0.03 & 0.01 & 0.02\\
  PRE\_5 & 0.30 & 0.22 & 0.17 & 0.31 & 1.00 & \textbf{0.03} & 0.01 & 0.02 & 0.01 & 0.02\\
  POST\_1 & 0.02 & 0.03 & 0.02 & 0.01 & 0.03 & 1.00 & \textbf{0.30} & 0.19 & 0.21 & 0.26\\
  POST\_2 & 0.00 & 0.02 & 0.02 & 0.03 & 0.01 & 0.30 & 1.00 & \textbf{0.30} & 0.19 & 0.19\\
  POST\_3 & 0.02 & 0.01 & 0.02 & 0.03 & 0.02 & 0.19 & 0.30 & 1.00 & \textbf{0.31} & 0.17\\
  POST\_4 & 0.03 & 0.02 & 0.02 & 0.01 & 0.01 & 0.21 & 0.19 & 0.31 & 1.00 & \textbf{0.32}\\
  POST\_5 & 0.01 & 0.02 & 0.02 & 0.02 & 0.02 & 0.26 & 0.19 & 0.17 & 0.32 & 1.00\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}

\begin{table}[H]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Days EMA Scenario}}
  \label{tab:cor-ema-55-days}
  \tiny
  \begin{tabular}{@{}ccccccccccc@{}}
  \toprule
  & \multicolumn{1}{c}{PRE\_1} & \multicolumn{1}{c}{PRE\_2} & \multicolumn{1}{c}{PRE\_3} & \multicolumn{1}{c}{PRE\_4} & \multicolumn{1}{c}{PRE\_5} & \multicolumn{1}{c}{POST\_1} & \multicolumn{1}{c}{POST\_2} & \multicolumn{1}{c}{POST\_3} & \multicolumn{1}{c}{POST\_4} & \multicolumn{1}{c}{POST\_5}\\
  \midrule
  PRE\_1 & 1.00 & \textbf{0.28} & 0.34 & 0.36 & 0.33 & 0.00 & 0.02 & 0.01 & 0.00 & 0.02\\
  PRE\_2 & 0.28 & 1.00 & \textbf{0.30} & 0.35 & 0.35 & 0.00 & 0.02 & 0.01 & 0.00 & 0.02\\
  PRE\_3 & 0.34 & 0.30 & 1.00 & \textbf{0.31} & 0.36 & 0.00 & 0.02 & 0.01 & 0.01 & 0.02\\
  PRE\_4 & 0.36 & 0.35 & 0.31 & 1.00 & \textbf{0.28} & -0.01 & 0.01 & 0.00 & -0.02 & -0.01\\
  PRE\_5 & 0.33 & 0.35 & 0.36 & 0.28 & 1.00 & \textbf{0.01} & 0.01 & 0.02 & 0.02 & 0.01\\
  POST\_1 & 0.00 & 0.00 & 0.00 & -0.01 & 0.01 & 1.00 & \textbf{0.28} & 0.34 & 0.35 & 0.33\\
  POST\_2 & 0.02 & 0.02 & 0.02 & 0.01 & 0.01 & 0.28 & 1.00 & \textbf{0.30} & 0.34 & 0.32\\
  POST\_3 & 0.01 & 0.01 & 0.01 & 0.00 & 0.02 & 0.34 & 0.30 & 1.00 & \textbf{0.31} & 0.32\\
  POST\_4 & 0.00 & 0.00 & 0.01 & -0.02 & 0.02 & 0.35 & 0.34 & 0.31 & 1.00 & \textbf{0.29}\\
  POST\_5 & 0.02 & 0.02 & 0.02 & -0.01 & 0.01 & 0.33 & 0.32 & 0.32 & 0.29 & 1.00\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}

\vspace*{\fill}

<!------------------------------------------------------------------------------------------------------------->
## Appendix B: Data Pre-Processing {#pre-pro}

The data sets originally generated for the study of @Schuster.2020 were further prepared to be apt for the particular questions of the present study. The strategy and process will be thoroughly described step by step in the following subsections, satisfying the required level of transparency needed for future reproduction and replication attempts. In the same regard, the R code utilized for these crucial steps of data preparation is included in Appendix \@ref(code)).

<!------------------------------------------------------------------------------------------------------------->
### Extension of Individual Assessments

#### K-Nearest-Neighbor Search

In order to investigate the sensitivity and specificity of estimates obtained through single and short-interval assessment formats in comparison to each subject´s respective _true symptom levels_ -- defined by the score fluctuation in their underlying structure of daily assessments -- it was necessary to extend the originally simulated assessment intervals. As both the questionnaire and EMA scenarios were first modeled for 5-fold intervals, they were extended for further analyses to obtain 30-fold pre- and post assessment intervals. This was achieved with the following approach.
\par
In both simulated data sets comprising _N_ = 100.000 participants each, subjects with equal interval means and standard deviations were matched using a k-nearest-neighbor (KNN) search algorithm. In particular, this was done using the k-dimensional tree algorithm within the function `get.knn()` from the R package _FNN_ [@Beygelzimer.2019]. This KNN-search method compares all cases to one another on one or more dimensions of interest by computing the Euclidian distances between them. For instance, to compare two participants $p = (p_{1},p_{2})$ and $q = (q_{1},q_{2})$ regarding the symptom severity and variability within their baseline assessments, with $p_{1}$ and $q_{1}$ denoting the mean scores and $p_{2}$ and $q_{2}$ denoting the standard deviations of their respective baseline intervals, the Euclidian distance _d_ between them is given by Equation \@ref(eq:eucl-dist):

\begin{equation}
d(p,q) = \sqrt{(q_{1} - p_{1})^2 + (q_{2} - p_{2})^2} (\#eq:eucl-dist)
\end{equation}

Cases were matched separately by pre- and post-treatment intervals to ensure an appropriate balance between (1) within-interval similarity and (2) individual between-interval changes:

1. within-interval similarity between matched cases: cases need to have an exactly equal mean score and fluctuation within the respective interval
2. individual between-interval changes: intervals are matched and concatenated separately: matching pre-treatment intervals are concatenated case-wise and matching post-treatment intervals are concatenated case-wise in another step.

\par \noindent
By specifying the KNN-search function with _k_ = 5, it calculates the similarity of all cases to each other and matches each case with its 5 nearest neighbors, resulting in lists of 6 matched case IDs for each specific (observed) combination of interval means and standard deviations. In this way, cases with similar average symptom scores and similar pre- and post standard deviations were matched inside each data set (questionnaire and EMA). Thereby, only participants with both similar average score changes from pre to post and similar intra-individual variability were matched together.

<!------------------------------------------------------------------------------------------------------------->
#### Generation of 30-fold Individual Assessment Intervals

The individual assessment intervals of these similar cases were then concatenated after one another in order to extend the number of simulated assessments from 5-fold to 30-fold intervals for each participant. In detail, for each combination of 6 perfect neighbors regarding the pre-treatment interval, the IDs of these neighbors were used to bind their 5-fold pre-treatment intervals together to obtain a table of cases with 30-fold pre-treatment intervals. Within this data set of matched pre-case IDs, cases were first sorted within each set of 6 matched IDs (i.e., within rows), then sorted by rows (i.e., by the lowest ID in each row), and then filtered to contain only unique combinations of matched case IDs. This was also done for all cases that were matched by their post-treatment intervals.
\par
Finally, pre- and post-KNN lists were joined by the first, and therefore lowest, ID in each case row. Hence, the number of cases was further filtered to comprise only cases which contained both 6-fold pre- and 6-fold post-case IDs, i.e. only cases with 6 pre-nearest neighbors and 6 post-nearest neighbors, which could be linked together by their lowest ID. Using this KNN-search information, the final 30-fold assessment intervals were created by concatenating the assessments of matched cases from the originally simulated questionnaire- and EMA-like data sets. Both the questionnaire-like and EMA-like samples were reduced by the extension process by about 92 %, resulting in sample sizes of _N_ = 8.240 (questionnaire) and _N_ = 8.087 (EMA). R code for this procedure is provided in the Appendices \@ref(r-knn-search) and \@ref(r-extension).
\par
It should be noted that this strategy to extend assessment intervals, i.e. by stringing together 5-fold intervals from multiple different cases, was only considered appropriate because the originally simulated data presented no signs of autoregressive effects within individual intervals, i.e. neither systematic longitudinal effects between consecutive assessments (i.e., overall improvement or deterioration of symptoms within an interval) nor systematic variability (i.e., regression towards the mean or regression towards the tail). These assumptions can be confirmed, for instance, from the correlation matrices given in Appendix \@ref(app-corrmats).

<!------------------------------------------------------------------------------------------------------------->
### Random Sampling of Assessments From the Intense-Assessment Intervals

In order to realistically simulate drawing arbitrary 5-fold (EMA-like) samples of assessments from each subject´s 30-fold intervals, the following approach was taken within both questionnaire and EMA data sets (see the R code in Appendix \@ref(r-random-sampling)).
\par
For each subject individually, 5-fold windows of pre-treatment and post-treatment assessments were randomly drawn from their respective 30-fold intervals in order to create the scenarios PP\textsubscript{5.5-Window} and EMA\textsubscript{5.5-Window}. This scenario simulates a study design in which participants are monitored via questionnaires or EMA on 5 consecutive days before and after receiving a treatment. Furthermore, only within EMA data, for each subject individually, 5 single pre-treatment and post-treatment assessments were randomly drawn from their respective 30-fold intervals in order to create the scenario EMA\textsubscript{5.5-Days}. This scenario simulates a study design in which participants are monitored via EMA on 5 arbitrary and not necessarily consecutive days before and after receiving a treatment. This was included to analyze potential systematic differences between implementing a daily vs. a non-daily EMA routine.

<!------------------------------------------------------------------------------------------------------------->
### Exclusion of Cases Without Variance

A small number of cases with no symptom variability (i.e. with perfectly constant scores) throughout one or both of their assessment intervals were excluded from all analyses. This criterion for exclusion was formulated because it was deemed improbable for participants to show no fluctuation in PHQ-9 scores over 5-fold or, even more improbable, over 30-fold assessments. Including these cases would also have affected the outcome of the RCI\textsubscript{ind, pre SD}, which incorporates individual standard deviations as estimates of within-subject fluctuations Calculating this estimate of reliable change (see Equation \@ref(eq:rci-ind-presd-se)) with an individual interval standard deviation of 0 would result in an infinite value for the CSI\textsubscript{RCI\textsubscript{ind,pre-SD}}.
\par
The exclusion of these cases was the last step of pre-processing and resulted in the final structure of data sets, as displayed in Table \@ref(tab:data-structure), with the following sample sizes: Among __treatment condition__ trials, _n_ = 60 cases were excluded from questionnaire scenarios and _n_ = 47 cases were excluded from EMA scenarios, resulting in final samples comprising _N_ = 8.180 participants with questionnaire assessments and _N_ = 8.040 participants with EMA assessments. Among __no-treatment condition__ trials, _n_ = 190 cases were excluded from questionnaire scenarios and _n_ = 36 cases were excluded from EMA scenarios, resulting in final samples comprising _N_ = 99.810 participants with questionnaire assessments and _N_ = 99.964 participants with EMA assessments. No-treatment scenarios had larger final sample sizes than treatment scenarios, as the pre-processing steps described above (knn-search, interval extension, and random sampling of assessments) were not applied on them. Within those scenarios, analyses of false-positive rates and specificity levels were only conducted in 5-fold and single-assessment frequencies, therefore not requiring the generation of 30-fold assessment intervals.


<!------------------------------------------------------------------------------------------------------------->
## Appendix C: Distributions of Individual Symptom Changes {#distr-plots}

<!------------------------------------------------------------------------------------------------------------->
### Questionnaire Scenarios {#pp-distr-plot}

Figure \@ref(fig:k20-pp-55-3030-11-pre-post-plot) gives an complete overview over within-subjects treatment effects observed in the three questionnaire scenarios, with individual score changes depicted by thin gray lines, the overall average pre-post effect given by the bold black line, and pre- and post-treatment score distributions depicted as density and box plots. The top-left plot in Figure \@ref(fig:k20-pp-55-3030-11-pre-post-plot) displays individual changes for participants in the 5-fold questionnaire scenario between pre- and post-treatment intervals.^[Repeated-measures box- and violin plots were created following an open-visualizations tutorial available on \url{https://jorvlan.github.io/publications/repmes_tutorial_R.pdf}, which is now also available in the R package \textit{raincloudplots}.] The top-right plot in Figure \@ref(fig:k20-pp-55-3030-11-pre-post-plot) displays individual changes for participants in the 30-fold questionnaire scenario between pre- and post-treatment intervals. The bottom-left plot in Figure \@ref(fig:k20-pp-55-3030-11-pre-post-plot) displays individual changes for participants in the single-assessment questionnaire scenario between their pre- and post-treatment assessments.

\begin{figure}[H]
\caption{\textit{PHQ-9 Score Distributions of (1) 5-Fold and (2) 30-Fold Individual Pre- and Post-Treatment Interval Mean Scores and (3) Single Individual Pre- and Post-Treatment Scores in a Simulated Standard-Questionnaire Scenario}}\label{fig:k20-pp-55-3030-11-pre-post-plot}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_PP_5.5_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_PP_30.30_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_PP_1.1_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\end{figure}

<!------------------------------------------------------------------------------------------------------------->
### EMA Scenarios {#ema-distr-plot}

Figure \@ref(fig:k20-ema-3030-55w-55d-pre-post-plot) gives an overview over within-subjects treatment effects observed in the three EMA scenarios, with individual score changes depicted by thin gray lines, the overall average pre-post effect given by the bold black line, and pre- and post-treatment score distributions depicted as density and box plots. The top-left plot in Figure \@ref(fig:k20-ema-3030-55w-55d-pre-post-plot) displays individual changes for participants in the 30-fold EMA scenario between pre- and post-treatment intervals. The top-right plot in Figure \@ref(fig:k20-ema-3030-55w-55d-pre-post-plot) displays individual changes for participants in the 5-fold Random Window EMA scenario between pre- and post-treatment intervals. The bottom-left plot in Figure \@ref(fig:k20-ema-3030-55w-55d-pre-post-plot) displays individual changes for participants in the 5-fold Random Days EMA scenario between pre- and post-treatment intervals.

\begin{figure}[H]
\caption{\textit{Individual Mean Differences in PHQ-9 Scores Between (1) 30-Fold, (2) 5-Fold Random Window, and (3) 5-Fold Random Days Pre-Treatment and Post-Treatment Intervals in a Simulated EMA Scenario}}\label{fig:k20-ema-3030-55w-55d-pre-post-plot}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_EMA_30.30_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_EMA_5.5_Window_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_EMA_5.5_Days_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\end{figure}


<!------------------------------------------------------------------------------------------------------------->
## Appendix D: R Code {#code}

The third appendix includes information about the R version and packages that were used to prepare and process data, as well as R code for the most important pre-processing steps and the computation of clinical change methods.

<!------------------------------------------------------------------------------------------------------------->
### R Session Information and Used Packages {#session-info}

```{r label="session-info", results="asis", echo=TRUE}
toLatex(sessionInfo())
```

<!------------------------------------------------------------------------------------------------------------->
### K-Nearest-Neighbor Search {#r-knn-search}

K-Nearest-Neighbor Search (using `get.knn()` from the package _FNN_) for the questionnaire data set $PP_{5.5}$ as an example (similar procedure for both the EMA and the questionnaire data set).

```{r label="r-knn-search", eval=FALSE}
pacman::p_load(dplyr, FNN)

# opening the originally simulated data set (N = 100.000) and 
# calculating interval means and standard deviations
PP_5.5 = read.delim("cor_07_k20/cor_07_dataset_k20.txt", 
                    row.names=NULL) %>%
  select(PRE1_1:POST1_5) %>%
  add_column(., .before = "PRE1_1", ID = 1:nrow(.)) %>%
  as_tibble()

pre_5mzp = c("PRE1_1","PRE1_2","PRE1_3","PRE1_4","PRE1_5")
post_5mzp = c("POST1_1","POST1_2","POST1_3","POST1_4","POST1_5")

PP_5.5$PRE_Mean = apply(PP_5.5[pre_5mzp], 1, mean)
PP_5.5$POST_Mean = apply(PP_5.5[post_5mzp], 1, mean)
PP_5.5$MeanDiff = PP_5.5$PRE_Mean - PP_5.5$POST_Mean
PP_5.5$ind.pretestSD = apply(PP_5.5[pre_5mzp], 1, sd)
PP_5.5$ind.posttestSD = apply(PP_5.5[post_5mzp], 1, sd)
save(PP_5.5, file = "cor_07_k20/PP_5.5.RData")

# PRE interval: finding the k=5 nearest neighbors regarding their 
# mean score and standard deviation (with distance == 0)
pre_data = PP_5.5 %>% select(PRE_Mean, ind.pretestSD)
PP_PRE_KNN_df = FNN::get.knn(pre_data, k=5, algorithm = "kd_tree")

x = as_tibble(PP_PRE_KNN_df[[1]], .name_repair = "minimal")
colnames(x) = c("neighbor1", "neighbor2", "neighbor3", 
                "neighbor4", "neighbor5")
y = as_tibble(PP_PRE_KNN_df[[2]], .name_repair = "minimal")
colnames(y) = c("distance1", "distance2", "distance3", 
                "distance4", "distance5")

PP_PRE_KNN_df = bind_cols(x, y) %>%
  add_column(., .before = "neighbor1", ID = 1:nrow(.)) %>%
  filter(distance1 == 0 & distance2 == 0 & distance3 == 0 & 
           distance4 == 0 & distance5 == 0)

# POST interval: finding the k=5 nearest neighbors regarding their 
# mean score and standard deviation (with distance == 0)
post_data = PP_5.5 %>% select(POST_Mean, ind.posttestSD)
PP_POST_KNN_df = FNN::get.knn(post_data, k=5, algorithm = "kd_tree")

x = as_tibble(PP_POST_KNN_df[[1]], .name_repair = "minimal")
colnames(x) = c("neighbor1", "neighbor2", "neighbor3", 
                "neighbor4", "neighbor5")
y = as_tibble(PP_POST_KNN_df[[2]], .name_repair = "minimal")
colnames(y) = c("distance1", "distance2", "distance3", 
                "distance4", "distance5")

PP_POST_KNN_df = bind_cols(x, y) %>%
  add_column(., .before = "neighbor1", ID = 1:nrow(.)) %>%
  filter(distance1 == 0 & distance2 == 0 & distance3 == 0 & 
           distance4 == 0 & distance5 == 0)

# filtering the resulting knn combinations to keep only unique 
# rows of 6 perfectly matching neighbors
# PRE interval
PP_PRE_KNN_df = PP_PRE_KNN_df %>%
  select(ID, neighbor1, neighbor2, neighbor3, neighbor4, neighbor5) %>%
  apply(., 1, sort) %>%
  t() %>%
  as_tibble() %>%
  arrange(., V1, V2, V3, V4, V5, V6) %>%
  distinct() %>%
  filter(V1 != V2 & V2 != V3 & V3 != V4 & V4 != V5 & V5 != V6) %>%
  group_by(V1) %>%
  filter(row_number() == 1) %>%
  ungroup()

colnames(PP_PRE_KNN_df) = c("ID1_PRE", "ID2_PRE", "ID3_PRE", 
                            "ID4_PRE", "ID5_PRE", "ID6_PRE")

# POST interval
PP_POST_KNN_df = PP_POST_KNN_df %>%
  select(ID, neighbor1, neighbor2, neighbor3, neighbor4, neighbor5) %>%
  apply(., 1, sort) %>%
  t() %>%
  as_tibble() %>%
  arrange(., V1, V2, V3, V4, V5, V6) %>%
  distinct() %>%
  filter(V1 != V2 & V2 != V3 & V3 != V4 & V4 != V5 & V5 != V6) %>%
  group_by(V1) %>%
  filter(row_number() == 1) %>%
  ungroup()

colnames(PP_POST_KNN_df) = c("ID1_POST", "ID2_POST", "ID3_POST", 
                             "ID4_POST", "ID5_POST", "ID6_POST")

# joining the matched IDs of pre- and post-neighbors in a data frame
PP_KNNs = inner_join(PP_PRE_KNN_df, PP_POST_KNN_df, 
              by = c("ID1_PRE" = "ID1_POST"))
PP_KNNs = PP_KNNs %>%
  add_column(., .before = "ID2_POST", ID1_POST = PP_KNNs$ID1_PRE)
save(PP_KNNs, file = "cor_07_k20/PP_KNNs.RData")
```

<!------------------------------------------------------------------------------------------------------------->
### Extension of Assessment Intervals {#r-extension}

Extension of assessment intervals for the questionnaire data set $PP_{5.5}$ as an example (similar procedure for both the EMA and the questionnaire data set).

```{r label="r-extension", eval=FALSE}
pacman::p_load(dplyr)
load("cor_07_k20/PP_KNNs.RData")
load("cor_07_k20/PP_5.5.RData")
PP_KNNs = PP_KNNs %>% as.data.frame()

PP_30.30 = data.frame(
  ID1_PRE = c(), ID2_PRE = c(), ID3_PRE = c(), 
  ID4_PRE = c(), ID5_PRE = c(), ID6_PRE = c(),
  ID1_POST = c(), ID2_POST = c(), ID3_POST = c(), 
  ID4_POST = c(), ID5_POST = c(), ID6_POST = c(),

  PRE1_1 = c(), PRE1_2 = c(), PRE1_3 = c(), PRE1_4 = c(), 
  PRE1_5 = c(), PRE1_6 = c(), PRE1_7 = c(), PRE1_8 = c(), 
  PRE1_9 = c(), PRE1_10 = c(), PRE1_11 = c(), PRE1_12 = c(), 
  PRE1_13 = c(), PRE1_14 = c(), PRE1_15 = c(), PRE1_16 = c(), 
  PRE1_17 = c(), PRE1_18 = c(), PRE1_19 = c(), PRE1_20 = c(),
  PRE1_21 = c(), PRE1_22 = c(), PRE1_23 = c(), PRE1_24 = c(), 
  PRE1_25 = c(), PRE1_26 = c(), PRE1_27 = c(), PRE1_28 = c(), 
  PRE1_29 = c(), PRE1_30 = c(),

  POST1_1 = c(), POST1_2 = c(), POST1_3 = c(), POST1_4 = c(), 
  POST1_5 = c(), POST1_6 = c(), POST1_7 = c(), POST1_8 = c(), 
  POST1_9 = c(), POST1_10 = c(), POST1_11 = c(), POST1_12 = c(), 
  POST1_13 = c(), POST1_14 = c(), POST1_15 = c(), POST1_16 = c(), 
  POST1_17 = c(), POST1_18 = c(), POST1_19 = c(), POST1_20 = c(),
  POST1_21 = c(), POST1_22 = c(), POST1_23 = c(), POST1_24 = c(), 
  POST1_25 = c(), POST1_26 = c(), POST1_27 = c(), POST1_28 = c(), 
  POST1_29 = c(), POST1_30 = c())

for (i in 1:length(PP_KNNs$ID1_PRE)) {
  PP_30.30[i,"ID1_PRE"] = PP_KNNs[i,"ID1_PRE"]
  PP_30.30[i,"ID2_PRE"] = PP_KNNs[i,"ID2_PRE"]
  PP_30.30[i,"ID3_PRE"] = PP_KNNs[i,"ID3_PRE"]
  PP_30.30[i,"ID4_PRE"] = PP_KNNs[i,"ID4_PRE"]
  PP_30.30[i,"ID5_PRE"] = PP_KNNs[i,"ID5_PRE"]
  PP_30.30[i,"ID6_PRE"] = PP_KNNs[i,"ID6_PRE"]
  PP_30.30[i,"ID1_POST"] = PP_KNNs[i,"ID1_POST"]
  PP_30.30[i,"ID2_POST"] = PP_KNNs[i,"ID2_POST"]
  PP_30.30[i,"ID3_POST"] = PP_KNNs[i,"ID3_POST"]
  PP_30.30[i,"ID4_POST"] = PP_KNNs[i,"ID4_POST"]
  PP_30.30[i,"ID5_POST"] = PP_KNNs[i,"ID5_POST"]
  PP_30.30[i,"ID6_POST"] = PP_KNNs[i,"ID6_POST"]

  PP_30.30[i,"PRE1_1"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_2"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_3"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_4"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_5"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_5"]
  PP_30.30[i,"PRE1_6"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_7"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_8"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_9"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_10"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_5"]

  PP_30.30[i,"PRE1_11"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_12"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_13"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_14"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_15"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_5"]
  PP_30.30[i,"PRE1_16"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_17"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_18"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_19"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_20"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_5"]

  PP_30.30[i,"PRE1_21"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_22"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_23"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_24"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_25"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_5"]
  PP_30.30[i,"PRE1_26"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_27"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_28"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_29"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_30"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_5"]


  PP_30.30[i,"POST1_1"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_1"]
  PP_30.30[i,"POST1_2"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_2"]
  PP_30.30[i,"POST1_3"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_3"]
  PP_30.30[i,"POST1_4"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_4"]
  PP_30.30[i,"POST1_5"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_5"]
  PP_30.30[i,"POST1_6"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_1"]
  PP_30.30[i,"POST1_7"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_2"]
  PP_30.30[i,"POST1_8"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_3"]
  PP_30.30[i,"POST1_9"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_4"]
  PP_30.30[i,"POST1_10"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_5"]

  PP_30.30[i,"POST1_11"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_1"]
  PP_30.30[i,"POST1_12"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_2"]
  PP_30.30[i,"POST1_13"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_3"]
  PP_30.30[i,"POST1_14"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_4"]
  PP_30.30[i,"POST1_15"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_5"]
  PP_30.30[i,"POST1_16"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_1"]
  PP_30.30[i,"POST1_17"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_2"]
  PP_30.30[i,"POST1_18"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_3"]
  PP_30.30[i,"POST1_19"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_4"]
  PP_30.30[i,"POST1_20"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_5"]

  PP_30.30[i,"POST1_21"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_1"]
  PP_30.30[i,"POST1_22"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_2"]
  PP_30.30[i,"POST1_23"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_3"]
  PP_30.30[i,"POST1_24"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_4"]
  PP_30.30[i,"POST1_25"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_5"]
  PP_30.30[i,"POST1_26"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_1"]
  PP_30.30[i,"POST1_27"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_2"]
  PP_30.30[i,"POST1_28"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_3"]
  PP_30.30[i,"POST1_29"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_4"]
  PP_30.30[i,"POST1_30"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_5"]}
```

<!------------------------------------------------------------------------------------------------------------->
### Random Sampling of 5-fold EMA Windows and Days {#r-random-sampling}

Random sampling of 5-fold EMA assessments from 30-fold intervals for the generation of individual (1) 5-fold windows (similar process for the 5-fold Random-Window standard-questionnaire data set) and (2) 5-fold single assessment days.

```{r label="r-random-sampling", eval=FALSE}
pacman::p_load(dplyr)
set.seed(42)

pre_5mzp = c("PRE1_1","PRE1_2","PRE1_3","PRE1_4","PRE1_5")
post_5mzp = c("POST1_1","POST1_2","POST1_3","POST1_4","POST1_5")

pre_30mzp = c("PRE1_1","PRE1_2","PRE1_3","PRE1_4","PRE1_5",
            "PRE1_6","PRE1_7","PRE1_8","PRE1_9","PRE1_10",
            "PRE1_11","PRE1_12","PRE1_13","PRE1_14","PRE1_15",
            "PRE1_16","PRE1_17","PRE1_18","PRE1_19","PRE1_20",
            "PRE1_21","PRE1_22","PRE1_23","PRE1_24","PRE1_25",
            "PRE1_26","PRE1_27","PRE1_28","PRE1_29","PRE1_30")
post_30mzp = c("POST1_1","POST1_2","POST1_3","POST1_4","POST1_5",
             "POST1_6","POST1_7","POST1_8","POST1_9","POST1_10",
             "POST1_11","POST1_12","POST1_13","POST1_14","POST1_15",
             "POST1_16","POST1_17","POST1_18","POST1_19","POST1_20",
             "POST1_21","POST1_22","POST1_23","POST1_24","POST1_25",
             "POST1_26","POST1_27","POST1_28","POST1_29","POST1_30")

# (1) 5-fold windows (EMA_5.5_Window)
# random sampling of 5-fold pre- and post-assessment windows (5 days 
# in a row) from individual 30-fold intervals (EMA_30.30)
EMA_5.5_Window = data.frame(ID = c(), 
  Pre_MZP1 = c(), Pre_MZP2 = c(), Pre_MZP3 = c(), Pre_MZP4 = c(),
  Pre_MZP5 = c(), Post_MZP1 = c(), Post_MZP2 = c(), Post_MZP3 = c(),
  Post_MZP4 = c(), Post_MZP5 = c(), PRE1_1 = c(), PRE1_2 = c(),
  PRE1_3 = c(), PRE1_4 = c(), PRE1_5 = c(), POST1_1 = c(),
  POST1_2 = c(), POST1_3 = c(), POST1_4 = c(), POST1_5 = c())

for (i in EMA_30.30$ID) {
  a = sample(1:26, 1)
  EMA_5.5_pre_Window = pre_30mzp[seq(from = a, to = a+4)]
  b = sample(1:26, 1)
  EMA_5.5_post_Window = post_30mzp[seq(from = b, to = b+4)]
  
  EMA_5.5_Window[i,"ID"] = i
  EMA_5.5_Window[i,"Pre_MZP1"] = EMA_5.5_pre_Window[1]
  EMA_5.5_Window[i,"Pre_MZP2"] = EMA_5.5_pre_Window[2]
  EMA_5.5_Window[i,"Pre_MZP3"] = EMA_5.5_pre_Window[3]
  EMA_5.5_Window[i,"Pre_MZP4"] = EMA_5.5_pre_Window[4]
  EMA_5.5_Window[i,"Pre_MZP5"] = EMA_5.5_pre_Window[5]
  EMA_5.5_Window[i,"Post_MZP1"] = EMA_5.5_post_Window[1]
  EMA_5.5_Window[i,"Post_MZP2"] = EMA_5.5_post_Window[2]
  EMA_5.5_Window[i,"Post_MZP3"] = EMA_5.5_post_Window[3]
  EMA_5.5_Window[i,"Post_MZP4"] = EMA_5.5_post_Window[4]
  EMA_5.5_Window[i,"Post_MZP5"] = EMA_5.5_post_Window[5]
  
  EMA_5.5_Window[i,"PRE1_1"] = EMA_30.30[i,EMA_5.5_pre_Window[1]]
  EMA_5.5_Window[i,"PRE1_2"] = EMA_30.30[i,EMA_5.5_pre_Window[2]]
  EMA_5.5_Window[i,"PRE1_3"] = EMA_30.30[i,EMA_5.5_pre_Window[3]]
  EMA_5.5_Window[i,"PRE1_4"] = EMA_30.30[i,EMA_5.5_pre_Window[4]]
  EMA_5.5_Window[i,"PRE1_5"] = EMA_30.30[i,EMA_5.5_pre_Window[5]]
  EMA_5.5_Window[i,"POST1_1"] = EMA_30.30[i,EMA_5.5_post_Window[1]]
  EMA_5.5_Window[i,"POST1_2"] = EMA_30.30[i,EMA_5.5_post_Window[2]]
  EMA_5.5_Window[i,"POST1_3"] = EMA_30.30[i,EMA_5.5_post_Window[3]]
  EMA_5.5_Window[i,"POST1_4"] = EMA_30.30[i,EMA_5.5_post_Window[4]]
  EMA_5.5_Window[i,"POST1_5"] = EMA_30.30[i,EMA_5.5_post_Window[5]]}

# (2) 5-fold single assessment days (EMA_5.5_Days)
# random sampling of 5-fold pre- and post-assessments (not necessarily
# days in a row) from individual 30-fold intervals (EMA_30.30)
EMA_5.5_Days = data.frame(ID = c(), 
  Pre_MZP1 = c(), Pre_MZP2 = c(), Pre_MZP3 = c(), Pre_MZP4 = c(),
  Pre_MZP5 = c(), Post_MZP1 = c(), Post_MZP2 = c(), Post_MZP3 = c(),
  Post_MZP4 = c(), Post_MZP5 = c(), PRE1_1 = c(), PRE1_2 = c(),
  PRE1_3 = c(), PRE1_4 = c(), PRE1_5 = c(), POST1_1 = c(),
  POST1_2 = c(), POST1_3 = c(), POST1_4 = c(), POST1_5 = c())

for (i in EMA_30.30$ID) {
  EMA_5.5_pre_Days = pre_30mzp[sort(sample(1:30, 5))]
  EMA_5.5_post_Days = post_30mzp[sort(sample(1:30, 5))]
  
  EMA_5.5_Days[i,"ID"] = i
  EMA_5.5_Days[i,"Pre_MZP1"] = EMA_5.5_pre_Days[1]
  EMA_5.5_Days[i,"Pre_MZP2"] = EMA_5.5_pre_Days[2]
  EMA_5.5_Days[i,"Pre_MZP3"] = EMA_5.5_pre_Days[3]
  EMA_5.5_Days[i,"Pre_MZP4"] = EMA_5.5_pre_Days[4]
  EMA_5.5_Days[i,"Pre_MZP5"] = EMA_5.5_pre_Days[5]
  EMA_5.5_Days[i,"Post_MZP1"] = EMA_5.5_post_Days[1]
  EMA_5.5_Days[i,"Post_MZP2"] = EMA_5.5_post_Days[2]
  EMA_5.5_Days[i,"Post_MZP3"] = EMA_5.5_post_Days[3]
  EMA_5.5_Days[i,"Post_MZP4"] = EMA_5.5_post_Days[4]
  EMA_5.5_Days[i,"Post_MZP5"] = EMA_5.5_post_Days[5]
  
  EMA_5.5_Days[i,"PRE1_1"] = EMA_30.30[i,EMA_5.5_pre_Days[1]]
  EMA_5.5_Days[i,"PRE1_2"] = EMA_30.30[i,EMA_5.5_pre_Days[2]]
  EMA_5.5_Days[i,"PRE1_3"] = EMA_30.30[i,EMA_5.5_pre_Days[3]]
  EMA_5.5_Days[i,"PRE1_4"] = EMA_30.30[i,EMA_5.5_pre_Days[4]]
  EMA_5.5_Days[i,"PRE1_5"] = EMA_30.30[i,EMA_5.5_pre_Days[5]]
  EMA_5.5_Days[i,"POST1_1"] = EMA_30.30[i,EMA_5.5_post_Days[1]]
  EMA_5.5_Days[i,"POST1_2"] = EMA_30.30[i,EMA_5.5_post_Days[2]]
  EMA_5.5_Days[i,"POST1_3"] = EMA_30.30[i,EMA_5.5_post_Days[3]]
  EMA_5.5_Days[i,"POST1_4"] = EMA_30.30[i,EMA_5.5_post_Days[4]]
  EMA_5.5_Days[i,"POST1_5"] = EMA_30.30[i,EMA_5.5_post_Days[5]]}
```

<!------------------------------------------------------------------------------------------------------------->
### R Code for the Calculation of Clinical Change Methods

#### Percentage Change {#r-pc}

Calculation of the Percentage Change method PC for interpreting the score difference between two assessment intervals (i.e. Mean Percentage Change), as well as between two single assessments for the questionnaire data set as an example (similar process for both the EMA and the questionnaire data set).

```{r label="r-pc", eval=FALSE}
pacman::p_load(dplyr)

### PP_5.5:
PP_5.5$Mean_PC = (1-(PP_5.5$POST_Mean / PP_5.5$PRE_Mean)) * 100

# creating the interpretation categories for Percentage Change,
# ranging from -2 (strong deterioration) to 2 (strong improvement):
PP_5.5 = PP_5.5 %>% 
  mutate(Mean_PC_klass = case_when(
    Mean_PC <= -50 ~ -2,
    Mean_PC > -50 & Mean_PC <= -25 ~ -1,
    Mean_PC > -25 & Mean_PC < 25 ~ 0,
    Mean_PC >= 25 & Mean_PC < 50 ~ 1,
    Mean_PC >= 50 ~ 2,
    TRUE ~ Mean_PC))

### PP_30.30:
PP_30.30$Mean_PC = (1-(PP_30.30$POST_Mean / PP_30.30$PRE_Mean)) * 100

# creating the interpretation categories for Percentage Change,
# ranging from -2 (strong deterioration) to 2 (strong improvement):
PP_30.30 = PP_30.30 %>% 
  mutate(Mean_PC_klass = case_when(
    Mean_PC <= -50 ~ -2,
    Mean_PC > -50 & Mean_PC <= -25 ~ -1,
    Mean_PC > -25 & Mean_PC < 25 ~ 0,
    Mean_PC >= 25 & Mean_PC < 50 ~ 1,
    Mean_PC >= 50 ~ 2,
    TRUE ~ Mean_PC))

### PP_1.1:
PP_1.1$PC = (1 - (PP_1.1$POST / PP_1.1$PRE)) * 100

# creating the interpretation categories for Percentage Change,
# ranging from -2 (strong deterioration) to 2 (strong improvement):
PP_1.1 = PP_1.1 %>% 
  mutate(PC_klass = case_when(
    PC <= -50 ~ -2,
    PC > -50 & PC <= -25 ~ -1,
    PC > -25 & PC < 25 ~ 0,
    PC >= 25 & PC < 50 ~ 1,
    PC >= 50 ~ 2,
    TRUE ~ as.numeric(PC)))
```

#### Clinical Significance {#r-csi}

Implementation of the Clinical Significance method CSI [see @McMillan.2010] for interpreting the score difference between two assessment intervals, as well as between two single assessments for the questionnaire data set as an example (similar process for both the EMA and the questionnaire data set).

```{r label="r-csi", eval=FALSE}
# creating the interpretation categories for Clinically Sig. Change,
# ranging from -1 (improvement) to 1 (deterioration):
pacman::p_load(dplyr)

### PP_5.5:
PP_5.5 = PP_5.5 %>% 
   mutate(CSI_klass = case_when(
     PRE_Mean >= 10 & POST_Mean <= 9 & Mean_PC >= 50 ~ -1,
     PRE_Mean <= 9 & POST_Mean >= 10 & Mean_PC <= -50 ~ 1,
     TRUE ~ 0))

### PP_30.30:
PP_30.30 = PP_30.30 %>% 
   mutate(CSI_klass = case_when(
     PRE_Mean >= 10 & POST_Mean <= 9 & Mean_PC >= 50 ~ -1,
     PRE_Mean <= 9 & POST_Mean >= 10 & Mean_PC <= -50 ~ 1,
     TRUE ~ 0))

### PP_1.1:
PP_1.1 = PP_1.1 %>% 
   mutate(CSI_klass = case_when(
     PRE >= 10 & POST <= 9 & PC >= 50 ~ -1,
     PRE <= 9 & POST >= 10 & PC <= -50 ~ 1,
     TRUE ~ 0))
```

#### Average Internal Consistency {#r-alpha}

Calculation of the average internal consistency Cronbach´s $\alpha$ as the estimate of reliability to be used to compute Reliable Change Indices. The population´s internal consistency of PHQ-9 assessments was first calculated within each 5-fold interval (pre and post). Then, $\alpha_{pre}$ and $\alpha_{post}$ were Fisher-Z transformed to take the average of both estimates, and finally this value was transformed back to obtain a pooled Cronbach´s $\alpha$. The same calculation method was used for questionnaire and EMA data sets.

```{r label="r-alpha", eval=FALSE}
pacman::p_load(DescTools)
PRE_alpha = CronbachAlpha(PP_5.5[pre_5mzp])
POST_alpha = CronbachAlpha(PP_5.5[post_5mzp])
PP_5.5_Alpha = FisherZInv(mean(c(FisherZ(PRE_alpha), 
                                 FisherZ(POST_alpha))))
```

#### Reliable Change Index [@Jacobson.1984; @Jacobson.1991] {#r-rci-jt}

Calculation of the Reliable Change Index \textit{RCI\textsubscript{JT}} and its population-level significance cutoff sensu @Jacobson.1984 and @Jacobson.1991 for the difference between two single assessments for the questionnaire data set as an example (similar process for both the EMA and the questionnaire data set).

```{r label="r-rci-jt", eval=FALSE}
pacman::p_load(dplyr)

PP_1.1$RCI_JT = (PP_1.1$POST - PP_1.1$PRE) / 
    sqrt(2 * (sd(PP_1.1$PRE) * sqrt(1 - PP_5.5_Alpha)) ^ 2)
RCI_JT_Cutoff = 1.96 * sqrt(2 * (sd(PP_1.1$PRE) * 
    sqrt(1 - PP_5.5_Alpha)) ^ 2)

# creating the interpretation categories for the RCI(JT), ranging 
# from -1 (reliable improvement) to 1 (reliable deterioration):
PP_1.1 = PP_1.1 %>% 
  mutate(RCI_JT_klass = case_when(
    PRE >= 10 & POST <= 9 & RCI_JT < -1.96 ~ -1,
    PRE <= 9 & POST >= 10 & RCI_JT > 1.96 ~ 1,
    TRUE ~ 0))
```

#### Individualized Reliable Change Index (Pre-SD) {#r-rci-ind-pre}

Calculation of a proposed Individualized Reliable Change Index \textit{RCI\textsubscript{ind,pre-SD}} and its corresponding individual significance cutoff for the difference between two assessment intervals, including the subject´s standard deviation from the baseline interval as a measure of individual variability. The same calculation method was used for both questionnaire and EMA data sets.

```{r label="r-rci-ind-pre", eval=FALSE}
pacman::p_load(dplyr)

### PP_5.5:
PP_5.5$SEd_pre = sqrt(2 * (PP_5.5$ind.pretestSD * 
                            sqrt(1 - PP_5.5_Alpha)) ^ 2)
PP_5.5$RCI_ind_preSD = (PP_5.5$POST_Mean - PP_5.5$PRE_Mean) / 
                            PP_5.5$SEd_pre
PP_5.5$RCI_ind_preSD_Cutoff =  1.96 * PP_5.5$SEd_pre

# creating the interpretation categories for the RCI(JT), ranging 
# from -1 (reliable improvement) to 1 (reliable deterioration):
PP_5.5 = PP_5.5 %>% 
  mutate(RCI_ind_preSD_klass = case_when(
    PRE_Mean >= 10 & POST_Mean <= 9 & RCI_ind_preSD < -1.96 ~ -1,
    PRE_Mean <= 9 & POST_Mean >= 10 & RCI_ind_preSD > 1.96 ~ 1,
    TRUE ~ 0))

### PP_30.30:
PP_30.30$SEd_pre = sqrt(2 * (PP_30.30$ind.pretestSD * 
                            sqrt(1 - PP_5.5_Alpha)) ^ 2)
PP_30.30$RCI_ind_preSD = (PP_30.30$POST_Mean - PP_30.30$PRE_Mean) / 
                            PP_30.30$SEd_pre
PP_30.30$RCI_ind_preSD_Cutoff =  1.96 * PP_30.30$SEd_pre

# creating the interpretation categories for the RCI(JT), ranging 
# from -1 (reliable improvement) to 1 (reliable deterioration):
PP_30.30 = PP_30.30 %>% 
  mutate(RCI_ind_preSD_klass = case_when(
    PRE_Mean >= 10 & POST_Mean <= 9 & RCI_ind_preSD < -1.96 ~ -1,
    PRE_Mean <= 9 & POST_Mean >= 10 & RCI_ind_preSD > 1.96 ~ 1,
    TRUE ~ 0))
```

<!--chapter:end:07-appendix.Rmd-->

