% This file was created with Citavi 6.6.0.0

@article{AafjesvanDoorn.2020,
 abstract = {Machine learning (ML) offers robust statistical and probabilistic techniques that can help to make sense of large amounts of data. This scoping review paper aims to broadly explore the nature of research activity using ML in the context of psychological talk therapies, highlighting the scope of current methods and considerations for clinical practice and directions for future research. Using a systematic search methodology, fifty-one studies were identified. A narrative synthesis indicates two types of studies, those who developed and tested an ML model (k=44), and those who reported on the feasibility of a particular treatment tool that uses an ML algorithm (k=7). Most model development studies used supervised learning techniques to classify or predict labeled treatment process or outcome data, whereas others used unsupervised techniques to identify clusters in the unlabeled patient or treatment data. Overall, the current applications of ML in psychotherapy research demonstrated a range of possible benefits for indications of treatment process, adherence, therapist skills and treatment response prediction, as well as ways to accelerate research through automated behavioral or linguistic process coding. Given the novelty and potential of this research field, these proof-of-concept studies are encouraging, however, do not necessarily translate to improved clinical practice (yet).},
 author = {{Aafjes-van Doorn}, Katie and Kamsteeg, C{\'e}line and Bate, Jordan and Aafjes, Marc},
 year = {2020},
 title = {A Scoping Review of Machine Learning in Psychotherapy Research},
 journal = {Psychotherapy Research},
 doi = {10.1080/10503307.2020.1808729},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32862761}
}


@article{Norman.1997,
 author = {Norman, Geoffrey R. and Stratford, Paul and Regehr, Glenn},
 year = {1997},
 title = {Methodological Problems in the Retrospective Computation of Responsiveness to Change: The Lesson of Cronbach},
 pages = {869--879},
 volume = {50},
 number = {8},
 journal = {Journal of clinical epidemiology}
}


@article{Olthof.2020,
 author = {Olthof, Merlijn and Hasselman, Fred and Strunk, Guido and {van Rooij}, Marieke and Aas, Benjamin and Helmich, Marieke A. and Schiepek, G{\"u}nter and Lichtwarck-Aschoff, Anna},
 year = {2020},
 title = {Critical Fluctuations as an Early-Warning Signal for Sudden Gains and Losses in Patients Receiving Psychotherapy for Mood Disorders},
 pages = {25--35},
 volume = {8},
 number = {1},
 issn = {2167-7026},
 journal = {Clinical Psychological Science},
 doi = {10.1177/2167702619865969}
}


@article{Parabiaghi.2005,
 author = {Parabiaghi, Alberto and Barbato, Angelo and D'avanzo, Barbara and Erlicher, Arcadio and Lora, Antonio},
 year = {2005},
 title = {Assessing Reliable and Clinically Significant Change on Health of the Nation Outcome Scales: Method for Displaying Longitudinal Data},
 url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1018.8757&rep=rep1&type=pdf},
 pages = {719--724},
 volume = {39},
 number = {8},
 journal = {Australian {\&} New Zealand Journal of Psychiatry}
}


@article{Pastore.2018,
 author = {Pastore, Massimiliano},
 year = {2018},
 title = {Overlapping: a R package for Estimating Overlapping in Empirical Distributions},
 pages = {1023},
 volume = {3},
 number = {32},
 journal = {The Journal of Open Source Software},
 doi = {10.21105/joss.01023}
}


@article{Pastore.2018b,
 author = {Pastore, Massimiliano},
 year = {2018},
 title = {Overlapping: A R Package for Estimating Overlapping in Empirical Distributions},
 volume = {3},
 number = {32},
 journal = {JOSS},
 doi = {10.21105/joss.01023}
}


@misc{Pettigrew.2015,
 author = {Pettigrew, Stephen},
 year = {2015},
 title = {Matching},
 url = {http://www.stephenpettigrew.com/teaching/gov2001/section11_2015.pdf}
}


@misc{RCoreTeam.2020,
 author = {{R Core Team}},
 year = {2020},
 title = {R: A Language and Environment for Statistical Computing},
 url = {https://www.R-project.org},
 address = {Vienna, Austria},
 institution = {{R Foundation for Statistical Computing}}
}


@book{Rast.2020,
 author = {Rast, Philippe and Martin, Stephen Ross and Liu, Siwei and Williams, Donald Ray},
 year = {2020},
 title = {A New Frontier for Studying Within-Person Variability: Bayesian Multivariate Generalized Autoregressive Conditional Heteroskedasticity Models},
 url = {osf.io/nef2q},
 doi = {10.31234/osf.io/j57pk}
}


@article{Raymond.2006,
 abstract = {OBJECTIVE

This paper compares four techniques used to assess change in neuropsychological test scores before and after coronary artery bypass graft surgery (CABG), and includes a rationale for the classification of a patient as overall impaired.

METHODS

A total of 55 patients were tested before and after surgery on the MicroCog neuropsychological test battery. A matched control group underwent the same testing regime to generate test-retest reliabilities and practice effects. Two techniques designed to assess statistical change were used: the Reliable Change Index (RCI), modified for practice, and the Standardised Regression-based (SRB) technique. These were compared against two fixed cutoff techniques (standard deviation and 20{\%} change methods).

RESULTS

The incidence of decline across test scores varied markedly depending on which technique was used to describe change. The SRB method identified more patients as declined on most measures. In comparison, the two fixed cutoff techniques displayed relatively reduced sensitivity in the detection of change.

CONCLUSIONS

Overall change in an individual can be described provided the investigators choose a rational cutoff based on likely spread of scores due to chance. A cutoff value of {\textgreater} or =20{\%} of test scores used provided acceptable probability based on the number of tests commonly encountered. Investigators must also choose a test battery that minimises shared variance among test scores.},
 author = {Raymond, Paul D. and Hinton-Bayre, Anton D. and Radel, Michael and Ray, Michael J. and Marsh, Neville A.},
 year = {2006},
 title = {Assessment of Statistical Change Criteria Used to Define Significant Change in Neuropsychological Test Performance Following Cardiac Surgery},
 pages = {82--88},
 volume = {29},
 number = {1},
 journal = {European Journal of Cardio-Thoracic Surgery},
 doi = {10.1016/j.ejcts.2005.10.016},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/16337395}
}


@misc{Revelle.2020,
 author = {Revelle, William},
 year = {2020},
 title = {psych: Procedures for Psychological, Psychometric, and Personality Research: R package version 2.0.8},
 url = {https://CRAN.R-project.org/package=psych},
 address = {Evanston, Illinois},
 institution = {{Northwestern University}}
}


@misc{Revelle.2020b,
 author = {Revelle, William},
 year = {2020},
 title = {psych: Procedures for Psychological, Psychometric, and Personality Research},
 url = {https://CRAN.R-project.org/package=psych},
 address = {Evanston, Illinois},
 institution = {{Northwestern University}}
}


@article{Roberts.2008,
 abstract = {Recent longitudinal and cross-sectional aging research has shown that personality traits continue to change in adulthood. In this article, we review the evidence for mean-level change in personality traits, as well as for individual differences in change across the life span. In terms of mean-level change, people show increased selfconfidence, warmth, self-control, and emotional stability with age. These changes predominate in young adulthood (age 20-40). Moreover, mean-level change in personality traits occurs in middle and old age, showing that personality traits can change at any age. In terms of individual differences in personality change, people demonstrate unique patterns of development at all stages of the life course, and these patterns appear to be the result of specific life experiences that pertain to a person's stage of life.},
 author = {Roberts, Brent W. and Mroczek, Daniel},
 year = {2008},
 title = {Personality Trait Change in Adulthood},
 pages = {31--35},
 volume = {17},
 number = {1},
 journal = {Current directions in psychological science},
 doi = {10.1111/j.1467-8721.2008.00543.x},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2743415},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/19756219}
}


@article{Rot.2012,
 abstract = {In the past two decades, the study of mood disorder patients using experience sampling methods (ESM) and ecological momentary assessment (EMA) has yielded important findings. In patients with major depressive disorder (MDD), the dynamics of their everyday mood have been associated with various aspects of their lives. To some degree similar studies have been conducted in patients with bipolar disorder (BD). In this paper we present the results of a systematic review of all ESM/EMA studies in MDD and BD to date. We focus not only on the correlates of patients' everyday mood but also on the impact on treatment, residual symptoms in remitted patients, on findings in pediatric populations, on MDD/BD specificity, and on links with neuroscience. After reviewing these six topics, we highlight the benefits of ESM/EMA for researchers, clinicians, and patients, and offer suggestions for future studies.},
 author = {aan het Rot, Marije and Hogenelst, Koen and Schoevers, Robert A.},
 year = {2012},
 title = {Mood Disorders in Everyday Life: A Systematic Review of Experience Sampling and Ecological Momentary Assessment Studies},
 pages = {510--523},
 volume = {32},
 number = {6},
 journal = {Clinical psychology review},
 doi = {10.1016/j.cpr.2012.05.007},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/22721999}
}


@article{Rozental.2019,
 abstract = {Background: Negative effects of psychological treatments have recently received increased attention in both research and clinical practice. Most investigations have focused on determining the occurrence and characteristics of deterioration and other adverse and unwanted events, such as interpersonal issues, indicating that patients quite frequently experience such incidents in treatment. However, non-response is also negative if it might have prolonged an ongoing condition and caused unnecessary suffering. Yet few attempts have been made to directly explore non-response in psychological treatment or its plausible causes. Internet-based cognitive behavior therapy (ICBT) has been found effective for a number of diagnoses but has not yet been systematically explored with regard to those patients who do not respond. Methods: The current study collected and aggregated data from 2,866 patients in 29 clinical randomized trials of ICBT for three categories of diagnoses: anxiety disorders, depression, and other (erectile dysfunction, relationship problems, and gambling disorder). Raw scores from each patient variable were used in an individual patient data meta-analysis to determine the rate of non-response on the primary outcome measure for each clinical trial, while its potential predictors were examined using binomial logistic regression. The reliable change index (RCI) was used to classify patients as non-responders. Results: Of the 2,118 patients receiving treatment, and when applying a RCI of z $\geq$ 1.96, 567 (26.8{\%}) were classified as non-responders. In terms of predictors, patients with higher symptom severity on the primary outcome measure at baseline, Odds Ratio (OR) = 2.04, having a primary anxiety disorder (OR = 5.75), and being of male gender (OR = 1.80), might have higher odds of not responding to treatment. Conclusion: Non-response seems to occur among approximately a quarter of all patients in ICBT, with predictors related to greater symptoms, anxiety disorders, and gender indicating increasing the odds of not responding. However, the results need to be replicated before establishing their clinical relevance, and the use of the RCI as a way of determining non-response needs to be validated by other means, such as by interviewing patients classified as non-responders.},
 author = {Rozental, Alexander and Andersson, Gerhard and Carlbring, Per},
 year = {2019},
 title = {In the Absence of Effects: An Individual Patient Data Meta-Analysis of Non-Response and its Predictors in Internet-Based Cognitive Behavior Therapy},
 volume = {10},
 journal = {Frontiers in Psychology},
 doi = {10.3389/fpsyg.2019.00589},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30984061},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6450428}
}


@article{Rozental.2018,
 abstract = {Background: Procrastination can be stressful and frustrating, but it seldom causes any major distress. However, for some people, it can become problematic, resulting in anxiety, lowered mood, physical complaints, and decreased well-being. Still, few studies have investigated the benefits of targeting procrastination. In addition, no attempt has previously been made to determine the overall efficacy of providing psychological treatments. Methods: A systematic review and meta-analysis was conducted by searching for eligible records in Scopus, Proquest, and Google Scholar. Only randomized controlled trials comparing psychological treatments for procrastination to an inactive comparator and assessing the outcomes by a self-report measure were included. A random effects model was used to determine the standardized mean difference Hedge's g at post-treatment. Furthermore, test for heterogeneity was performed, fail-safe N was calculated, and the risk of bias was explored. The study was pre-registered at Prospero: CRD42017069981. Results: A total of 1,639 records were identified, with 12 studies (21 comparisons, N = 718) being included in the quantitative synthesis. Overall effect size g when comparing treatment to control was 0.34, 95{\%} Confidence Interval [0.11, 0.56], but revealing significant heterogeneity, Q(20) = 46.99, p {\textless} 0.00, and I2 = 61.14{\%}, 95{\%} CI [32.83, 84.24]. Conducting a subgroup analysis of three out of four studies using cognitive behavior therapy (CBT) found an effect size g of 0.55, 95{\%} CI [0.32, 0.77], and no longer showing any heterogeneity, Q(4) = 3.92, p = 0.42, I2 = 0.00{\%}, 95{\%} CI [0.00, 91.02] (N = 236). Risk of publication bias, as assessed by the Egger's test was not significant, z = -1.05, p = 0.30, fail-safe N was 370 studies, and there was some risk of bias as rated by two independent researchers. In terms of secondary outcomes, the self-report measures were too varied to present an aggregated estimate. Conclusions: Psychological treatments seem to have small benefits on procrastination, but the studies displayed significant between-study variation. Meanwhile, CBT was associated with a moderate benefit, but consisted of only three studies. Recommendations for future research are provided, including the use of more valid and reliable outcomes and a screening interview at intake.},
 author = {Rozental, Alexander and Bennett, Sophie and Forsstr{\"o}m, David and Ebert, David D. and Shafran, Roz and Andersson, Gerhard and Carlbring, Per},
 year = {2018},
 title = {Targeting Procrastination Using Psychological Treatments: A Systematic Review and Meta-Analysis},
 volume = {9},
 journal = {Frontiers in Psychology},
 doi = {10.3389/fpsyg.2018.01588},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30214421},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6125391}
}


@article{Rozental.2017,
 abstract = {OBJECTIVE

Psychological treatments can relieve mental distress and improve well-being, and the dissemination of evidence-based methods can help patients gain access to the right type of aid. Meanwhile, Internet-based cognitive-behavioral therapy (ICBT) has shown promising results for many psychiatric disorders. However, research on the potential for negative effects of psychological treatments has been lacking.

METHOD

An individual patient data meta-analysis of 29 clinical trials of ICBT (N = 2,866) was performed using the Reliable Change Index for each primary outcome measures to distinguish deterioration rates among patients in treatment and control conditions. Statistical analyses of predictors were conducted using generalized linear mixed models. Missing data was handled by multiple imputation.

RESULTS

Deterioration rates were 122 (5.8{\%}) in treatment and 130 (17.4{\%}) in control conditions. Relative to receiving treatment, patients in a control condition had higher odds of deteriorating, odds ratios (ORs) = 3.10, 95{\%} confidence interval (CI) [2.21, 4.34]. Clinical severity at pretreatment was related to lower odds, OR = 0.62, 95{\%} CI [0.50, 0.77], and OR = 0.51, 95{\%} CI [0.51, 0.80], for treatment and control conditions. In terms of sociodemographic variables, being in a relationship, OR = 0.58, 95{\%} CI [0.35, 0.95], having at least a university degree, OR = 0.54, 95{\%} CI [0.33, 0.88], and being older, OR = 0.78, 95{\%} CI, [0.62, 0.98], were also associated with lower odds of deterioration, but only for patients assigned to a treatment condition.

CONCLUSION

Deterioration among patients receiving ICBT or being in a control condition can occur and should be monitored by researchers to reverse and prevent a negative treatment trend. (PsycINFO Database Record},
 author = {Rozental, Alexander and Magnusson, Kristoffer and Boettcher, Johanna and Andersson, Gerhard and Carlbring, Per},
 year = {2017},
 title = {For Better or Worse: An Individual Patient Data Meta-Analysis of Deterioration Among Participants Receiving Internet-Based Cognitive Behavior Therapy},
 pages = {160--177},
 volume = {85},
 number = {2},
 issn = {0022-006X},
 journal = {Journal of consulting and clinical psychology},
 doi = {10.1037/ccp0000158},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/27775414}
}


@article{Ruwaard.2018,
 author = {Ruwaard, J. and Kooistra, L. and Thong, M.},
 year = {2018},
 title = {Ecological Momentary Assessment in Mental Health Research: A Practical Introduction With Examples in R},
 url = {https://www.researchgate.net/publication/329197814},
 journal = {EMA Research Manual}
}


@article{Nelson.2015,
 abstract = {BACKGROUND

Neurocognitive testing is widely performed for the assessment of concussion. Athletic trainers can use preseason baselines with reliable change indices (RCIs) to ascertain whether concussed athletes' cognitive abilities are below preinjury levels. Although the percentage of healthy individuals who show decline on any individual test is determined by its RCI's confidence level (eg, 10{\%} false-positive rate using an RCI with an 80{\%} confidence interval), the expected rate of 1 or more significant RCIs across multiple indices is unclear.

OBJECTIVE

To use a Monte Carlo simulation procedure to estimate the normal rate (ie, base rate) of significant decline on 1 or more RCIs in multitest batteries.

RESULTS {\&} CONCLUSION

For batteries producing 7 or more uncorrelated RCIs (80{\%} confidence intervals), the majority of normal individuals would show significant declines on at least 1 RCI. Expected rates are lower for tests with fewer indices, higher inter-RCI correlations, and more stringent impairment criteria. These reference points can help testers interpret RCI output for multitest batteries.},
 author = {Nelson, Lindsay D.},
 year = {2015},
 title = {False-Positive Rates of Reliable Change Indices for Concussion Test Batteries: A Monte Carlo Simulation},
 pages = {1319--1322},
 volume = {50},
 number = {12},
 journal = {Journal of athletic training},
 doi = {10.4085/1062-6050-51.1.09},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4741259},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26678291}
}


@article{Nelson.2015b,
 abstract = {Screening and diagnostic procedures often require a physician's subjective interpretation of a patient's test result using an ordered categorical scale to define the patient's disease severity. Because of wide variability observed between physicians' ratings, many large-scale studies have been conducted to quantify agreement between multiple experts' ordinal classifications in common diagnostic procedures such as mammography. However, very few statistical approaches are available to assess agreement in these large-scale settings. Many existing summary measures of agreement rely on extensions of Cohen's kappa. These are prone to prevalence and marginal distribution issues, become increasingly complex for more than three experts, or are not easily implemented. Here we propose a model-based approach to assess agreement in large-scale studies based upon a framework of ordinal generalized linear mixed models. A summary measure of agreement is proposed for multiple experts assessing the same sample of patients' test results according to an ordered categorical scale. This measure avoids some of the key flaws associated with Cohen's kappa and its extensions. Simulation studies are conducted to demonstrate the validity of the approach with comparison with commonly used agreement measures. The proposed methods are easily implemented using the software package R and are applied to two large-scale cancer agreement studies.},
 author = {Nelson, Kerrie P. and Edwards, Don},
 year = {2015},
 title = {Measures of Agreement Between Many Raters for Ordinal Classifications},
 pages = {3116--3132},
 volume = {34},
 number = {23},
 journal = {Statistics in medicine},
 doi = {10.1002/sim.6546},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4560692},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26095449}
}


@article{Moore.2016,
 abstract = {As mobile data capture tools for patient-reported outcomes proliferate in clinical research, a key dimension of measure performance is sensitivity to change. This study compared performance of patient-reported measures of mindfulness, depression, and anxiety symptoms using traditional paper-and-pencil forms versus real-time, ambulatory measurement of symptoms via ecological momentary assessment (EMA). Sixty-seven emotionally distressed older adults completed paper-and-pencil measures of mindfulness, depression, and anxiety along with two weeks of identical items reported during ambulatory monitoring via EMA before and after participation in a randomized trial of Mindfulness-Based Stress Reduction (MBSR) or a health education intervention. We calculated effect sizes for these measures across both measurement approaches and estimated the Number-Needed-to-Treat (NNT) in both measurement conditions. Study outcomes greatly differed depending on which measurement method was used. When EMA was used to measure clinical symptoms, older adults who participated in the MBSR intervention had significantly higher mindfulness and significantly lower depression and anxiety than participants in the health education intervention at post-treatment. However, these significant changes in symptoms were not found when outcomes were measured with paper-and-pencil measures. The NNT for mindfulness and depression measures administered through EMA were approximately 25-50{\%} lower than NNTs derived from paper-and-pencil administration. Sensitivity to change in anxiety was similar across administration modes. In conclusion, EMA measures of depression and mindfulness substantially outperformed paper-and-pencil measures with the same items. The additional resources associated with EMA in clinical trials would seem to be offset by its greater sensitivity to detect change in key outcome variables.},
 author = {Moore, Raeanne C. and Depp, Colin A. and Wetherell, Julie Loebach and Lenze, Eric J.},
 year = {2016},
 title = {Ecological Momentary Assessment Versus Standard Assessment Instruments for Measuring Mindfulness, Depressed Mood, and Anxiety Among Older Adults},
 pages = {116--123},
 volume = {75},
 journal = {Journal of psychiatric research},
 doi = {10.1016/j.jpsychires.2016.01.011},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4769895},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26851494}
}


@article{Mocenni.2019,
 author = {Mocenni, Chiara and Montefrancesco, Giuseppe and Tiezzi, Silvia},
 year = {2019},
 title = {A Model of Spontaneous Remission From Addiction},
 pages = {21--48},
 volume = {8},
 number = {1},
 issn = {2160-9802},
 journal = {International Journal of Applied Behavioral Economics},
 doi = {10.4018/IJABE.2019010102}
}


@article{Lambert.2009,
 abstract = {It is recommended that an estimate of clinical significance be included in all psychotherapy outcome studies and that this estimate be based on the work of Jacobson and Truax (1991). The concept of clinical significance is defined and put in the context of broadly accepted statistical methods along with its advantages and a rationale for using the Jacobson methods. One implication of this recommendation is that the use of the term will have a standard meaning. Examples of loss of meaning are provided and suggest that conclusions about best practices will be affected unless such a voluntary step is taken. Some problems with the concept of clinical significance are noted and a call for validity studies is made.},
 author = {Lambert, Michael J. and Ogles, Benjamin M.},
 year = {2009},
 title = {Using Clinical Significance in Psychotherapy Outcome Research: The Need for a Common Procedure and Validity Data},
 pages = {493--501},
 volume = {19},
 number = {4-5},
 journal = {Psychotherapy Research},
 doi = {10.1080/10503300902849483},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/20183403}
}


@article{Lamers.2008,
 abstract = {OBJECTIVE

To assess the psychometric properties of the Patient Health Questionnaire-9 (PHQ-9) as a screening instrument for depression in elderly patients with diabetes mellitus (DM) and chronic obstructive pulmonary disease (COPD) without known depression.

STUDY DESIGN AND SETTING

DM and COPD patients aged {\textgreater}59 years were selected from general practices. A test-retest was conducted in 105 patients. Criterion validity, using the Mini International Neuropsychiatric Interview psychiatric interview to diagnose major depressive disorder (MDD) and any depressive disorder (ADD) as diagnostic standard, was evaluated for both summed and algorithm-based PHQ-9 score in 713 patients. Correlations with quality of life and severity of illness were calculated to assess construct validity.

RESULTS

Cohen's kappa for the algorithm-based score was 0.71 for MDD and 0.69 for ADD. Correlation for test-retest assessment of the summed score was 0.91. The algorithm-based score had low sensitivity and high specificity, but both sensitivity and specificity were high for the optimal cut-off point of 6 on the summed score for ADD (Se 95.6{\%}, Sp 81.0{\%}). Correlations between summed score and quality of life and severity of illness were acceptable.

CONCLUSION

The summed PHQ-9 score seems a valid and reliable screening instrument for depression in elderly primary care patients with DM and COPD.},
 author = {Lamers, Femke and Jonkers, Catharina C. M. and Bosma, Hans and Penninx, Brenda W. J. H. and Knottnerus, J. Andr{\'e} and {van Eijk}, Jacques Th M.},
 year = {2008},
 title = {Summed Score of the Patient Health Questionnaire-9 was a Reliable and Valid Method for Depression Screening in Chronically ill Elderly Patients},
 pages = {679--687},
 volume = {61},
 number = {7},
 journal = {Journal of clinical epidemiology},
 doi = {10.1016/j.jclinepi.2007.07.018},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/18538262}
}


@article{Lane.2018,
 author = {Lane, Sean P. and Hennes, Erin P.},
 year = {2018},
 title = {Power Struggles: Estimating Sample Size for Multilevel Relationships Research},
 pages = {7--31},
 volume = {35},
 number = {1},
 issn = {0265-4075},
 journal = {Journal of Social and Personal Relationships},
 doi = {10.1177/0265407517710342}
}


@article{Liu.2014,
 author = {Liu, Fang and Li, Qing},
 year = {2014},
 title = {Exact Sequential Test of Equivalence Hypothesis Based on Bivariate Non-Central t-Statistics},
 pages = {14--24},
 volume = {77},
 issn = {01679473},
 journal = {Computational Statistics {\&} Data Analysis},
 doi = {10.1016/j.csda.2014.02.007}
}


@article{Liu.2019,
 abstract = {When planning mediation studies, researchers are often interested in the sample size needed to achieve adequate power for testing mediation. Power depends on population effect sizes, which are unknown in practice. In conventional power analysis, effect size estimates, however, are often used as population values, which could result in underpowered studies. Uncertainty in effect size estimates has been considered in other sample size planning contexts (e.g., t-test, ANOVA), but has not been handled properly for planning mediation studies. In the current study, we proposed an easy-to-use sample size planning method for testing mediation with uncertainty in effect size estimates considered. We conducted simulation studies to demonstrate the impact of uncertainty in effect size estimates on power of testing mediation, and to provide sample size suggestions under different levels of uncertainty. Empirical examples were provided to illustrate the application of our method. R functions and a web application were developed to facilitate implementation.},
 author = {Liu, Xiao and Wang, Lijuan},
 year = {2019},
 title = {Sample Size Planning for Detecting Mediation Effects: A Power Analysis Procedure Considering Uncertainty in Effect Size Estimates},
 pages = {822--839},
 volume = {54},
 number = {6},
 journal = {Multivariate behavioral research},
 doi = {10.1080/00273171.2019.1593814},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30983425}
}


@article{Liu.2016,
 abstract = {Daily diaries and other everyday experience methods are increasingly used to study relationships between two time-varying variables X and Y. Although daily data potentially often have weekly cyclical patterns (e.g., stress may be higher on weekdays and lower on weekends), the majority of daily diary studies have ignored this possibility. In this study, we investigated the effect of ignoring existing weekly cycles. We reanalyzed an empirical dataset (stress and alcohol consumption) and performed Monte Carlo simulations to investigate the impact of omitting weekly cycles. In the empirical dataset, ignoring cycles led to the inference of a significant within-person X-Y relation whereas modeling cycles suggested that this relationship did not exist. Simulation results indicated that ignoring cycles that existed in both X and Y led to bias in the estimated within-person X-Y relationship. The amount and direction of bias depended on the magnitude of the cycles, magnitude of the true within-person X-Y relation, and synchronization of the cycles. We encourage researchers conducting daily diary studies to address potential weekly cycles in their data. We provide guidelines for detecting and modeling cycles to remove their influence and discuss challenges of causal inference in daily experience studies.},
 author = {Liu, Yu and West, Stephen G.},
 year = {2016},
 title = {Weekly Cycles in Daily Report Data: An Overlooked Issue},
 pages = {560--579},
 volume = {84},
 number = {5},
 journal = {Journal of personality},
 doi = {10.1111/jopy.12182},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/25973649}
}


@article{Longwell.2005,
 author = {Longwell, B. Thomas and Truax, Paula},
 year = {2005},
 title = {The Differential Effects of Weekly, Monthly, and Bimonthly Administrations of the Beck Depression Inventory-II: Psychometric Properties and Clinical Implications},
 pages = {265--275},
 volume = {36},
 number = {3},
 journal = {Behavior Therapy}
}


@article{Luedtke.2019,
 author = {Luedtke, Alex and Sadikova, Ekaterina and Kessler, Ronald C.},
 year = {2019},
 title = {Sample Size Requirements for Multivariate Models to Predict Between-Patient Differences in Best Treatments of Major Depressive Disorder},
 pages = {445--461},
 volume = {7},
 number = {3},
 issn = {2167-7026},
 journal = {Clinical Psychological Science},
 doi = {10.1177/2167702618815466}
}


@article{Schaffer.2013,
 abstract = {Standard clinical trial methodology in depression does not allow for careful examination of early changes in symptom intensity. The purpose of this study was to use daily {\textquotedbl}Mental Health Telemetry{\textquotedbl} (MHT) to prospectively record change in depressive and anxiety symptoms for depressed patients receiving augmentation treatment, and determine the extent and predictive capacity of early changes. We report results of a 6-week, open-label study of the addition of quetiapine XR (range, 50-300 mg) for adult patients (n = 26) with major depressive disorder who were nonresponsive to antidepressant treatment. In addition to regular study visits, all participants completed daily, wirelessly transmitted self-report ratings of symptoms on a Smartphone. Daily and 3-day moving average mean scores were calculated, and associations between early symptom change and eventual response to treatment were determined. Improvement in depressive and anxiety symptoms was identified as early as day 1 of treatment. Of the total decline in depression severity over 6 weeks, 9{\%} was present at day 1, 28{\%} at day 2, 39{\%} at days 3 and 4, 65{\%} at day 7, and 80{\%} at day 10. Self-report rating of early improvement ($\geq$20{\%}) in depressive symptoms at day 7 significantly predicted responder status at week 6 (P = 0.03). Clinician-rated depressive and anxiety symptoms only became significantly associated with responder status at day 14. In conclusion, very early changes in depressive symptoms were identified using MHT, early changes accounted for most of total change, and MHT-recorded improvement as early as day 7 significantly predicted response to treatment at study end point.},
 author = {Schaffer, Ayal and Kreindler, David and Reis, Catherine and Levitt, Anthony J.},
 year = {2013},
 title = {Use of Mental Health Telemetry to Enhance Identification and Predictive Value of Early Changes During Augmentation Treatment of Major Depression},
 pages = {775--781},
 volume = {33},
 number = {6},
 journal = {Journal of clinical psychopharmacology},
 doi = {10.1097/JCP.0b013e31829e8359},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/24100787}
}


@article{Maassen.2006,
 abstract = {In his comments on our previous article, Hinton-Bayre advocates the use of the regression based approach in most cases of determining reliable change. This article comments on Hinton-Bayre's argument, discusses cases where the regression method might not be the preferred method, and presents adjustments that make the method more generally preferable.},
 author = {Maassen, G. H. and Bossema, E. R. and Brand, N.},
 year = {2006},
 title = {Reliable Change Assessment With Practice Effects in Sport Concussion Research: A Comment on Hinton-Bayre},
 pages = {829--833},
 volume = {40},
 number = {10},
 journal = {British journal of sports medicine},
 doi = {10.1136/bjsm.2005.023713},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/16926260},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2465072}
}


@article{Maassen.2004,
 author = {Maassen, Gerard H.},
 year = {2004},
 title = {The Standard Error in the Jacobson and Truax Reliable Change Index (the Classical Approach to the Assessment of Reliable Change)},
 url = {https://dspace.library.uu.nl/bitstream/handle/1874/11263/Maassen_The_standard_error.pdf?sequence=2},
 pages = {888--893},
 volume = {10},
 journal = {Journal of the International Neuropsychological Society}
}


@article{Maassen.2009,
 abstract = {In this article the outcomes of three indices for the assessment of reliable change (RCIs) are compared: the null hypothesis method of Chelune, Naugle, Luders, Sedlak, and Awad (1993), the regression-based method of McSweeny, Naugle, Chelune, and Luders (1993), and a recently proposed adjustment to the latter procedure (Maassen, 2003). Simulated data demonstrated the importance of using large control samples. The regression-based method proved to be the most lenient in designating individuals as reliably changed, resulting in the most correct and the most incorrect designations. The adjusted procedure resulted in fewer correct designations and the lowest numbers of incorrect designations. Real-world data showed the same patterns.},
 author = {Maassen, Gerard H. and Bossema, Ercolie and Brand, Nico},
 year = {2009},
 title = {Reliable Change and Practice Effects: Outcomes of Various Indices Compared},
 pages = {339--352},
 volume = {31},
 number = {3},
 issn = {1380-3395},
 journal = {Journal of Clinical and Experimental Neuropsychology},
 doi = {10.1080/13803390802169059},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/18618359}
}


@article{McCarthy.2015,
 abstract = {AIMS

Ecological Momentary Assessment (EMA) captures real-time reports in subjects' natural environments. This experiment manipulated EMA frequency to estimate effects on abstinence and peri-cessation subjective experiences.

DESIGN

In this randomized trial, subjects had an equal chance of being assigned to low-frequency (once) or high-frequency (six times) daily EMA for 4 weeks (1 week pre- and 3 weeks post-cessation). Participants completed six office visits over 5 weeks and 6- and 12-week follow-up telephone interviews.

SETTING

Community participants were recruited from central New Jersey, USA.

PARTICIPANTS

One hundred and ten adult daily smokers seeking to quit smoking were included in intent-to-treat analyses of tobacco abstinence; 94 were available for secondary analyses of peri-cessation subjective ratings.

MEASUREMENTS

Primary outcomes were cessation (abstaining at least 24 hours within 2 weeks of attempting to quit) and prolonged abstinence (no relapse between weeks 2 and 12 post-quit). Secondary outcomes were mean levels and growth in ratings of cigarette craving, affect and quitting motivation and self-efficacy.

FINDINGS

EMA frequency was unrelated to cessation (odds ratio = 1.367, 95{\%} confidence interval = 0.603-3.098) or prolonged abstinence (odds ratio = 1.040, 95{\%} confidence interval = 0.453-2.388) in intent-to-treat analyses. High-frequency EMA was associated with lower craving (B = -0.544, standard error (SE) = 0.183, P = 0.004, anxiety (B = -0.424, SE = 0.170, P = 0.015), anger (B = -0.474, SE = 0.139, P = 0.001), hunger (B = -0.388, SE = 0.170, P = 0.025) and positive affect (B = -0.430, SE = 0.196, P = 0.03).

CONCLUSIONS

In smokers trying to quit, more frequent ecological momentary assessment self-monitoring results in lower craving, anxiety, anger, hunger and positive affect. It is not clear whether this translates into higher rates of smoking abstinence.},
 author = {McCarthy, Danielle E. and Minami, Haruka and Yeh, Vivian M. and Bold, Krysten W.},
 year = {2015},
 title = {An Experimental Investigation of Reactivity to Ecological Momentary Assessment Frequency Among Adults Trying to Quit Smoking},
 pages = {1549--1560},
 volume = {110},
 number = {10},
 journal = {Addiction},
 doi = {10.1111/add.12996},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26011583},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4565778}
}


@article{McGlinchey.2002,
 author = {McGlinchey, Joseph B. and Atkins, David C. and Jacobson, Neil S.},
 year = {2002},
 title = {Clinical significance methods: Which one to use and how useful are they?},
 pages = {529--550},
 volume = {33},
 number = {4},
 journal = {Behavior Therapy},
 doi = {10.1016/S0005-7894(02)80015-6}
}


@article{McMillan.2010,
 abstract = {BACKGROUND

Although the PHQ-9 is widely used in primary care, little is known about its performance in quantifying improvement. The original validation study of the PHQ-9 defined clinically significant change as a post-treatment score of $\leq$9 combined with improvement of 50{\%}, but it is unclear how this relates to other theoretically informed methods of defining successful outcome. We compared a range of definitions of clinically significant change (original definition, asymptomatic criterion, reliable and clinically significant change criteria a, b and c) in a clinical trial of a community-level depression intervention.

METHOD

Randomised Control Trial of collaborative care for depression. Levels of agreement were calculated between the standard definition, other definitions, and gold-standard diagnostic interview.

RESULTS

The standard definition showed good agreement (kappa{\textgreater}0.60) with the other definitions and had moderate, though acceptable, agreement with the diagnostic interview (kappa=0.58). The standard definition corresponded closely to reliable and clinically significant change criterion c, the recommended method of quantifying improvement when clinical and non-clinical distributions overlap.

LIMITATIONS

The absence of follow-up data meant that an asymptomatic criterion rather than remission or recovery criteria were used.

CONCLUSION

The close agreement between the standard definition and reliable and clinically significant change criterion c provides some support for the standard definition of improvement. However, it may be preferable to use a reliable change index rather than 50{\%} improvement. Remission status, based on the asymptomatic range and a lower PHQ-9 score, may provide a useful additional category of clinical change.},
 author = {McMillan, Dean and Gilbody, Simon and Richards, David},
 year = {2010},
 title = {Defining successful treatment outcome in depression using the PHQ-9: a comparison of methods},
 pages = {122--129},
 volume = {127},
 number = {1-3},
 journal = {Journal of affective disorders},
 doi = {10.1016/j.jad.2010.04.030},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/20569992}
}


@book{Meehl.1954,
 author = {Meehl, Paul E.},
 year = {1954},
 title = {Clinical Versus Statistical Prediction: A Theoretical Analysis and a Review of the Evidence},
 publisher = {{University of Minnesota Press}}
}


@book{Mehl.2012,
 abstract = {Machine generated contents note: I.THEORETICAL BACKGROUND -- 1.Why Researchers Should Think {\textquotedbl}Real-World{\textquotedbl}: A Conceptual Rationale / Harry T. Reis -- 2.Why Researchers Should Think {\textquotedbl}Real-Time{\textquotedbl}: A Cognitive Rationale / Norbert Schwarz -- 3.Why Researchers Should Think {\textquotedbl}Within-Person{\textquotedbl}: A Paradigmatic Rationale / Ellen L. Hamaker -- 4.Conducting Research in Daily Life: A Historical Review / Kurt Pawlik -- II.STUDY DESIGN CONSIDERATIONS AND METHODS OF DATA COLLECTION -- 5.Getting Started: Launching a Study in Daily Life / Barbara J. Lehman -- 6.Measurement Reactivity in Diary Research / Mark D. Litt -- 7.Computerized Sampling of Experiences and Behavior / Katharina Krog -- 8.Daily Diary Methods / Susan J. Wenze -- 9.Event-Contingent Recording / Gentiana Sadikaj -- 10.Naturalistic Observation Sampling: The Electronically Activated Recorder (EAR) / Megan L. Robbins -- 11.Ambulatory Psychoneuroendocrinology: Assessing Salivary Cortisol and Other Hormones in Daily Life / Wolff Schlotz -- 12.Bridging the Gap between the Laboratory and the Real World: Integrative Ambulatory Psychophysiology / Maren I. Muller -- 13.Ambulatory Assessment of Movement Behavior: Methodology, Measurement, and Application / Ulrich W. Ebner-Priemer -- 14.Passive Telemetric Monitoring: Novel Methods for Real-World Behavioral Assessment / Matthew S. Goodwin -- 15.Emerging Technology for Studying Daily Life / Stephen S. Intille -- III.DATA-ANALYTIC METHODS -- 16.Power Analysis for Intensive Longitudinal Studies / Jean-Philippe Laurenceau -- 17.Psychometrics / Sean P. Lane -- 18.A Guide for Data Cleaning in Experience Sampling Studies / William Fleeson -- 19.Techniques for Analyzing Intensive Longitudinal Data with Missing Values / Gregory Matthews -- 20.Multilevel Modeling Analyses of Diary-Style Data / John B. Nezlek -- 21.Structural Equation Modeling of Ambulatory Assessment Data / Tanja Lischetzke -- 22.Analyzing Diary and Intensive Longitudinal Data from Dyads / Niall Bolger -- 23.Investigating Temporal Instability in Psychological Variables: Understanding the Real World as Time Dependent / Timothy J. Trull -- 24.Modeling Nonlinear Dynamics in Intraindividual Variability / Pascal R. Deboeck -- 25.Within-Person Factor Analysis: Modeling How the Individual Fluctuates and Changes across Time / Nilam Ram -- 26.Multilevel Mediational Analysis in the Study of Daily Lives / Noel A. Card -- IV.RESEARCH APPLICATIONS: PERSPECTIVES FROM DIFFERENT FIELDS -- 27.Emotion Research / Randy J. Larsen -- 28.Close Relationships / Thery Prok -- 29.Personality Research / Erik E. Noftle -- 30.Cross-Cultural Research / Christie Napa Scollon -- 31.Positive Psychology / Sonja Lyubomirsky -- 32.Health Psychology / Kristin E. Heron -- 33.Developmental Psychology / Joel M. Hektner -- 34.Industrial/Organizational Psychology / Daniel J. Beal -- 35.Clinical Psychology / Emily M. Scheiderer -- 36.Psychiatry / Inez Myin-Germeys},
 year = {2012},
 title = {Handbook of Research Methods for Studying Daily Life},
 address = {New York},
 publisher = {{Guilford Press}},
 isbn = {1609187474},
 editor = {Mehl, Matthias R. and Conner, Tamlin S. and Csikszentmihalyi, Mihaly}
}


@article{Mitani.2017,
 author = {Mitani, Aya A. and Nelson, Kerrie P.},
 year = {2017},
 title = {Modeling Agreement Between Binary Classifications of Multiple Raters in R and SAS},
 pages = {277--309},
 volume = {16},
 number = {2},
 journal = {Journal of Modern Applied Statistical Methods},
 doi = {10.22237/jmasm/1509495300}
}


@article{Maassen.2000,
 author = {Maassen, Gerard H.},
 year = {2000},
 title = {Principles of Defining Reliable Change Indices},
 pages = {622--632},
 volume = {22},
 number = {5},
 issn = {1380-3395},
 journal = {Journal of Clinical and Experimental Neuropsychology},
 doi = {10.1076/1380-3395(200010)22:5;1-9;FT622}
}


@article{Lakens.2014,
 abstract = {Recent events have led psychologists to acknowledge that the inherent uncertainty encapsulated in an inductive science is amplified by problematic research practices. In this article, we provide a practical introduction to recently developed statistical tools that can be used to deal with these uncertainties when performing and evaluating research. In Part 1, we discuss the importance of accurate and stable effect size estimates as well as how to design studies to reach a corridor of stability around effect size estimates. In Part 2, we explain how, given uncertain effect size estimates, well-powered studies can be designed with sequential analyses. In Part 3, we (a) explain what p values convey about the likelihood that an effect is true, (b) illustrate how the v statistic can be used to evaluate the accuracy of individual studies, and (c) show how the evidential value of multiple studies can be examined with a p-curve analysis. We end by discussing the consequences of incorporating our recommendations in terms of a reduced quantity, but increased quality, of the research output. We hope that the practical recommendations discussed in this article will provide researchers with the tools to make important steps toward a psychological science that allows researchers to differentiate among all possible truths on the basis of their likelihood.},
 author = {Lakens, Dani{\"e}l and Evers, Ellen R. K.},
 year = {2014},
 title = {Sailing From the Seas of Chaos Into the Corridor of Stability: Practical Recommendations to Increase the Informational Value of Studies},
 pages = {278--292},
 volume = {9},
 number = {3},
 journal = {Perspectives on Psychological Science},
 doi = {10.1177/1745691614528520},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26173264}
}


@article{Schiepek.2020,
 author = {Schiepek, G{\"u}nter and Gelo, Omar and Viol, Kathrin and Kratzer, Leonhard and Orsucci, Franco and Felice, Giulio and St{\"o}ger--Schmidinger, Barbara and Sammet, Isa and Aichhorn, Wolfgang and Sch{\"o}ller, Helmut},
 year = {2020},
 title = {Complex Individual Pathways or Standard Tracks? A Data--Based Discussion on the Trajectories of Change in Psychotherapy},
 issn = {1473-3145},
 journal = {Counselling and Psychotherapy Research},
 doi = {10.1002/capr.12300}
}


@article{Schmitz.2000,
 author = {Schmitz, Norbert and Hartkamp, Norbert and Franke, Gabriele H.},
 year = {2000},
 title = {Assessing Clinically Significant Change: Application to the SCL-90--R},
 url = {https://www.researchgate.net/profile/Gabriele_Franke/publication/12539029_Assessing_Clinically_Significant_Change_Application_to_the_SCL-90-R/links/0fcfd51022fdd49d0e000000/Assessing-Clinically-Significant-Change-Application-to-the-SCL-90-R.pdf},
 pages = {263--274},
 volume = {86},
 number = {1},
 journal = {Psychological reports}
}


@misc{Tibshirani.2019,
 author = {Tibshirani, Rob and Leisch, Friedrich},
 year = {2019},
 title = {bootstrap: Functions for the Book {\textquotedbl}An Introduction to the Bootstrap{\textquotedbl}: R package version 2019.6},
 url = {https://CRAN.R-project.org/package=bootstrap}
}


@misc{Tiedemann.2020,
 author = {Tiedemann, Frederik},
 year = {2020},
 title = {gghalves: Compose Half-Half Plots Using Your Favourite Geoms: R package version 0.1.0},
 url = {https://CRAN.R-project.org/package=gghalves}
}


@article{Tingey.1996,
 abstract = {Jacobson, Follette, and Revenstorf's (1984) proposal for assessing clinical significance provides a needed convention for psychotherapy outcome research. Several limitations that exists in this method (Jacobson {\&} Revenstorf, 1988) are addressed in this paper and extensions are proposed. Specifically, limitations regarding the operationalization of the underlying social validation methodology in the derivation of normative samples and the resultant standards they set are discussed. Extensions and guidelines are proposed for specifying normative samples, determining the distinctness of these samples, and expanding procedures to accommodate multiple samples. This paper initially assumes a psychometric perspective and presents extensions, based on the Symptom Checklist 90-R. Then it shifts to a clinician perspective and applies reliable change estimates and cutoff scores to actual outcome data by analyzing the progress of four patients during and after therapy. The overall merit and utility of extensions to clinical significance are then discussed.},
 author = {Tingey, R. and Lambert, M. and Burlingame, G. and Hansen, N.},
 year = {1996},
 title = {Assessing Clinical Significance: Proposed Extensions to Method},
 pages = {109--123},
 volume = {6},
 number = {2},
 journal = {Psychotherapy Research},
 doi = {10.1080/10503309612331331638},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/22242565}
}


@article{Titov.2011,
 abstract = {The Patient Health Questionnaire-9 Item (PHQ-9) and Beck Depression Inventory-II (BDI-II) are frequently used measures of depression severity, but little is known about their relative psychometric properties. The authors assessed psychometric properties of both measures during treatment for depression. The PHQ-9 and BDI-II scores from 172 depressed participants in two randomized controlled trials of treatment for depression were assessed and combined. Tests of internal consistency (Cronbach's \textgreek{a}), factor analyses, correlational analyses, estimates of clinically significant change, and effect sizes (Cohen's d) were calculated after treatment and follow-up. Both scales demonstrated adequate internal consistency at pre- and posttreatment (PHQ-9 \textgreek{a}~=~.74 and .81; BDI-II \textgreek{a}~=~.87 and .90, respectively). Factor analysis failed to confirm the one-factor model previously reported for the PHQ-9, but two factors evidenced good fit for the BDI-II. Both scales converged more with each other than with the Sheehan Disability Scale at pre- but not at posttreatment. Responsiveness to change of PHQ-9 and BDI-II was similar at both posttreatment and follow-up. The consistency of agreement on indices of clinical significance was fair to moderate, but the BDI-II categorised a greater proportion of participants with severe depression than the PHQ-9. The BDI-II and PHQ-9 demonstrated adequate reliability, convergent/discriminant validity, and similar responsiveness to change. Differences were found in how they categorised severity. Pending the results of further studies, the attributes of the PHQ-9, of being shorter and based on the diagnostic criteria for depression, may indicate an advantage over the BDI-II.},
 author = {Titov, Nickolai and Dear, Blake F. and McMillan, Dean and Anderson, Tracy and Zou, Judy and Sunderland, Matthew},
 year = {2011},
 title = {Psychometric comparison of the PHQ-9 and BDI-II for measuring response during treatment of depression},
 pages = {126--136},
 volume = {40},
 number = {2},
 journal = {Cognitive behaviour therapy},
 doi = {10.1080/16506073.2010.550059},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/25155813}
}


@article{vanderElst.2013,
 abstract = {Serial cognitive assessment is conducted to monitor changes in the cognitive abilities of patients over time. At present, mainly the regression-based change and the ANCOVA approaches are used to establish normative data for serial cognitive assessment. These methods are straightforward, but they have some severe drawbacks. For example, they can only consider the data of two measurement occasions. In this article, we propose three alternative normative methods that are not hampered by these problems-that is, multivariate regression, the standard linear mixed model (LMM), and the linear mixed model combined with multiple imputation (LMM with MI) approaches. The multivariate regression method is primarily useful when a small number of repeated measurements are taken at fixed time points. When the data are more unbalanced, the standard LMM and the LMM with MI methods are more appropriate because they allow for a more adequate modeling of the covariance structure. The standard LMM has the advantage that it is easier to conduct and that it does not require a Monte Carlo component. The LMM with MI, on the other hand, has the advantage that it can flexibly deal with missing responses and missing covariate values at the same time. The different normative methods are illustrated on the basis of the data of a large longitudinal study in which a cognitive test (the Stroop Color Word Test) was administered at four measurement occasions (i.e., at baseline and 3, 6, and 12~years later). The results are discussed and suggestions for future research are provided.},
 author = {{van der Elst}, Wim and Molenberghs, Geert and {van Boxtel}, Martin P. J. and Jolles, Jelle},
 year = {2013},
 title = {Establishing Normative Data for Repeated Cognitive Assessment: A Comparison of Different Statistical Methods},
 pages = {1073--1086},
 volume = {45},
 number = {4},
 journal = {Behavior research methods},
 doi = {10.3758/s13428-012-0305-y},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/23344738}
}


@article{Vork.2019,
 abstract = {BACKGROUND

Confirming treatment response in clinical trials for irritable bowel syndrome (IBS) is challenging, due to the lack of biomarkers and limitations of the currently available symptom assessment tools. The Experience Sampling Method (ESM) might overcome these limitations by collecting digital assessments randomly and repeatedly during daily life. This study evaluated differences in change in abdominal pain between real-time (ie, ESM) and retrospective (ie, Gastrointestinal Symptom Rating Scale [GSRS] and an end-of-day symptom diary) measurements, using data of an RCT on escitalopram vs placebo in patients with IBS and comorbid panic disorder.

METHODS

Twenty-nine IBS patients with comorbid panic disorder were included in a 6-month RCT. The GSRS, diary, and ESM were completed at baseline (t~=~0) and after 3 (t~=~3) and 6~months (t~=~6). Linear mixed models were used.

KEY RESULTS

Experience Sampling Method analyses revealed a significant interaction between escitalopram and time, and ESM abdominal pain scores were 1.4 points lower in the escitalopram group compared to placebo at t~=~6 (on a 1-to-7 scale; P~=~0.021). When including the interaction with momentary anxiety, the reduction in abdominal pain scores in escitalopram vs placebo was even more pronounced for higher levels of anxiety. Average GSRS- and end-of-day abdominal pain scores were not significantly different between escitalopram and placebo at t~=~3 and 6.

CONCLUSIONS {\&} INFERENCES

Real-time ESM has the potential to capture treatment response more sensitively compared to a retrospective end-of-day GI symptom diary and the GSRS, by taking into account day-to-day symptom variability as well as momentary factors that might moderate treatment effect, such as anxiety.},
 author = {Vork, Lisa and Mujagic, Zlatan and Drukker, Marjan and Keszthelyi, Daniel and Conchillo, Jos{\'e} M. and Hesselink, Martine A. M. and {van Os}, Jim and Masclee, Ad A. M. and Leue, Carsten and Kruimel, Joanna W.},
 year = {2019},
 title = {The Experience Sampling Method: Evaluation of Treatment Effect of Escitalopram in IBS With Comorbid Panic Disorder},
 volume = {31},
 number = {1},
 journal = {Neurogastroenterology and Motility},
 doi = {10.1111/nmo.13515},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30460734},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7379179}
}


@article{Wang.2020,
 abstract = {In psychological and educational measurement, it is often of interest to assess change in an individual. The current study expanded on previous research by introducing methods that can evaluate individual change on multiple latent traits measured on multiple occasions. The four methods considered are the likelihood ratio test (LRT), the multivariate Wald test (MWT), the modified multivariate Wald test (MMWT), and the score test (ST). Simulation studies were conducted to examine the true positive rate (TPR) and the false positive rate (FPR) of the new methods under a conventional fixed-form test and a computerized adaptive test (CAT). Manipulated variables included the number of occasions, change magnitudes, patterns of change, and correlations between latent traits. Results revealed that, in terms of FPR, all methods except MWT had close adherence to the nominal significance level. Among the three methods, the LRT is recommended as it provided a balance between FPR and TPR. Larger change magnitude yielded higher TPR, regardless of the remaining factors. With the same test length, a CAT yielded higher TPR than a conventional test. Real-data examples are provided of identifying psychometrically significant change across two to four occasions using a multivariate adaptive self-report medical outcomes measure from hospitalized patients. The detection of significant change among the three methods agreed highly, and those patients identified as having significant change exhibited large profile differences, which provided support for the valid performance of the proposed methods.},
 author = {Wang, Chun and Weiss, David J. and Suen, King Yiu},
 year = {2020},
 title = {Hypothesis Testing Methods for Multivariate Multi-Occasion Intra-Individual Change},
 pages = {1--17},
 journal = {Multivariate behavioral research},
 doi = {10.1080/00273171.2020.1730739},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32124648}
}


@article{Watson.2010,
 abstract = {The correct approach to analyzing method agreement is discussed. Whether we are considering agreement between two measurements on the same samples (repeatability) or two individuals using identical methodology on identical samples (reproducibility) or comparing two methods, appropriate procedures are described, and worked examples are shown. The correct approaches for both categorical and numerical variables are explained. More complex analyses involving a comparison of more than two pairs of data are mentioned and guidance for these analyses given. Simple formulae for calculating the approximate sample size needed for agreement analysis are also given. Examples of good practice from the reproduction literature are cited, and common errors of methodology are indicated.},
 author = {Watson, P. F. and Petrie, A.},
 year = {2010},
 title = {Method Agreement Analysis: A Review of Correct Methodology},
 pages = {1167--1179},
 volume = {73},
 number = {9},
 journal = {Theriogenology},
 doi = {10.1016/j.theriogenology.2010.01.003},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/20138353}
}


@article{Watts.2015,
 abstract = {OBJECTIVES

There were three aims of this study, the first was to examine the efficacy of CBT versus treatment-as-usual (TAU) in the treatment of anxiety and depressive disorders, the second was to examine how TAU is defined in TAU control groups for those disorders, and the third was to explore whether the type of TAU condition influences the estimate of effects of CBT.

METHOD

A systematic search of Cochrane Central Register of Controlled Trials, PsycINFO, and CINAHL was conducted.

RESULTS

48 studies of CBT for depressive or anxiety disorders (n=6926) that specified that their control group received TAU were identified. Most (n=45/48) provided an explanation of the TAU group however there was significant heterogeneity amongst TAU conditions. The meta-analysis showed medium effects favoring CBT over TAU for both anxiety (g=0.69, 95{\%} CI 0.47-0.92, p{\textless}0.001, n=1318) and depression (g=0.70, 95{\%} CI 0.49-0.90, p{\textless}0.001, n=5054), with differential effects observed across TAU conditions.

CONCLUSIONS

CBT is superior to TAU and the size of the effect of CBT compared to TAU depends on the nature of the TAU condition. The term TAU is used in different ways and should be more precisely described. The four key details to be reported can be thought of as {\textquotedbl}who, what, how many, and any additional treatments?{\textquotedbl}},
 author = {Watts, Sarah E. and Turnell, Adrienne and Kladnitski, Natalie and Newby, Jill M. and Andrews, Gavin},
 year = {2015},
 title = {Treatment-As-Usual (TAU) is Anything but Usual: A Meta-Analysis of CBT Versus TAU for Anxiety and Depression},
 keywords = {Anxiety;Anxiety Disorders/therapy;Anxiety/therapy;Cognitive behavior therapy;Cognitive Behavioral Therapy;Depression;Depression/therapy;Depressive Disorder/therapy;Humans;Meta-analysis;Thinking;Treatment-as-usual},
 pages = {152--167},
 volume = {175},
 journal = {Journal of affective disorders},
 doi = {10.1016/j.jad.2014.12.025},
 file = {Watts et al:C\:\\Users\\steph\\OneDrive\\Dokumente\\Citavi 6\\Projects\\Reliable Change Index (RCI) und klinische Signifikanz\\Citavi Attachments\\Watts et al.pdf:pdf}
}


@article{Wendt.2020,
 author = {Wendt, Leon P. and Wright, Aidan G.C. and Pilkonis, Paul A. and Woods, William C. and Denissen, Jaap J.A. and K{\"u}hnel, Anja and Zimmermann, Johannes},
 year = {2020},
 title = {Indicators of Affect Dynamics: Structure, Reliability, and Personality Correlates},
 journal = {European Journal of Personality},
 doi = {10.1002/per.2277}
}


@book{Wickham.2016,
 author = {Wickham, Hadley},
 year = {2016},
 title = {ggplot2: Elegant Graphics for Data Analysis},
 url = {https://ggplot2.tidyverse.org},
 publisher = {{Springer-Verlag New York}},
 isbn = {978-3-319-24277-4}
}


@article{Wickham.2019,
 author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and Fran{\c{c}}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
 year = {2019},
 title = {Welcome to the tidyverse},
 pages = {1686},
 volume = {4},
 number = {43},
 journal = {Journal of Open Source Software},
 doi = {10.21105/joss.01686}
}


@misc{Wickham.2020,
 author = {Wickham, Hadley and Fran{\c{c}}ois, Romain and Henry, Lionel and M{\"u}ller, Kirill},
 year = {2020},
 title = {dplyr: A Grammar of Data Manipulation: R package version 1.0.2},
 url = {https://CRAN.R-project.org/package=dplyr}
}


@article{Wiedemann.2020,
 abstract = {Sudden gains are large and stable improvements in an outcome variable between consecutive measurements, for example during a psychological intervention with multiple assessments. Researching these occurrences could help understand individual change processes in longitudinal data. Three criteria are generally used to identify sudden gains in psychological interventions. However, applying these criteria can be time consuming and prone to errors if not fully automated. Adaptations to these criteria and methodological decisions such as how multiple gains are handled vary across studies and are reported with different levels of detail. These problems limit the comparability of individual studies and make it hard to understand or replicate the exact methods used. The R package suddengains provides a set of tools to facilitate sudden gains research. This article illustrates how to use the package to identify sudden gains or sudden losses and how to extract descriptive statistics as well as exportable data files for further analysis. It also outlines how these analyses can be customised to apply adaptations of the standard criteria. The suddengains package therefore offers significant scope to improve the efficiency, reporting, and reproducibility of sudden gains research.},
 author = {Wiedemann, Milan and Thew, Graham R. and Stott, Richard and Ehlers, Anke},
 year = {2020},
 title = {suddengains: An R Package to Identify Sudden Gains in Longitudinal Data},
 pages = {e0230276},
 volume = {15},
 number = {3},
 journal = {PloS one},
 doi = {10.1371/journal.pone.0230276},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32150589},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7062272}
}


@article{Williams.2019,
 author = {Williams, Matt},
 year = {2019},
 title = {Scales of Measurement and Statistical Analyses},
 url = {https://psyarxiv.com/c5278/download?format=pdf}
}


@article{Wyrwich.2004,
 abstract = {Several recently published investigations have examined the relationship between the magnitude of the standard error of measurement (SEM) and established thresholds for a minimal clinically important difference (MCID) or a minimal important difference (MID) for change scores on health-related quality of life (HRQOL) or health status measures. These investigations, however, have resulted in differing SEM criteria for the MCID or MID. This study reviews and compares two sets of studies: (1) three investigations using a disease-specific HRQOL measure among patient samples with the chronic disease (heart disease, chronic obstructive pulmonary disease, or asthma) that have consistently demonstrated a 1 SEM correspondence with the established MCIDs or MIDs and (2) three investigations among patients referred to physical therapists with back, lower extremity, and neck pain showing that approximately 2.3 SEMs estimated the established MCID standards for three different measures of health status. Chronic disease patients were classified to have a MCID or MID if their global change ratings for the better or the worse were 1, 2, or 3 on a Likert scale ranging from 1 (almost the same, hardly any better, or worse at all) to 7 (a very great deal better or worse). Back pain patients, however, needed average global transition scores of 5, 6, or 7 (a good, a great, or a very great deal better) on the same 7-point Likert scale in order to experience an MCID in their condition. Charting these change levels against their respective SEM-MID criteria provides insight and promise for linking SEM-based criteria to MCID standards for other HRQOL and health status measures.},
 author = {Wyrwich, Kathleen W.},
 year = {2004},
 title = {Minimal Important Difference Thresholds and the Standard Error of Measurement: Is There a Connection?},
 pages = {97--110},
 volume = {14},
 number = {1},
 issn = {1054-3406},
 journal = {Journal of biopharmaceutical statistics},
 doi = {10.1081/BIP-120028508},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/15027502}
}


@article{Youngstrom.2015,
 author = {Youngstrom, Eric A. and Choukas-Bradley, Sophia and Calhoun, Casey D. and Jensen-Doss, Amanda},
 year = {2015},
 title = {Clinical Guide to the Evidence-Based Assessment Approach to Diagnosis and Treatment},
 pages = {20--35},
 volume = {22},
 number = {1},
 issn = {10777229},
 journal = {Cognitive and Behavioral Practice},
 doi = {10.1016/j.cbpra.2013.12.005}
}


@article{Temkin.1999,
 author = {Temkin, Nancy R. and Heaton, Robert K. and Grant, Igor and Dikmen, Sureyya S.},
 year = {1999},
 title = {Detecting Significant Change in Neuropsychological Test Performance: A Comparison of Four Models},
 pages = {357--369},
 volume = {5},
 number = {4},
 journal = {Journal of the International Neuropsychological Society}
}


@article{Tang.1999,
 author = {Tang, Tony Z. and DeRubeis, Robert J.},
 year = {1999},
 title = {Sudden Gains and Critical Sessions in Cognitive-Behavioral Therapy for Depression},
 url = {https://cpb-us-w2.wpmucdn.com/web.sas.upenn.edu/dist/d/257/files/2016/10/1999-01811-008-1-2c0jnyp.pdf},
 volume = {67},
 number = {6},
 issn = {0022-006X},
 journal = {Journal of consulting and clinical psychology}
}


@article{Stuart.2010,
 abstract = {When estimating causal effects using observational data, it is desirable to replicate a randomized experiment as closely as possible by obtaining treated and control groups with similar covariate distributions. This goal can often be achieved by choosing well-matched samples of the original treated and control groups, thereby reducing bias due to the covariates. Since the 1970's, work on matching methods has examined how to best choose treated and control subjects for comparison. Matching methods are gaining popularity in fields such as economics, epidemiology, medicine, and political science. However, until now the literature and related advice has been scattered across disciplines. Researchers who are interested in using matching methods-or developing methods related to matching-do not have a single place to turn to learn about past and current research. This paper provides a structure for thinking about matching methods and guidance on their use, coalescing the existing research (both old and new) and providing a summary of where the literature on matching methods is now and where it should be headed.},
 author = {Stuart, Elizabeth A.},
 year = {2010},
 title = {Matching Methods for Causal Inference: A Review and a Look Forward},
 pages = {1--21},
 volume = {25},
 number = {1},
 journal = {Statistical Science},
 doi = {10.1214/09-STS313},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/20871802},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2943670}
}


@book{Streiner.2008,
 author = {Streiner, David L. and Norman, Geoffrey R.},
 year = {2008},
 title = {Health Measurement Scales: A Practical Guide to Their Development and Use},
 address = {Oxford},
 edition = {4th ed.},
 publisher = {{Oxford University Press}},
 isbn = {9780199231881},
 doi = {10.1093/acprof:oso/9780199231881.001.0001}
}


@article{Schreuder.2020,
 abstract = {BACKGROUND

Intensive longitudinal (IL) designs provide the potential to study symptoms as they evolve in real-time within individuals. This has promising clinical implications, potentially allowing conclusions at the level of specific individuals. The current study aimed to establish the feasibility of IL designs, as indicated by self-rated burden and attrition, in the context of psychiatry. Additionally, we evaluated three core assumptions about the instruments (diary items) used in IL designs. These assumptions are: diary items (1) reflect experiences that change over time within individuals (indicated by item variability), (2) are interpreted consistently over time, and (3) correspond to retrospective assessments of psychopathology.

METHODS

TRAILS TRANS-ID is an add-on IL study in the clinical cohort of the TRAILS study. Daily diaries on psychopathological symptoms for six consecutive months were completed by 134 at risk young adults (age 22.6 $\pm$ 0.6 years). At baseline, immediately after the diary period, and one year after the diary period, participants completed a diagnostic interview.

RESULTS

Excellent compliance (88.5{\%} of the diaries completed), low participant burden (M = 3.21; SD = 1.42; range 1-10), and low attrition (8.2{\%}) supported the feasibility of six-month IL designs. Diary items differed in their variability over time. Evaluation of the consistency of diary item interpretations showed that within-individual variability in scores could not be attributed to changing interpretations over time. Further, daily symptom reports reasonably correlated with retrospective assessments (over a six month period) of psychopathology obtained with the diagnostic interview, suggesting that both measures might complement each other.

CONCLUSION

The current study is the first to show that IL designs over extensive periods (i.e., multiple months) in psychiatry are feasible, and meet three core assumptions to study change in psychopathology. This might allow for addressing novel and promising hypotheses in our field, and might substantially alter how we treat and study mental ill-health.},
 author = {Schreuder, Marieke J. and Groen, Robin N. and Wigman, Johanna T. W. and Hartman, Catharina A. and Wichers, Marieke},
 year = {2020},
 title = {Measuring Psychopathology as it Unfolds in Daily Life: Addressing Key Assumptions of Intensive Longitudinal Methods in the TRAILS TRANS-ID Study},
 volume = {20},
 number = {1},
 journal = {BMC psychiatry},
 doi = {10.1186/s12888-020-02674-1},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7336426},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32631277}
}


@article{Schueller.2015,
 abstract = {OBJECTIVE

Monitoring depressive symptoms during treatment can guide clinical decision making and improve outcomes. The aim of this study was to explore values on the Patient Health Questionnaire (PHQ-9) that could predict response to treatment.

METHOD

Data came from two independent trials, including three treatment modalities of cognitive-behavioral treatment for depression. Four hundred eighty-seven participants who either met the Diagnostic and Statistical Manual of Mental Disorders, 4th Edition criteria for major depressive disorder or had PHQ-9 scores consistent with a diagnosis of depression were included in our analyses. Participants either received 18 weeks of telephone or face-to-face (n=279) or 8 weeks of Web-delivered (n=208) cognitive-behavioral therapy. Depressive symptoms, evaluated using the PHQ-9, were reported every 4 weeks in the telephone and face-to-face trial and weekly in the Web-delivered intervention trial.

RESULTS

Optimal cut points for predicting end-of-treatment response were consistent in both trials. Our results suggested using cut points of a PHQ-9 $\geq$17 at Week 4, and PHQ-9 $\geq$13 at Week 9 and PHQ-9 $\geq$9 at Week 14.

CONCLUSIONS

Consistent cut points were found within the included trials. These cut points may be valuable for algorithms to support clinical decision making.},
 author = {Schueller, Stephen M. and Kwasny, Mary J. and Dear, Blake F. and Titov, Nickolai and Mohr, David C.},
 year = {2015},
 title = {Cut points on the Patient Health Questionnaire (PHQ-9) that predict response to cognitive-behavioral treatments for depression},
 pages = {470--475},
 volume = {37},
 number = {5},
 journal = {General hospital psychiatry},
 doi = {10.1016/j.genhosppsych.2015.05.009},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4558333},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26077754}
}


@article{Schuster.2018,
 abstract = {BACKGROUND

Web-based and blended (face-to-face plus Web-based) interventions for mental health disorders are gaining significance. However, many licensed psychotherapists still have guarded attitudes toward computer-assisted therapy, hindering dissemination efforts.

OBJECTIVE

The objective of this study was to provide a therapist-oriented evaluation of Web-based and blended therapies and identify commonalities and differences in attitudes toward both formats. Furthermore, it aimed to test the impact of an information clip on expressed attitudes.

METHODS

In total, 95 Austrian psychotherapists were contacted and surveyed via their listed occupational email address. An 8-minute information video was shown to half of the therapists before 19 advantages and 13 disadvantages had to be rated on a 6-point Likert scale.

RESULTS

The sample resembled all assessed properties of Austrian psychotherapists (age, theoretical orientation, and region). Therapists did not hold a uniform overall preference. Instead, perceived advantages of both interventions were rated as neutral (t94=1.89, P=.06; d=0.11), whereas Web-based interventions were associated with more disadvantages and risks (t94=9.86, P{\textless}.001; d=0.81). The information clip did not excerpt any detectable effect on therapists' attitudes (r95=-.109, P=.30). The application of modern technologies in the own therapeutic practice and cognitive behavioral orientation were positively related to the given ratings.

CONCLUSIONS

This study is the first to directly compare therapists' attitudes toward Web-based and blended therapies. Positive attitudes play a pivotal role in the dissemination of new technologies, but unexperienced therapists seem to lack knowledge on how to benefit from technology-aided treatments. To speed up implementation, these aspects need to be addressed in the development of new interventions. Furthermore, the preference of blended treatments over Web-based interventions seems to relate to avoidance of risks. Although this study is likely to represent therapists' attitudes in countries with less advanced electronic health services, therapists' attitudes in more advanced countries might present differently.},
 author = {Schuster, Raphael and Pokorny, Raffaela and Berger, Thomas and Topooco, Naira and Laireiter, Anton-Rupert},
 year = {2018},
 title = {The Advantages and Disadvantages of Online and Blended Therapy: Survey Study Amongst Licensed Psychotherapists in Austria},
 volume = {20},
 number = {12},
 journal = {Journal of medical Internet research},
 doi = {10.2196/11007},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6315274},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30563817}
}


@article{Schuster.2020,
 abstract = {Smartphone-based devices are increasingly recognized to assess disease symptoms in daily life (e.g. ecological momentary assessment, EMA). Despite this development in digital psychiatry, clinical trials are mainly based on point assessments of psychopathology. This study investigated expectable increases in statistical power by intense assessment in randomized controlled trials (RCTs). A simulation study, based on three scenarios and several empirical data sets, estimated power gains of two- or fivefold pre-post-assessment. For each condition, data sets of various effect sizes were generated, and AN(C)OVAs were applied to the sample of interest (N~=~50-N~=~200). Power increases ranged from 6{\%} to 92{\%}, with higher gains in more underpowered scenarios and with higher number of repeated assessments. ANCOVA profited from a more precise estimation of the baseline covariate, resulting in additional gains in statistical power. Fivefold pre-post EMA resulted in highest absolute statistical power and clearly outperformed traditional questionnaire assessments. For example, ANCOVA of automatized PHQ-9 questionnaire data resulted in absolute power of 55 (for N~=~200 and d~=~0.3). Fivefold EMA, however, resulted in power of 88.9. Non-parametric and multi-level analyses resulted in comparable outcomes. Besides providing psychological treatment, digital mental health can help optimizing sensitivity in RCT-based research. Intense assessment appears advisable whenever psychopathology needs to be assessed with high precision at pre- and post-assessment (e.g. small sample sizes, small treatment effects, or when applying optimization problems like machine learning). First empiric studies are promising, but more evidence is needed. Simulations for various effects and a short guide for popular power software are provided for study planning.},
 author = {Schuster, Raphael and Schreyer, Manuela Larissa and Kaiser, Tim and Berger, Thomas and Klein, Jan Philipp and Moritz, Steffen and Laireiter, Anton-Rupert and Trutschnig, Wolfgang},
 year = {2020},
 title = {Effects of Intense Assessment on Statistical Power in Randomized Controlled Trials: Simulation Study on Depression},
 volume = {20},
 journal = {Internet interventions},
 doi = {10.1016/j.invent.2020.100313},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7090342},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32215257}
}


@article{Schuster.2018b,
 abstract = {BACKGROUND

Blended group therapy combines group sessions with Web- and mobile-based treatment modules. Consequently, blended group therapy widens the choice within blended interventions at reasonable costs. This is the first qualitative study on blended group therapy.

OBJECTIVE

The objective of this study was to investigate the patient-centered feasibility of blended group therapy for major depression, with special emphasis on the fit and dynamic interplay between face-to-face and internet-based elements.

METHODS

A total of 22 patients who had a variety of experiences through participating in one of the two blended group therapy interventions were interviewed following a semistructured interview guide. In-depth interviews were analyzed by three trained psychologists, using thematic analysis and a rule-guided internet-based program (QCAmap). The transcript of the interviews (113,555 words) was reduced to 1081 coded units, with subsequent extraction of 16 themes.

RESULTS

Web- and mobile-based elements were described as a treatment facilitator and motivator, increasing the salience and consolidation of cognitive behavioral therapy materials, resulting in in- and inter-session alignment to the treatment. Additionally, patients valued the option of intimate Web-based self-disclosure (by lateral patient-therapist communication), and therapists were provided with tools for between-session monitoring and reinforcement of exercising. In this context, group phenomena seemed to back up therapists' efforts to increase treatment engagement. The dissonance because of noncompliance with Web-based tasks and the constriction of in-session group interaction were considered as possible negative effects. Finally, issues of tailoring and structure seemed to fulfill different preconditions compared with individual therapy.

CONCLUSIONS

Blended group therapy constitutes a structured and proactive approach to work with depression, and the integration of both modalities initiates a beneficial interplay. Results support the patient-centered value of blended group therapy and provide the first insight into blended group therapy's role in fostering therapeutic treatment factors. However, potential negative effects should be considered carefully.},
 author = {Schuster, Raphael and Sigl, Sophia and Berger, Thomas and Laireiter, Anton-Rupert},
 year = {2018},
 title = {Patients' Experiences of Web- and Mobile-Assisted Group Therapy for Depression and Implications of the Group Setting: Qualitative Follow-Up Study},
 volume = {5},
 number = {3},
 issn = {2368-7959},
 journal = {JMIR mental health},
 doi = {10.2196/mental.9613},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6060305},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29997106}
}


@article{Schuster.2020b,
 author = {Schuster, Raphael and Terhorst, Yannik and Kaiser, Tim and Messner, Eva Maria},
 year = {2020},
 title = {Sample Size, Sample Size Planning, and the Impact of Study Context: Current Practice and Recommendations by the Example of Psychological Depression Treatment},
 url = {https://www.researchgate.net/publication/340718029_Sample_size_sample_size_planning_and_the_impact_of_study_context_Current_practice_and_recommendations_by_the_example_of_psychological_depression_treatment}
}


@article{SEN.2019,
 author = {{\c{S}}EN, N. and {\c{S}}EN, S.},
 year = {2019},
 title = {Calculation of Effect Size in Single-Subject Experimental Studies: Examination of Non-Regression-Based Methods},
 pages = {30--49},
 volume = {10},
 number = {1},
 journal = {Journal of Measurement and Evaluation in Education and Psychology},
 doi = {10.21031/epod.419625}
}


@article{Serbetar.2015,
 author = {{\v{S}}erbetar, Ivan},
 year = {2015},
 title = {Establishing Some Measures of Absolute and Relative Reliability of a Motor Test},
 volume = {17},
 journal = {Croatian Journal of Education},
 doi = {10.15516/cje.v17i0.1484}
}


@article{Schiepek.2020b,
 author = {Schiepek, G{\"u}nter and Sch{\"o}ller, Helmut and de Felice, Giulio and Steffensen, Sune Vork and Bloch, Marie Skaalum and Fartacek, Clemens and Aichhorn, Wolfgang and Viol, Kathrin},
 year = {2020},
 title = {Convergent Validation of Methods for the Identification of Psychotherapeutic Phase Transitions in Time Series of Empirical and Model Systems},
 volume = {11},
 journal = {Frontiers in Psychology},
 doi = {10.3389/fpsyg.2020.01970}
}


@article{Shieh.2017,
 abstract = {Analysis of covariance (ANCOVA) is commonly used in behavioral and educational research to reduce the error variance and improve the power of analysis of variance by adjusting the covariate effects. For planning and evaluating randomized ANCOVA designs, a simple sample-size formula has been proposed to account for the variance deflation factor in the comparison of two treatment groups. The objective of this article is to highlight an overlooked and potential problem of the exiting approximation and to provide an alternative and exact solution of power and sample size assessments for testing treatment contrasts. Numerical investigations are conducted to reveal the relative performance of the two procedures as a reliable technique to accommodate the covariate features that make ANCOVA design particularly distinctive. The described approach has important advantages over the current method in general applicability, methodological justification, and overall accuracy. To enhance the practical usefulness, computer algorithms are presented to implement the recommended power calculations and sample-size determinations.},
 author = {Shieh, Gwowen},
 year = {2017},
 title = {Power and Sample Size Calculations for Contrast Analysis in ANCOVA},
 pages = {1--11},
 volume = {52},
 number = {1},
 journal = {Multivariate behavioral research},
 doi = {10.1080/00273171.2016.1219841},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/28121163}
}


@article{Shiffman.2008,
 abstract = {Assessment in clinical psychology typically relies on global retrospective self-reports collected at research or clinic visits, which are limited by recall bias and are not well suited to address how behavior changes over time and across contexts. Ecological momentary assessment (EMA) involves repeated sampling of subjects' current behaviors and experiences in real time, in subjects' natural environments. EMA aims to minimize recall bias, maximize ecological validity, and allow study of microprocesses that influence behavior in real-world contexts. EMA studies assess particular events in subjects' lives or assess subjects at periodic intervals, often by random time sampling, using technologies ranging from written diaries and telephones to electronic diaries and physiological sensors. We discuss the rationale for EMA, EMA designs, methodological and practical issues, and comparisons of EMA and recall data. EMA holds unique promise to advance the science and practice of clinical psychology by shedding light on the dynamics of behavior in real-world settings.},
 author = {Shiffman, Saul and Stone, Arthur A. and Hufford, Michael R.},
 year = {2008},
 title = {Ecological momentary assessment},
 pages = {1--32},
 volume = {4},
 issn = {1548-5943},
 journal = {Annual review of clinical psychology},
 doi = {10.1146/annurev.clinpsy.3.022806.091415},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/18509902}
}


@article{Shim.2019,
 abstract = {The objective of this paper is to describe general approaches of diagnostic test accuracy (DTA) that are available for the quantitative synthesis of data using R software. We conduct a DTA that summarizes statistics for univariate analysis and bivariate analysis. The package commands of R software were {\textquotedbl}metaprop{\textquotedbl} and {\textquotedbl}metabin{\textquotedbl} for sensitivity, specificity, and diagnostic odds ratio; forest for forest plot; reitsma of {\textquotedbl}mada{\textquotedbl} for a summarized receiver-operating characteristic (ROC) curve; and {\textquotedbl}metareg{\textquotedbl} for meta-regression analysis. The estimated total effect sizes, test for heterogeneity and moderator effect, and a summarized ROC curve are reported using R software. In particular, we focus on how to calculate the effect sizes of target studies in DTA. This study focuses on the practical methods of DTA rather than theoretical concepts for researchers whose fields of study were non-statistics related. By performing this study, we hope that many researchers will use R software to determine the DTA more easily, and that there will be greater interest in related research.},
 author = {Shim, Sung Ryul and Kim, Seong-Jang and Lee, Jonghoo},
 year = {2019},
 title = {Diagnostic Test Accuracy: Application and Practice Using R Software},
 volume = {41},
 journal = {Epidemiology and health},
 doi = {10.4178/epih.e2019007},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30999739},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6545496}
}


@article{Silk.2011,
 abstract = {This study used a new cell phone ecological momentary assessment approach to investigate daily emotional dynamics in 47 youths with major depressive disorder (MDD) and 32 no-psychopathology controls (CON) (ages 7-17 years). Information about emotional experience in the natural environment was obtained using answer-only cell phones, while MDD youths received an 8-week course of cognitive behavioral therapy and/or psychopharmacological treatment. Compared with CON youths, MDD youths reported more intense and labile global negative affect; greater sadness, anger, and nervousness; and a lower ratio of positive to negative affect. These differences increased with pubertal maturation. MDD youths spent more time alone and less time with their families than CON youths. Although differences in emotional experiences were found across social contexts, MDD youths were more negative than CON youths in all contexts examined. As the MDD participants progressed through treatment, diagnostic group differences in the intensity and lability of negative affect decreased, but there were no changes in the ratio of positive to negative affect or in measures of social context. We discuss methodological innovations and advantages of this approach, including improved ecological validity and access to information about variability in emotions, change in emotions over time, the balance of positive and negative emotions, and the social context of emotional experience.},
 author = {Silk, Jennifer S. and Forbes, Erika E. and Whalen, Diana J. and Jakubcak, Jennifer L. and Thompson, Wesley K. and Ryan, Neal D. and Axelson, David A. and Birmaher, Boris and Dahl, Ronald E.},
 year = {2011},
 title = {Daily Emotional Dynamics in Depressed Youth: A Cell Phone Ecological Momentary Assessment Study},
 pages = {241--257},
 volume = {110},
 number = {2},
 journal = {Journal of experimental child psychology},
 doi = {10.1016/j.jecp.2010.10.007},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/21112595},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3061240}
}


@article{Speer.1992,
 author = {Speer, David C.},
 year = {1992},
 title = {Clinically significant change: Jacobson and Truax (1991) revisited},
 pages = {402--408},
 volume = {60},
 number = {3},
 issn = {0022-006X},
 journal = {Journal of consulting and clinical psychology},
 doi = {10.1037/0022-006X.60.3.402}
}


@article{Speer.1995,
 author = {Speer, David C. and Greenbaum, Paul E.},
 year = {1995},
 title = {Five Methods for Computing Significant Individual Client Change and Improvement Rates: Support for an Individual Growth Curve Approach},
 volume = {63},
 number = {6},
 issn = {0022-006X},
 journal = {Journal of consulting and clinical psychology},
 doi = {10.1037/0022-006X.63.6.1044}
}


@article{Staunton.2019,
 abstract = {Establishing meaningful change thresholds for Clinical Outcome Assessments (COA) is critical for score interpretation. While anchor- and distribution-based statistical methods are well-established, qualitative approaches are less frequently used. This commentary summarizes and expands on a symposium presented at the International Society for Quality of Life Research (ISOQOL) 2017 annual conference, which provided an overview of qualitative methods that can be used to support understanding of meaningful change thresholds on COAs. Further published literature and additional examples from multiple disease areas which have also qualitatively explored the concept of meaningful change are presented.Semi-structured interviews conducted independently from a clinical trial, exit interviews conducted in the context of a clinical trial, focus groups, vignettes and the Delphi panel method can be used to obtain data regarding meaningful change thresholds, with advantages and disadvantages to each method. Semi-structured interviews using concept elicitation (CE) or cognitive debriefing (CD) methods conducted independently from a clinical trial can be an efficient way to gain in-depth patient/caregiver insights. However, there can be challenges~with reconciling heterogeneous data across diverse samples and in interpreting the qualitative insights in the context of quantitative score changes. Semi-structured qualitative interviews using CE/CD methods embedded as exit interviews in a clinical trial context with patients/caregivers can provide insights which can augment quantitative findings based on analysis of clinical trial data. However, there are logistical challenges relating to embedding the interviews in a clinical trial.Focus groups and the~Delphi panel method can be valuable for reaching consensus regarding meaningful change thresholds; however, for face-to-face interactions, social desirability bias can affect responses. Finally, using vignettes and taking a mixed methods approach can aid in achieving consensus on the minimum score change endorsed by respondents as a meaningful improvement/decrement. However, the approach can be cognitively challenging for participants~and reaching a~consensus is not~guaranteed.Anchor- and distribution- based methods remain critical in establishing responder definitions. Nonetheless, qualitative data has the potential to provide complementary support that a certain level of change on the target COA, which has been statistically supported, is truly important and meaningful for the target population.},
 author = {Staunton, Hannah and Willgoss, Tom and Nelsen, Linda and Burbridge, Claire and Sully, Kate and Rofail, Diana and Arbuckle, Rob},
 year = {2019},
 title = {An Overview of Using Qualitative Techniques to Explore and Define Estimates of Clinically Important Change on Clinical Outcome Assessments},
 volume = {3},
 number = {1},
 journal = {Journal of patient-reported outcomes},
 doi = {10.1186/s41687-019-0100-y},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6399361},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30830492}
}


@article{Stein.2012,
 abstract = {OBJECTIVE

The diagnosis of dementia includes evidence of decline in cognitive functioning over time measured by objective cognitive tasks. Normative data for changes adjusted for the impact of socio-demographic factors on cognitive test performance are lacking to interpret changes in Mini-Mental State Examination (MMSE) test scores.

METHOD

As part of the German Study on Ageing, Cognition and Dementia in Primary Care Patients (AgeCoDe Study), a sample of 1090 cognitively healthy individuals, aged 75 years and older, was assessed at 1.5-year intervals over a period of 4.5 years using the MMSE. Age- and education-specific Reliable Change Indices (RCIs) were computed.

RESULTS

Age and education were significantly associated with MMSE test performance, and gender indicated no impact. Across different age and education subgroups, changes from at least 2 up to 3 points indicated significant (i.e., reliable) changes in MMSE test scores at the 90{\%} confidence level. Furthermore, the calculation of RCIs for individual patients is demonstrated.

CONCLUSION

This study provides age- and education-specific MMSE norms based upon RCI methods to interpret cognitive changes in older age groups. The computation of RCI scores improves the interpretation of changes in MMSE test scores by controlling for measurement error, practice effects, or regression to the mean.},
 author = {Stein, J. and Luppa, M. and Maier, W. and Wagner, M. and Wolfsgruber, S. and Scherer, M. and K{\"o}hler, M. and Eisele, M. and Weyerer, S. and Werle, J. and Bickel, H. and M{\"o}sch, E. and Wiese, B. and Prokein, J. and Pentzek, M. and Fuchs, A. and Leicht, H. and K{\"o}nig, H-H and Riedel-Heller, S. G.},
 year = {2012},
 title = {Assessing Cognitive Changes in the Elderly: Reliable Change Indices for the Mini-Mental State Examination},
 pages = {208--218},
 volume = {126},
 number = {3},
 journal = {Acta psychiatrica Scandinavica},
 doi = {10.1111/j.1600-0447.2012.01850.x},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/22375927}
}


@article{Stolarova.2014,
 abstract = {This report has two main purposes. First, we combine well-known analytical approaches to conduct a comprehensive assessment of agreement and correlation of rating-pairs and to dis-entangle these often confused concepts, providing a best-practice example on concrete data and a tutorial for future reference. Second, we explore whether a screening questionnaire developed for use with parents can be reliably employed with daycare teachers when assessing early expressive vocabulary. A total of 53 vocabulary rating pairs (34 parent-teacher and 19 mother-father pairs) collected for two-year-old children (12 bilingual) are evaluated. First, inter-rater reliability both within and across subgroups is assessed using the intra-class correlation coefficient (ICC). Next, based on this analysis of reliability and on the test-retest reliability of the employed tool, inter-rater agreement is analyzed, magnitude and direction of rating differences are considered. Finally, Pearson correlation coefficients of standardized vocabulary scores are calculated and compared across subgroups. The results underline the necessity to distinguish between reliability measures, agreement and correlation. They also demonstrate the impact of the employed reliability on agreement evaluations. This study provides evidence that parent-teacher ratings of children's early vocabulary can achieve agreement and correlation comparable to those of mother-father ratings on the assessed vocabulary scale. Bilingualism of the evaluated child decreased the likelihood of raters' agreement. We conclude that future reports of agreement, correlation and reliability of ratings will benefit from better definition of terms and stricter methodological approaches. The methodological tutorial provided here holds the potential to increase comparability across empirical reports and can help improve research practices and knowledge transfer to educational and therapeutic settings.},
 author = {Stolarova, Margarita and Wolf, Corinna and Rinker, Tanja and Brielmann, Aenne},
 year = {2014},
 title = {How to Assess and Compare Inter-Rater Reliability, Agreement and Correlation of Ratings: An Exemplary Analysis of Mother-Father and Parent-Teacher Expressive Vocabulary Rating Pairs},
 volume = {5},
 journal = {Frontiers in Psychology},
 doi = {10.3389/fpsyg.2014.00509},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4063345},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/24994985}
}


@article{Shieh.2020,
 abstract = {The analysis of covariance (ANCOVA) has notably proven to be an effective tool in a broad range of scientific applications. Despite the well-documented literature about its principal uses and statistical properties, the corresponding power analysis for the general linear hypothesis tests of treatment differences remains a less discussed issue. The frequently recommended procedure is a direct application of the ANOVA formula in combination with a reduced degrees of freedom and a correlation-adjusted variance. This article aims to explicate the conceptual problems and practical limitations of the common method. An exact approach is proposed for power and sample size calculations in ANCOVA with random assignment and multinormal covariates. Both theoretical examination and numerical simulation are presented to justify the advantages of the suggested technique over the current formula. The improved solution is illustrated with an example regarding the comparative effectiveness of interventions. In order to facilitate the application of the described power and sample size calculations, accompanying computer programs are also presented.},
 author = {Shieh, Gwowen},
 year = {2020},
 title = {Power Analysis and Sample Size Planning in ANCOVA Designs},
 pages = {101--120},
 volume = {85},
 number = {1},
 journal = {Psychometrika},
 doi = {10.1007/s11336-019-09692-3},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/31823115}
}


@article{Zahra.2010,
 author = {Zahra, Daniel and Hedge, Craig},
 year = {2010},
 title = {The Reliable Change Index: Why Isn't it More Popular in Academic Psychology},
 url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.667.6093&rep=rep1&type=pdf#page=15},
 pages = {14--19},
 volume = {76},
 number = {76},
 journal = {Psychology Postgraduate Affairs Group Quarterly}
}


@inproceedings{Kummerfeld.2019,
 author = {Kummerfeld, Erich and Rix, Alexander},
 title = {Simulations Evaluating Resampling Methods for Causal Discovery: Ensemble Performance and Calibration},
 url = {https://arxiv.org/pdf/1910.02047.pdf},
 pages = {2586--2593},
 booktitle = {2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
 year = {2019}
}


@article{Kraepelien.2018,
 abstract = {Disorder-specific internet-based cognitive-behavioural therapy (ICBT) is effective for depression, panic disorder and social anxiety. In this benchmarking study, a new, individually tailored, ICBT programme (TAIL) showed effects on depression (n = 284, d = 1.33) that were non-inferior to disorder-specific ICBT for depression in routine care (n = 2358, d = 1.35). However, the hypotheses that TAIL for individuals with social anxiety or panic disorder is inferior to disorder-specific ICBT could not be rejected (social anxiety: TAIL d = 0.74 versus disorder-specific d = 0.81; panic: TAIL d = 1.11 versus disorder-specific d = 1.47). Our findings strengthen the empirical base for TAIL as an alternative to disorder-specific ICBT for depression.

Declaration of interest

None.},
 author = {Kraepelien, Martin and Forsell, Erik and Karin, Eyal and Johansson, Robert and Lindefors, Nils and Kaldo, Viktor},
 year = {2018},
 title = {Comparing Individually Tailored to Disorder-Specific Internet-Based Cognitive-Behavioural Therapy: Benchmarking Study},
 pages = {282--284},
 volume = {4},
 number = {4},
 issn = {2056-4724},
 journal = {BJPsych open},
 doi = {10.1192/bjo.2018.41},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30083380},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6066990}
}


@article{Collie.2002,
 author = {Collie, Alexander and Darby, David G. and Falleti, Marina G. and Silbert, Brendan S. and Maruff, Paul},
 year = {2002},
 title = {Determining the Extent of Cognitive Change After Coronary Surgery: A Review of Statistical Procedures},
 pages = {2005--2011},
 volume = {73},
 number = {6},
 journal = {The Annals of thoracic surgery}
}


@misc{Comtois.2020,
 author = {Comtois, Dominic},
 year = {2020},
 title = {summarytools: Tools to Quickly and Neatly Summarize Data: R package version 0.9.6},
 url = {https://CRAN.R-project.org/package=summarytools}
}


@article{Cronbach.1947,
 author = {Cronbach, Lee J.},
 year = {1947},
 title = {Test ``Reliability'': Its Meaning and Determination},
 pages = {1--16},
 volume = {12},
 number = {1},
 journal = {Psychometrika}
}


@article{Cuijpers.2016,
 abstract = {AIMS

Suppose you are the developer of a new therapy for a mental health problem or you have several years of experience working with such a therapy, and you would like to prove that it is effective. Randomised trials have become the gold standard to prove that interventions are effective, and they are used by treatment guidelines and policy makers to decide whether or not to adopt, implement or fund a therapy.

METHODS

You would want to do such a randomised trial to get your therapy disseminated, but in reality your clinical experience already showed you that the therapy works. How could you do a trial in order to optimise the chance of finding a positive effect?

RESULTS

Methods that can help include a strong allegiance towards the therapy, anything that increases expectations and hope in participants, making use of the weak spots of randomised trials (risk of bias), small sample sizes and waiting list control groups (but not comparisons with existing interventions). And if all that fails one can always not publish the outcomes and wait for positive trials.

CONCLUSIONS

Several methods are available to help you show that your therapy is effective, even when it is not.},
 author = {Cuijpers, P. and Cristea, I. A.},
 year = {2016},
 title = {How To Prove That Your Therapy Is Effective, Even When It Is Not: A Guideline},
 pages = {428--435},
 volume = {25},
 number = {5},
 issn = {2045-7960},
 journal = {Epidemiology and psychiatric sciences},
 doi = {10.1017/S2045796015000864},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7137591},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26411384}
}


@article{Cuijpers.2010,
 abstract = {BACKGROUND

No meta-analytical study has examined whether the quality of the studies examining psychotherapy for adult depression is associated with the effect sizes found. This study assesses this association.

METHOD

We used a database of 115 randomized controlled trials in which 178 psychotherapies for adult depression were compared to a control condition. Eight quality criteria were assessed by two independent coders: participants met diagnostic criteria for a depressive disorder, a treatment manual was used, the therapists were trained, treatment integrity was checked, intention-to-treat analyses were used, N {\textgreater}or= 50, randomization was conducted by an independent party, and assessors of outcome were blinded.

RESULTS

Only 11 studies (16 comparisons) met the eight quality criteria. The standardized mean effect size found for the high-quality studies (d=0.22) was significantly smaller than in the other studies (d=0.74, p{\textless}0.001), even after restricting the sample to the subset of other studies that used the kind of care-as-usual or non-specific controls that tended to be used in the high-quality studies. Heterogeneity was zero in the group of high-quality studies. The numbers needed to be treated in the high-quality studies was 8, while it was 2 in the lower-quality studies.

CONCLUSIONS

We found strong evidence that the effects of psychotherapy for adult depression have been overestimated in meta-analytical studies. Although the effects of psychotherapy are significant, they are much smaller than was assumed until now, even after controlling for the type of control condition used.},
 author = {Cuijpers, P. and {van Straten}, A. and Bohlmeijer, E. and Hollon, S. D. and Andersson, G.},
 year = {2010},
 title = {The effects of psychotherapy for adult depression are overestimated: a meta-analysis of study quality and effect size},
 pages = {211--223},
 volume = {40},
 number = {2},
 journal = {Psychological medicine},
 doi = {10.1017/S0033291709006114},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/19490745}
}


@article{Cuijpers.2011,
 abstract = {OBJECTIVE

Interpersonal psychotherapy (IPT), a structured and time-limited therapy, has been studied in many controlled trials. Numerous practice guidelines have recommended IPT as a treatment of choice for unipolar depressive disorders. The authors conducted a meta-analysis to integrate research on the effects of IPT.

METHOD

The authors searched bibliographical databases for randomized controlled trials comparing IPT with no treatment, usual care, other psychological treatments, and pharmacotherapy as well as studies comparing combination treatment using pharmacotherapy and IPT. Maintenance studies were also included.

RESULTS

Thirty-eight studies including 4,356 patients met all inclusion criteria. The overall effect size (Cohen's d) of the 16 studies that compared IPT and a control group was 0.63 (95{\%} confidence interval [CI]=0.36 to 0.90), corresponding to a number needed to treat of 2.91. Ten studies comparing IPT and other psychological treatments showed a nonsignificant differential effect size of 0.04 (95{\%} CI=-0.14 to 0.21; number needed to treat=45.45) favoring IPT. Pharmacotherapy (after removal of one outlier) was more effective than IPT (d=-0.19, 95{\%} CI=-0.38 to -0.01; number needed to treat=9.43), and combination treatment was not more effective than IPT alone, although the paucity of studies precluded drawing definite conclusions. Combination maintenance treatment with pharmacotherapy and IPT was more effective in preventing relapse than pharmacotherapy alone (odds ratio=0.37; 95{\%} CI=0.19 to 0.73; number needed to treat=7.63).

CONCLUSIONS

There is no doubt that IPT efficaciously treats depression, both as an independent treatment and in combination with pharmacotherapy. IPT deserves its place in treatment guidelines as one of the most empirically validated treatments for depression.},
 author = {Cuijpers, Pim and Geraedts, Anna S. and {van Oppen}, Patricia and Andersson, Gerhard and Markowitz, John C. and {van Straten}, Annemieke},
 year = {2011},
 title = {Interpersonal psychotherapy for depression: a meta-analysis},
 pages = {581--592},
 volume = {168},
 number = {6},
 journal = {The American journal of psychiatry},
 doi = {10.1176/appi.ajp.2010.10101411},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/21362740},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3646065}
}


@misc{Dancho.2020,
 author = {Dancho, Matt and Vaughan, Davis},
 year = {2020},
 title = {timetk: A Tool Kit for Working with Time Series in R: R package version 2.2.1},
 url = {https://CRAN.R-project.org/package=timetk}
}


@article{Duff.2012,
 abstract = {Repeated assessments are a relatively common occurrence in clinical neuropsychology. The current paper will review some of the relevant concepts (e.g., reliability, practice effects, alternate forms) and methods (e.g., reliable change index, standardized based regression) that are used in repeated neuropsychological evaluations. The focus will be on the understanding and application of these concepts and methods in the evaluation of the individual patient through examples. Finally, some future directions for assessing change will be described.},
 author = {Duff, Kevin},
 year = {2012},
 title = {Evidence-Based Indicators of Neuropsychological Change in the Individual Patient: Relevant Concepts and Methods},
 pages = {248--261},
 volume = {27},
 number = {3},
 journal = {Archives of Clinical Neuropsychology},
 doi = {10.1093/arclin/acr120},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3499091},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/22382384}
}


@article{Ebert.2016,
 abstract = {BACKGROUND

Almost nothing is known about the potential negative effects of Internet-based psychological treatments for depression. This study aims at investigating deterioration and its moderators within randomized trials on Internet-based guided self-help for adult depression, using an individual patient data meta-analyses (IPDMA) approach.

METHOD

Studies were identified through systematic searches (PubMed, PsycINFO, EMBASE, Cochrane Library). Deterioration in participants was defined as a significant symptom increase according to the reliable change index (i.e. 7.68 points in the CES-D; 7.63 points in the BDI). Two-step IPDMA procedures, with a random-effects model were used to pool data.

RESULTS

A total of 18 studies (21 comparisons, 2079 participants) contributed data to the analysis. The risk for a reliable deterioration from baseline to post-treatment was significantly lower in the intervention v. control conditions (3.36 v. 7.60; relative risk 0.47, 95{\%} confidence interval 0.29-0.75). Education moderated effects on deterioration, with patients with low education displaying a higher risk for deterioration than patients with higher education. Deterioration rates for patients with low education did not differ statistically significantly between intervention and control groups. The benefit-risk ratio for patients with low education indicated that 9.38 patients achieve a treatment response for each patient experiencing a symptom deterioration.

CONCLUSIONS

Internet-based guided self-help is associated with a mean reduced risk for a symptom deterioration compared to controls. Treatment and symptom progress of patients with low education should be closely monitored, as some patients might face an increased risk for symptom deterioration. Future studies should examine predictors of deterioration in patients with low education.},
 author = {Ebert, D. D. and Donkin, L. and Andersson, G. and Andrews, G. and Berger, T. and Carlbring, P. and Rozenthal, A. and Choi, I. and Laferton, J. A. C. and Johansson, R. and Kleiboer, A. and Lange, A. and Lehr, D. and Reins, J. A. and Funk, B. and Newby, J. and Perini, S. and Riper, H. and Ruwaard, J. and Sheeber, L. and Snoek, F. J. and Titov, N. and {{\"U}nl{\"u} Ince}, B. and {van Bastelaar}, K. and Vernmark, K. and {van Straten}, A. and Warmerdam, L. and Salsman, N. and Cuijpers, P.},
 year = {2016},
 title = {Does Internet-Based Guided-Self-Help for Depression Cause Harm? An Individual Participant Data Meta-Analysis on Deterioration Rates and its Moderators in Randomized Controlled Trials},
 pages = {2679--2693},
 volume = {46},
 number = {13},
 journal = {Psychological medicine},
 doi = {10.1017/S0033291716001562},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/27649340},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5560500}
}


@article{EbnerPriemer.2009,
 abstract = {In this review, we discuss ecological momentary assessment (EMA) studies on mood disorders and mood dysregulation, illustrating 6 major benefits of the EMA approach to clinical assessment: (a) Real-time assessments increase accuracy and minimize retrospective bias; (b) repeated assessments can reveal dynamic processes; (c) multimodal assessments can integrate psychological, physiological, and behavioral data; (d) setting- or context-specific relationships of symptoms or behaviors can be identified; (e) interactive feedback can be provided in real time; and (f) assessments in real-life situations enhance generalizability. In the context of mood disorders and mood dysregulation, we demonstrate that EMA can address specific research questions better than laboratory or questionnaire studies. However, before clinicians and researchers can fully realize these benefits, sets of standardized e-diary questionnaires and time sampling protocols must be developed that are reliable, valid, and sensitive to change.},
 author = {Ebner-Priemer, Ulrich W. and Trull, Timothy J.},
 year = {2009},
 title = {Ecological Momentary Assessment of Mood Disorders and Mood Dysregulation},
 pages = {463--475},
 volume = {21},
 number = {4},
 journal = {Psychological assessment},
 doi = {10.1037/a0017075},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/19947781}
}


@article{Edwards.1978,
 author = {Edwards, Daniel W. and Yarvis, Richard M. and Mueller, Daniel P. and Zingale, Holly C. and Wagman, William J.},
 year = {1978},
 title = {Test-Taking and the Stability of Adjustment Scales},
 pages = {275--291},
 volume = {2},
 number = {2},
 issn = {0145-4692},
 journal = {Evaluation Quarterly},
 doi = {10.1177/0193841X7800200206}
}


@article{Ekeroth.2014,
 abstract = {Assessing clinically meaningful change is valuable for treatment planning, monitoring course of illness and evaluating outcome. Although DSM eating disorder (ED) diagnoses have been criticized for poor clinical utility, instability, and uncertainty, remission/change of diagnosis is often the standard for evaluating outcome. We tested the validity of the clinically significant reliable change index (CS/RCI) compared to change in DSM-IV ED-diagnoses. We investigated if CS/RCI was concordant to diagnostic change and compared explained variance on measures at follow-up. Using a database for specialized ED treatment in Sweden the sample contained 1042 female patients (246 adolescents/796 adults). CS/RCI was calculated for the Clinical Impairment Assessment (CIA) and the Eating Disorder Examination Questionnaire (EDE-Q). CS/RCI explained more variance in gain scores for psychopathology measures than diagnostic change (DSM-IV). Average agreement between diagnostic change and CS/RCI was 62{\%} and 60{\%} for CIA and EDE-Q, respectively. Diagnostic change always resulted in more positive outcome than CS/RCI. Together with clinical judgment, CS/RCI is a valuable method for determining clinically significant changes in clinical practice and research. It is economically sound and results are easily interpreted and communicated to patients.},
 author = {Ekeroth, Kerstin and Birgeg{\aa}rd, Andreas},
 year = {2014},
 title = {Evaluating Reliable and Clinically Significant Change in Eating Disorders: Comparisons to Changes in DSM-IV Diagnoses},
 pages = {248--254},
 volume = {216},
 number = {2},
 journal = {Psychiatry research},
 doi = {10.1016/j.psychres.2014.02.008},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/24582504}
}


@article{Enders.2017,
 abstract = {The last 20 years has seen an uptick in research on missing data problems, and most software applications now implement one or more sophisticated missing data handling routines (e.g., multiple imputation or maximum likelihood estimation). Despite their superior statistical properties (e.g., less stringent assumptions, greater accuracy and power), the adoption of these modern analytic approaches is not uniform in psychology and related disciplines. Thus, the primary goal of this manuscript is to describe and illustrate the application of multiple imputation. Although maximum likelihood estimation is perhaps the easiest method to use in practice, psychological data sets often feature complexities that are currently difficult to handle appropriately in the likelihood framework (e.g., mixtures of categorical and continuous variables), but relatively simple to treat with imputation. The paper describes a number of practical issues that clinical researchers are likely to encounter when applying multiple imputation, including mixtures of categorical and continuous variables, item-level missing data in questionnaires, significance testing, interaction effects, and multilevel missing data. Analysis examples illustrate imputation with software packages that are freely available on the internet.},
 author = {Enders, Craig K.},
 year = {2017},
 title = {Multiple Imputation as a Flexible Tool for Missing Data Handling in Clinical Research},
 pages = {4--18},
 volume = {98},
 journal = {Behaviour Research and Therapy},
 doi = {10.1016/j.brat.2016.11.008},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/27890222}
}


@article{Erbe.2016,
 abstract = {Background

Computerized versions of well-established measurements such as the PHQ-9 are widely used, but data on the comparability of psychometric properties are scarce.

Objective

Our objective was to compare the interformat reliability of the paper-and-pen version with a computerized version of the PHQ-9 in a clinical sample.

Methods

130 participants with mental health disorders were recruited during psychotherapeutic treatment in a mental health clinic. In a crossover design, they all completed the PHQ-9 in both the computerized and paper-and-pen versions in randomized order.

Results

The internal consistency was comparable for the computer (\textgreek{a}~=~0.88) and paper versions (\textgreek{a}~=~0.89), and highly significant correlations were found between the formats (r~=~0.92). PHQ-9 total scores were not significantly different between the paper and the computer delivered versions. There was a significant interaction effect between format and order of administration for the PHQ-9, indicating that the first administration delivered slightly higher scores.

Limitations

In order to reduce the required effort for the participants, we did not ask them to fill out anything but the PHQ-9 once in paper and once in computer version.

Conclusions

Our findings suggest that the PHQ-9 can be transferred to computerized use without affecting psychometric properties in a clinically meaningful way.},
 author = {Erbe, Doris and Eichert, Hans-Christoph and Rietz, Christian and Ebert, David},
 year = {2016},
 title = {Interformat Reliability of the Patient Health Questionnaire: Validation of the Computerized Version of the PHQ-9},
 volume = {5},
 journal = {Internet interventions},
 doi = {10.1016/j.invent.2016.06.006},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30135800},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6096192}
}


@article{Estrada.2018,
 abstract = {In a number of scientific fields, researchers need to assess whether a variable has changed between two time points. Average-based change statistics (ABC) such as Cohen's d or Hays' \textgreek{w}2 evaluate the change in the distributions' center, whereas Individual-based change statistics (IBC) such as the Standardized Individual Difference or the Reliable Change Index evaluate whether each case in the sample experienced a reliable change. Through an extensive simulation study we show that, contrary to what previous studies have speculated, ABC and IBC statistics are closely related. The relation can be assumed to be linear, and was found regardless of sample size, pre-post correlation, and shape of the scores' distribution, both in single group designs and in experimental designs with a control group. We encourage other researchers to use IBC statistics to evaluate their effect sizes because: (a) they allow the identification of cases that changed reliably; (b) they facilitate the interpretation and communication of results; and (c) they provide a straightforward evaluation of the magnitude of empirical effects while avoiding the problems of arbitrary general cutoffs.},
 author = {Estrada, Eduardo and Ferrer, Emilio and Pardo, Antonio},
 year = {2018},
 title = {Statistics for Evaluating Pre-post Change: Relation Between Change in the Distribution Center and Change in the Individual Scores},
 pages = {2696},
 volume = {9},
 journal = {Frontiers in Psychology},
 doi = {10.3389/fpsyg.2018.02696},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6331475},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30671008}
}


@article{Felice.2019,
 author = {Felice, Giulio and Orsucci, Franco and Scozzari, Andrea and Gelo, Omar and Serafini, Gabriele and Andreassi, Silvia and Vegni, Nicoletta and Paoloni, Giulia and Lagetto, Gloria and Mergenthaler, Erhard and Giuliani, Alessandro},
 year = {2019},
 title = {What Differentiates Poor and Good Outcome Psychotherapy? A Statistical-Mechanics-Inspired Approach to Psychotherapy Research},
 pages = {22},
 volume = {7},
 number = {2},
 journal = {Systems},
 doi = {10.3390/systems7020022}
}


@article{FernandezAlvarez.2019,
 abstract = {Ample evidence supports the use of Virtual Reality (VR) for anxiety disorders. Nonetheless, currently there is no evidence about moderators or potential negative effects of VR treatment strategies. An Individual Patient Data (IPD) approach was employed with 15 retrieved datasets. The current study sample was composed of 810 patients. Randomized control trials (RCTs) for each primary outcome measure were performed, in addition to moderator analyses of the socio-demographic variables. Deterioration rates were 14 patients (4.0{\%}) in VR, 8 (2.8{\%}) in active control conditions, and 27 (15{\%}) in the WL condition. With regard to receiving treatment, patients in a waiting list control condition had greater odds of deteriorating than in the two active conditions, odds ratios (ORs) 4.87, 95{\%} confidence interval (CI) [0.05, 0.67]. In the case of the socio-demographic variables, none of them were associated with higher or lower odds of deterioration, with the exception of marital status in the WL condition; married people presented a significantly lower probability of deterioration, OR 0.19, 95{\%} CI [0.05, 0.67]. Finally, when comparing pooled effects of VR versus all control conditions, the OR was 0.61 (95{\%} CI 0.31-1.23) in favor of VR, although this result was not statistically significant. This study provides evidence about the deterioration rates of a therapeutic VR approach, showing that the number of deteriorated patients coincides with other therapeutic approaches, and that deterioration is less likely to occur, compared to patients in WL control groups.},
 author = {Fern{\'a}ndez-{\'A}lvarez, Javier and Rozental, Alexander and Carlbring, Per and Colombo, Desir{\'e}e and Riva, Giuseppe and Anderson, Page L. and Ba{\~n}os, Rosa Mar{\'i}a and Benbow, Amanda A. and Bouchard, St{\'e}phane and Bret{\'o}n-L{\'o}pez, Juana Mar{\'i}a and C{\'a}rdenas, Georgina and Difede, JoAnn and Emmelkamp, Paul and Garc{\'i}a-Palacios, Azucena and Guill{\'e}n, Ver{\'o}nica and Hoffman, Hunter and Kampann, Isabel and Moldovan, Ramona and M{\"u}hlberger, Andreas and North, Max and Pauli, Paul and {Pe{\~n}ate Castro}, Wenceslao and Quero, Soledad and Tortella-Feliu, Miquel and Wyka, Kataryzna and Botella, Cristina},
 year = {2019},
 title = {Deterioration Rates in Virtual Reality Therapy: An Individual Patient Data Level Meta-Analysis},
 pages = {3--17},
 volume = {61},
 journal = {Journal of anxiety disorders},
 doi = {10.1016/j.janxdis.2018.06.005},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30057346}
}


@book{Cohen.2013,
 author = {Cohen, Jacob},
 year = {2013},
 title = {Statistical Power Analysis for the Behavioral Sciences},
 publisher = {{Academic press}}
}


@article{Cohen.1994,
 author = {Cohen, Jacob},
 year = {1994},
 title = {The earth is round (p{\textless}. 05)},
 pages = {997},
 volume = {49},
 number = {12},
 journal = {American psychologist}
}


@article{Christensen.1986,
 author = {Christensen, Larry and Mendoza, Jorge L.},
 year = {1986},
 title = {A Method of Assessing Change in a Single Subject: An Alteration of the RC Index},
 pages = {305--308},
 volume = {17},
 journal = {Behavior Therapy}
}


@article{Chelune.1993,
 author = {Chelune, Gordon J. and Naugle, Richard I. and L{\"u}ders, Hans and Sedlak, Jeffery and Awad, Issam A.},
 year = {1993},
 title = {Individual Change After Epilepsy Surgery: Practice Effects and Base-Rate Information},
 volume = {7},
 number = {1},
 journal = {Neuropsychology}
}


@article{Anderson.2014,
 abstract = {The Revised Dyadic Adjustment Scale (RDAS; Busby, Crane, Larson, {\&} Christensen, 1995) is a measure of couple relationship adjustment that is often used to differentiate between distressed and non-distressed couples. While the measure currently allows for a determination of whether group mean scores change significantly across administrations, it lacks the ability to determine whether an individual's change in dyadic adjustment is clinically significant. This study addresses this limitation by establishing a cutoff of 47.31 and reliable change index of 11.58 for the RDAS by pooling data across multiple community and clinical samples. An individual whose score on the RDAS moves across the cutoff changes by 12 or more points can be classified as experiencing clinically significant change.},
 author = {Anderson, Shayne R. and Tambling, Rachel B. and Huff, Scott C. and Heafner, Joy and Johnson, Lee N. and Ketring, Scott A.},
 year = {2014},
 title = {The Development of a Reliable Change Index and Cutoff for the Revised Dyadic Adjustment Scale},
 pages = {525--534},
 volume = {40},
 number = {4},
 journal = {Journal of marital and family therapy},
 doi = {10.1111/jmft.12095},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/25262619}
}


@article{Armey.2015,
 abstract = {Ecological momentary assessment (EMA) is one research method increasingly employed to better understand the processes that underpin depression and related phenomena. In particular, EMA is well suited to the study of affect (e.g., positive and negative affect), affective responses to stress (e.g., emotion reactivity), and behaviors (e.g., activity level, sleep) that are associated with depression. Additionally, EMA can provide insights into self-harm behavior (i.e. suicide and non-suicidal self-injury), and other mood disorders (e.g. bipolar disorder) commonly associated with depressive episodes. Given the increasing availability and affordability of handheld computing devices such as smartphones, EMA is likely to play an increasingly important role in the study of depression and related phenomena in the future.},
 author = {Armey, Michael F. and Schatten, Heather T. and Haradhvala, Natasha and Miller, Ivan W.},
 year = {2015},
 title = {Ecological Momentary Assessment (EMA) of Depression-Related Phenomena},
 pages = {21--25},
 volume = {4},
 issn = {2352-250X},
 journal = {Current opinion in psychology},
 doi = {10.1016/j.copsyc.2015.01.002},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/25664334},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313740}
}


@article{Atkins.2005,
 abstract = {Measures of clinical significance are frequently used to evaluate client change during therapy. Several alternatives to the original method devised by N. S. Jacobson, W. C. Follette, {\&} D. Revenstorf (1984) have been proposed, each purporting to increase accuracy. However, researchers have had little systematic guidance in choosing among alternatives. In this simulation study, the authors systematically explored data parameters (e.g., reliability of measurement, pre-post effect size, and pre-post correlation) that might yield differing results among the most widely considered clinical significance methods. Results indicated that classification across methods was far more similar than different, especially at greater levels of reliability. As such, the existing methods of clinical significance appear highly comparable; future directions for clinical significance use and research are discussed.},
 author = {Atkins, David C. and Bedics, Jamie D. and McGlinchey, Joseph B. and Beauchaine, Theodore P.},
 year = {2005},
 title = {Assessing Clinical Significance: Does It Matter Which Method We Use?},
 keywords = {Data Interpretation, Statistical;Humans;Models, Psychological;Outcome Assessment, Health Care/methods/statistics {\&} numerical data;Psychometrics/methods;Psychotherapy;Reproducibility of Results;Treatment Outcome},
 pages = {982--989},
 volume = {73},
 number = {5},
 issn = {0022-006X},
 journal = {Journal of consulting and clinical psychology},
 doi = {10.1037/0022-006X.73.5.982},
 file = {Atkins, Bedics & Beauchaine (2005) Assessing clinical significance - Does it matter which method we use:C\:\\Users\\steph\\OneDrive\\Dokumente\\Citavi 6\\Projects\\Reliable Change Index (RCI) und klinische Signifikanz\\Citavi Attachments\\Atkins, Bedics & Beauchaine (2005) Assessing clinical significance - Does it matter which method we use.pdf:pdf}
}


@article{Bangdiwala.2013,
 abstract = {BACKGROUND

When assessing the concordance between two methods of measurement of ordinal categorical data, summary measures such as Cohen's (1960) kappa or Bangdiwala's (1985) B-statistic are used. However, a picture conveys more information than a single summary measure.

METHODS

We describe how to construct and interpret Bangdiwala's (1985) agreement chart and illustrate its use in visually assessing concordance in several example clinical applications.

RESULTS

The agreement charts provide a visual impression that no summary statistic can convey, and summary statistics reduce the information to a single characteristic of the data. However, the visual impression is personal and subjective, and not usually reproducible from one reader to another.

CONCLUSIONS

The agreement chart should be used to complement the summary kappa or B-statistics, not to replace them. The graphs can be very helpful to researchers as an early step to understand relationships in their data when assessing concordance.},
 author = {Bangdiwala, Shrikant I. and Shankar, Viswanathan},
 year = {2013},
 title = {The Agreement Chart},
 pages = {97},
 volume = {13},
 journal = {BMC medical research methodology},
 doi = {10.1186/1471-2288-13-97},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/23890315},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3733724}
}


@unpublished{Bartholdy.2019,
 author = {Bartholdy, Stephan},
 year = {2019},
 title = {Defining and Testing an Individualized Reliable Change Index for Ecological Momentary Assessment Data},
 address = {University of Salzburg},
 originalyear = {2019}
}


@book{Bastiaansen.2020,
 author = {Bastiaansen, Jojanneke A. and Orn{\'e}e, Daan Alexander and Meurs, Maaike and Oldehinkel, Albertine},
 year = {2020},
 title = {An Evaluation of the Efficacy of Two Add-On Ecological Momentary Intervention Modules for Depression in a Pragmatic Randomized Controlled Trial (ZELF-i)},
 url = {https://osf.io/34d6t/},
 doi = {10.31234/osf.io/8jfgc}
}


@article{Bauer.2004,
 author = {Bauer, Michael and Grof, Paul and Gyulai, Laszlo and Rasgon, Natalie and Glenn, Tasha and Whybrow, Peter C.},
 year = {2004},
 title = {Using Technology to Improve Longitudinal Studies: Self-Reporting With ChronoRecord in Bipolar Disorder},
 pages = {67--74},
 volume = {6},
 number = {1},
 journal = {Bipolar Disorders}
}


@article{Bauer.2004b,
 abstract = {Clinically significant change refers to meaningful change in individual patient functioning during psychotherapy. Following the operational definition of clinically significant change offered by Jacobson, Follette, and Revenstorf (1984), several alternatives have been proposed because they were thought to be either more accurate or more sensitive to detecting meaningful change. In this study, we compared five methods using a sample of 386 outpatients who underwent treatment in routine clinical practice. Differences were found between methods, suggesting that the statistical method used to calculate clinical significance has an effect on estimates of meaningful change. The Jacobson method (Jacobson {\&} Truax, 1991) provided a moderate estimate of treatment effects and was recommended for use in outcome studies and research on clinically significant change, but future research is needed to validate this statistical method.},
 author = {Bauer, Stephanie and Lambert, Michael J. and Nielsen, Steven Lars},
 year = {2004},
 title = {Clinical Significance Methods: A Comparison of Statistical Techniques},
 pages = {60--70},
 volume = {82},
 number = {1},
 issn = {0022-3891},
 journal = {Journal of personality assessment},
 doi = {10.1207/s15327752jpa8201{\textunderscore }11},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/14979835}
}


@article{Ferrer.2014,
 abstract = {In applied research as well as in clinical practice, it is common to evaluate the change that patients experience as a consequence of the treatment they receive. Various methods designed to evaluate this change are reviewed in this study. This review has focused on a specific aspect that has not been given proper attention: the false positive rate. For this reason, a nonchange situation was simulated (pre-post design with no differences between pre and post) and the behavior of 8 different methods was evaluated in this scenario. Different distributions as well as different sample sizes were simulated. A thousand samples were created for each simulated condition. The percentage of times each method detected a change was obtained in order to evaluate the behavior of the selected methods. Because the simulated situation is a nonchange one, any change alert was considered as a false positive. Out of the 8 evaluated methods, the standardized individual difference and the confidence intervals of linear regression are the ones with the best performance. The remaining methods fail to correctly identify and reject sampling random fluctuations. In most cases there is a tendency to consider changes that respond only to random variations as statistically reliable. However, a simple modification related in reliability estimation allows a considerable improvement in the behavior of some methods.},
 author = {Ferrer, Rodrigo and Pardo, Antonio},
 year = {2014},
 title = {Clinically Meaningful Change: False Positives in the Estimation of Individual Change},
 url = {https://www.researchgate.net/publication/260063986},
 pages = {370--383},
 volume = {26},
 number = {2},
 journal = {Psychological assessment},
 doi = {10.1037/a0035419},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/24490678}
}


@article{Beaton.2000,
 author = {Beaton, Dorcas E.},
 year = {2000},
 title = {Understanding the Relevance of Measured Change Through Studies of Responsiveness},
 pages = {3192--3199},
 volume = {25},
 number = {24},
 journal = {Spine}
}


@misc{Beygelzimer.2019,
 author = {Beygelzimer, Alina and Kakadet, Sham and Langford, John and Arya, Sunil and Mount, David and {Li Shengqiao}},
 year = {2019},
 title = {FNN: Fast Nearest Neighbor Search Algorithms and Applications: R package version 1.1.3},
 url = {https://CRAN.R-project.org/package=FNN}
}


@article{Bland.1999,
 abstract = {Agreement between two methods of clinical measurement can be quantified using the differences between observations made using the two methods on the same subjects. The 95{\%} limits of agreement, estimated by mean difference +/- 1.96 standard deviation of the differences, provide an interval within which 95{\%} of differences between measurements by the two methods are expected to lie. We describe how graphical methods can be used to investigate the assumptions of the method and we also give confidence intervals. We extend the basic approach to data where there is a relationship between difference and magnitude, both with a simple logarithmic transformation approach and a new, more general, regression approach. We discuss the importance of the repeatability of each method separately and compare an estimate of this to the limits of agreement. We extend the limits of agreement approach to data with repeated measurements, proposing new estimates for equal numbers of replicates by each method on each subject, for unequal numbers of replicates, and for replicated data collected in pairs, where the underlying value of the quantity being measured is changing. Finally, we describe a nonparametric approach to comparing methods.},
 author = {Bland, J. M. and Altman, D. G.},
 year = {1999},
 title = {Measuring Agreement in Method Comparison Studies},
 pages = {135--160},
 volume = {8},
 number = {2},
 issn = {0962-2802},
 journal = {Statistical methods in medical research},
 doi = {10.1177/096228029900800204},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/10501650}
}


@article{Bland.1986,
 author = {Bland, J. Martin and Altman, Douglas G.},
 year = {1986},
 title = {Statistical Methods for Assessing Agreement Between Two Methods of Clinical Measurement},
 url = {https://www.researchgate.net/profile/Federico_Nave/post/How_do_you_analyze_the_mean_reduction_in_one_set_of_numbers_from_another/attachment/59d61fd679197b807797e48d/AS%3A287023551270913%401445443155144/download/Statistical+methods+for+assessing+agreement.pdf},
 pages = {307--310},
 volume = {327},
 number = {8476},
 journal = {The lancet}
}


@article{Bland.2007,
 abstract = {Limits of agreement provide a straightforward and intuitive approach to agreement between different methods for measuring the same quantity. When pairs of observations using the two methods are independent, i.e., on different subjects, the calculations are very simple and straightforward. Some authors collect repeated data, either as repeated pairs of measurements on the same subject, whose true value of the measured quantity may be changing, or more than one measurement by one or both methods of an unchanging underlying quantity. In this paper we describe methods for analysing such clustered observations, both when the underlying quantity is assumed to be changing and when it is not.},
 author = {Bland, J. Martin and Altman, Douglas G.},
 year = {2007},
 title = {Agreement Between Methods of Measurement With Multiple Observations per Individual},
 pages = {571--582},
 volume = {17},
 number = {4},
 issn = {1054-3406},
 journal = {Journal of biopharmaceutical statistics},
 doi = {10.1080/10543400701329422},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/17613642}
}


@article{Boman.2019,
 author = {Boman, Magnus and {Ben Abdesslem}, Fehmi and Forsell, Erik and Gillblad, Daniel and G{\"o}rnerup, Olof and Isacsson, Nils and Sahlgren, Magnus and Kaldo, Viktor},
 year = {2019},
 title = {Learning Machines in Internet-Delivered Psychological Treatment},
 pages = {475--485},
 volume = {8},
 number = {4},
 issn = {2192-6352},
 journal = {Progress in Artificial Intelligence},
 doi = {10.1007/s13748-019-00192-0}
}


@misc{Bone.2020,
 author = {Bone, Claire and Delgadillo, Jaime and Barkham, Michael},
 year = {2020},
 title = {A Systematic Review and Meta-Analysis of the Good-Enough Level (GEL) Literature},
 url = {https://www.researchgate.net/publication/344162413_A_Systematic_Review_and_Meta-Analysis_of_the_Good-Enough_Level_GEL_Literature},
 doi = {10.1037/cou0000521}
}


@article{Brysbaert.2019,
 abstract = {Given that an effect size of d = .4 is a good first estimate of the smallest effect size of interest in psychological research, we already need over 50 participants for a simple comparison of two within-participants conditions if we want to run a study with 80{\%} power. This is more than current practice. In addition, as soon as a between-groups variable or an interaction is involved, numbers of 100, 200, and even more participants are needed. As long as we do not accept these facts, we will keep on running underpowered studies with unclear results. Addressing the issue requires a change in the way research is evaluated by supervisors, examiners, reviewers, and editors. The present paper describes reference numbers needed for the designs most often used by psychologists, including single-variable between-groups and repeated-measures designs with two and three levels, two-factor designs involving two repeated-measures variables or one between-groups variable and one repeated-measures variable (split-plot design). The numbers are given for the traditional, frequentist analysis with p {\textless} .05 and Bayesian analysis with BF {\textgreater} 10. These numbers provide researchers with a standard to determine (and justify) the sample size of an upcoming study. The article also describes how researchers can improve the power of their study by including multiple observations per condition per participant.},
 author = {Brysbaert, Marc},
 year = {2019},
 title = {How Many Participants Do We Have to Include in Properly Powered Experiments? A Tutorial of Power Analysis with Reference Tables},
 pages = {16},
 volume = {2},
 number = {1},
 journal = {Journal of cognition},
 doi = {10.5334/joc.72},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6640316},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/31517234}
}


@article{Busch.2015,
 abstract = {Reliable change indices (RCIs) and standardized regression-based (SRB) change score norms permit evaluation of meaningful changes in test scores following treatment interventions, like epilepsy surgery, while accounting for test-retest reliability, practice effects, score fluctuations due to error, and relevant clinical and demographic factors. Although these methods are frequently used to assess cognitive change after epilepsy surgery in adults, they have not been widely applied to examine cognitive change in children with epilepsy. The goal of the current study was to develop RCIs and SRB change score norms for use in children with epilepsy. Sixty-three children with epilepsy (age range: 6-16; M=10.19, SD=2.58) underwent comprehensive neuropsychological evaluations at two time points an average of 12 months apart. Practice effect-adjusted RCIs and SRB change score norms were calculated for all cognitive measures in the battery. Practice effects were quite variable across the neuropsychological measures, with the greatest differences observed among older children, particularly on the Children's Memory Scale and Wisconsin Card Sorting Test. There was also notable variability in test-retest reliabilities across measures in the battery, with coefficients ranging from 0.14 to 0.92. Reliable change indices and SRB change score norms for use in assessing meaningful cognitive change in children following epilepsy surgery are provided for measures with reliability coefficients above 0.50. This is the first study to provide RCIs and SRB change score norms for a comprehensive neuropsychological battery based on a large sample of children with epilepsy. Tables to aid in evaluating cognitive changes in children who have undergone epilepsy surgery are provided for clinical use. An Excel sheet to perform all relevant calculations is also available to interested clinicians or researchers.},
 author = {Busch, Robyn M. and Lineweaver, Tara T. and Ferguson, Lisa and Haut, Jennifer S.},
 year = {2015},
 title = {Reliable Change Indices and Standardized Regression-Based Change Score Norms for Evaluating Neuropsychological Change in Children With Epilepsy},
 pages = {45--54},
 volume = {47},
 journal = {Epilepsy {\&} Behavior},
 doi = {10.1016/j.yebeh.2015.04.052},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26043163},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4475419}
}


@book{Berthold.2020,
 author = {Berthold, Michael R. and Borgelt, Christian and H{\"o}ppner, Frank and Klawonn, Frank and Silipo, Rosaria},
 year = {2020},
 title = {Guide to Intelligent Data Science},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-45573-6},
 doi = {10.1007/978-3-030-45574-3}
}


@article{Kroenke.2001,
 abstract = {OBJECTIVE

While considerable attention has focused on improving the detection of depression, assessment of severity is also important in guiding treatment decisions. Therefore, we examined the validity of a brief, new measure of depression severity.

MEASUREMENTS

The Patient Health Questionnaire (PHQ) is a self-administered version of the PRIME-MD diagnostic instrument for common mental disorders. The PHQ-9 is the depression module, which scores each of the 9 DSM-IV criteria as {\textquotedbl}0{\textquotedbl} (not at all) to {\textquotedbl}3{\textquotedbl} (nearly every day). The PHQ-9 was completed by 6,000 patients in 8 primary care clinics and 7 obstetrics-gynecology clinics. Construct validity was assessed using the 20-item Short-Form General Health Survey, self-reported sick days and clinic visits, and symptom-related difficulty. Criterion validity was assessed against an independent structured mental health professional (MHP) interview in a sample of 580 patients.

RESULTS

As PHQ-9 depression severity increased, there was a substantial decrease in functional status on all 6 SF-20 subscales. Also, symptom-related difficulty, sick days, and health care utilization increased. Using the MHP reinterview as the criterion standard, a PHQ-9 score {\textgreater} or =10 had a sensitivity of 88{\%} and a specificity of 88{\%} for major depression. PHQ-9 scores of 5, 10, 15, and 20 represented mild, moderate, moderately severe, and severe depression, respectively. Results were similar in the primary care and obstetrics-gynecology samples.

CONCLUSION

In addition to making criteria-based diagnoses of depressive disorders, the PHQ-9 is also a reliable and valid measure of depression severity. These characteristics plus its brevity make the PHQ-9 a useful clinical and research tool.},
 author = {Kroenke, K. and Spitzer, R. L. and Williams, J. B.},
 year = {2001},
 title = {The PHQ-9: Validity of a Brief Depression Severity Measure},
 pages = {606--613},
 volume = {16},
 number = {9},
 issn = {0884-8734},
 journal = {Journal of general internal medicine},
 doi = {10.1046/j.1525-1497.2001.016009606.x},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/11556941},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1495268}
}


@article{Ferrer.2019,
 author = {Ferrer, Rodrigo and Pardo, Antonio},
 year = {2019},
 title = {Clinically Meaningful Change},
 pages = {97--105},
 volume = {15},
 number = {3},
 issn = {1614-1881},
 journal = {Methodology},
 doi = {10.1027/1614-2241/a000168}
}


@article{Fisher.2005,
 abstract = {Controlled outcome studies investigating the efficacy of psychological treatments for obsessive-compulsive disorder (OCD) have employed different methods of determining the clinical significance of treatment effects. This makes it difficult to draw conclusions regarding the absolute and relative efficacy of psychological treatments for OCD. To address this issue, standardized Jacobson methodology for defining clinically significant change was applied to recent psychological outcome trials for OCD. The proportion of asymptomatic patients following treatment was also calculated. When recovery is defined by Jacobson methodology, exposure and response prevention (ERP) appears the most effective treatment currently available (50-60{\%} recovered). However, when the asymptomatic criterion is used as the index of outcome, ERP and cognitive therapy have low and equivalent recovery rates (approximately 25{\%}).},
 author = {Fisher, Peter L. and Wells, Adrian},
 year = {2005},
 title = {How Effective are Cognitive and Behavioral Treatments for Obsessive-Compulsive Disorder? A Clinical Significance Analysis},
 pages = {1543--1558},
 volume = {43},
 number = {12},
 journal = {Behaviour Research and Therapy},
 doi = {10.1016/j.brat.2004.11.007},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/16239151}
}


@article{Hsu.1989,
 author = {Hsu, Louis M.},
 year = {1989},
 title = {Reliable changes in psychotherapy: Taking into account regression toward the mean},
 journal = {Behavioral Assessment}
}


@article{Hsu.1999,
 author = {Hsu, Louis M.},
 year = {1999},
 title = {Caveats concerning comparisons of change rates obtained with five methods of identifying significant client changes: Comment on Speer and Greenbaum (1995)},
 pages = {594--598},
 volume = {67},
 number = {4},
 issn = {0022-006X},
 journal = {Journal of consulting and clinical psychology},
 doi = {10.1037/0022-006X.67.4.594}
}


@article{Hurst.2004,
 abstract = {BACKGROUND

To date, clinical trials have relied almost exclusively on the statistical significance of changes in scores from outcome measures in interpreting the effectiveness of treatment interventions. It is becoming increasingly important, however, to determine the clinical rather than statistical significance of these change scores.

OBJECTIVE

To determine cutoff values for change scores that distinguish patients who have clinically improved from those who have not.

METHOD

Data were obtained from 165 back and 100 neck patients undergoing chiropractic treatment. Patients completed the Bournemouth Questionnaire (BQ) before treatment and the BQ and Patient's Global Impression of Change (PGIC) scale after treatment. Three statistical methods were applied to individual change scores on the BQ. These were (1) the Reliable Change Index (RCI); (2) the effect size (ES); and (3) the raw and percentage change scores. The PGIC scale was used as the {\textquotedbl}gold standard{\textquotedbl} of clinically significant change.

RESULTS

The RCI, using the cutoff value of {\textgreater}1.96, appropriately identified clinical improvement in back patients but not in neck patients. An individual ES of approximately 0.5 had the highest sensitivity and specificity in distinguishing back and neck patients who had undergone clinically significant improvement from those who had not. In terms of raw score changes, percentage BQ change scores [(raw change score/baseline score) x 100] of 47{\%} and 34{\%} were identified as having the highest sensitivity and specificity in distinguishing clinically significant improvement from nonimprovement in back and neck patients, respectively.

CONCLUSION

This study provides a methodological framework for identifying clinically significant change in patients. This approach has important implications in providing clinically relevant information about the effect of a treatment intervention in an individual patient.},
 author = {Hurst, Hugh and Bolton, Jennifer},
 year = {2004},
 title = {Assessing the Clinical Significance of Change Scores Recorded on Subjective Outcome Measures},
 pages = {26--35},
 volume = {27},
 number = {1},
 issn = {0161-4754},
 journal = {Journal of manipulative and physiological therapeutics},
 doi = {10.1016/j.jmpt.2003.11.003},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/14739871}
}


@proceedings{IEEE.2019,
 year = {2019},
 title = {2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
 institution = {IEEE}
}


@misc{Ismay.2020,
 author = {Ismay, Chester and Solomon, Nick},
 year = {2020},
 title = {thesisdown: An updated R Markdown thesis template using the bookdown package: R package version 0.1.0},
 url = {https://github.com/ismayc/thesisdown}
}


@article{Jacobson.1984,
 author = {Jacobson, Neil S. and Follette, William C. and Revenstorf, Dirk},
 year = {1984},
 title = {Psychotherapy Outcome Research: Methods for Reporting Variability and Evaluating Clinical Significance},
 pages = {336--352},
 volume = {15},
 number = {4},
 journal = {Behavior Therapy}
}


@article{Jacobson.1991,
 author = {Jacobson, Neil S. and Truax, Paula},
 year = {1991},
 title = {Clinical Significance: A Statistical Approach to Defining Meaningful Change in Psychotherapy Research},
 pages = {12--19},
 volume = {59},
 number = {1},
 issn = {0022-006X},
 journal = {Journal of consulting and clinical psychology},
 doi = {10.1037/0022-006X.59.1.12}
}


@article{Jacobson.2019,
 abstract = {With the recent growth in intensive longitudinal designs and the corresponding demand for methods to analyze such data, there has never been a more pressing need for user-friendly analytic tools that can identify and estimate optimal time lags in intensive longitudinal data. The available standard exploratory methods to identify optimal time lags within univariate and multivariate multiple-subject time series are greatly underpowered at the group (i.e., population) level. We describe a hybrid exploratory-confirmatory tool, referred to herein as the Differential Time-Varying Effect Model (DTVEM), which features a convenient user-accessible function to identify optimal time lags and estimate these lags within a state-space framework. Data from an empirical ecological momentary assessment study are then used to demonstrate the utility of the proposed tool in identifying the optimal time lag for studying the linkages between nervousness and heart rate in a group of undergraduate students. Using a simulation study, we illustrate the effectiveness of DTVEM in identifying optimal lag structures in multiple-subject time-series data with missingness, as well as its strengths and limitations as a hybrid exploratory-confirmatory approach, relative to other existing approaches.},
 author = {Jacobson, Nicholas C. and Chow, Sy-Miin and Newman, Michelle G.},
 year = {2019},
 title = {The Differential Time-Varying Effect Model (DTVEM): A Tool for Diagnosing and Modeling Time Lags in Intensive Longitudinal Data},
 pages = {295--315},
 volume = {51},
 number = {1},
 journal = {Behavior research methods},
 doi = {10.3758/s13428-018-1101-0},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30120682},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6395514}
}


@article{Jennison.1989,
 author = {Jennison, Christopher and Turnbull, Bruce W.},
 year = {1989},
 title = {Interim Analyses: The Repeated Confidence Interval Approach},
 url = {https://ecommons.cornell.edu/bitstream/handle/1813/8680/TR000797.pdf?sequence=1},
 pages = {305--334},
 volume = {51},
 number = {3},
 journal = {Journal of the Royal Statistical Society: Series B (Methodological)}
}


@misc{Kalokerinos.2020,
 author = {Kalokerinos, Elise},
 year = {2020},
 title = {An Introduction to Experience Sampling Methods: GRiPS x Ethics and Well-Being Hub Workshop},
 url = {https://osf.io/3qu4d/}
}


@misc{Kanske.2016,
 author = {Kanske, Philipp},
 year = {2016},
 title = {Das Konzept der klinischen Signifikanz},
 url = {https://www.researchgate.net/profile/Philipp_Kanske/publication/305109736_Vortrag_Klinische_Signifikanz/links/5782412e08ae69ab88285b58/Vortrag-Klinische-Signifikanz.pdf}
}


@article{Karin.2018,
 abstract = {BACKGROUND

Missing cases following treatment are common in Web-based psychotherapy trials. Without the ability to directly measure and evaluate the outcomes for missing cases, the ability to measure and evaluate the effects of treatment is challenging. Although common, little is known about the characteristics of Web-based psychotherapy participants who present as missing cases, their likely clinical outcomes, or the suitability of different statistical assumptions that can characterize missing cases.

OBJECTIVE

Using a large sample of individuals who underwent Web-based psychotherapy for depressive symptoms (n=820), the aim of this study was to explore the characteristics of cases who present as missing cases at posttreatment (n=138), their likely treatment outcomes, and compare between statistical methods for replacing their missing data.

METHODS

First, common participant and treatment features were tested through binary logistic regression models, evaluating the ability to predict missing cases. Second, the same variables were screened for their ability to increase or impede the rate symptom change that was observed following treatment. Third, using recontacted cases at 3-month follow-up to proximally represent missing cases outcomes following treatment, various simulated replacement scores were compared and evaluated against observed clinical follow-up scores.

RESULTS

Missing cases were dominantly predicted by lower treatment adherence and increased symptoms at pretreatment. Statistical methods that ignored these characteristics can overlook an important clinical phenomenon and consequently produce inaccurate replacement outcomes, with symptoms estimates that can swing from -32{\%} to 70{\%} from the observed outcomes of recontacted cases. In contrast, longitudinal statistical methods that adjusted their estimates for missing cases outcomes by treatment adherence rates and baseline symptoms scores resulted in minimal measurement bias ({\textless}8{\%}).

CONCLUSIONS

Certain variables can characterize and predict missing cases likelihood and jointly predict lesser clinical improvement. Under such circumstances, individuals with potentially worst off treatment outcomes can become concealed, and failure to adjust for this can lead to substantial clinical measurement bias. Together, this preliminary research suggests that missing cases in Web-based psychotherapeutic interventions may not occur as random events and can be systematically predicted. Critically, at the same time, missing cases may experience outcomes that are distinct and important for a complete understanding of the treatment effect.},
 author = {Karin, Eyal and Dear, Blake F. and Heller, Gillian Z. and Crane, Monique F. and Titov, Nickolai},
 year = {2018},
 title = {{\textquotedbl}Wish You Were Here{\textquotedbl}: Examining Characteristics, Outcomes, and Statistical Solutions for Missing Cases in Web-Based Psychotherapeutic Trials},
 pages = {e22},
 volume = {5},
 number = {2},
 issn = {2368-7959},
 journal = {JMIR mental health},
 doi = {10.2196/mental.8363},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29674311},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5938693}
}


@article{Karin.2018b,
 abstract = {BACKGROUND

Accurate measurement of treatment-related change is a key part of psychotherapy research and the investigation of treatment efficacy. For this reason, the ability to measure change with accurate and valid methods is critical for psychotherapy.

OBJECTIVE

The aims of this study were to (1) explore the underlying characteristics of depressive symptom change, measured with the nine-item Patient Health Questionnaire (PHQ-9), following psychotherapy, and (2) compare the suitability of different ways to measure and interpret symptom change. A treatment sample of Web-based psychotherapy participants (n=1098) and a waitlist sample (n=96) were used to (1) explore the statistical characteristics of depressive symptom change, and (2) compare the suitability of two common types of change functions: linear and proportional change.

METHODS

These objectives were explored using hypotheses that tested (1) the relationship between baseline symptoms and the rate of change, (2) the shape of symptom score distribution following treatment, and (3) measurement error associated with linear and proportional measurement models.

RESULTS

Findings demonstrated that (1) individuals with severe depressive baseline symptoms had greater reductions in symptom scores than individuals with mild baseline symptoms (11.4 vs 3.7); however, as a percentage measurement, change remained similar across individuals with mild, moderate, or severe baseline symptoms (50{\%}-55{\%}); (2) positive skewness was observed in PHQ-9 score distributions following treatment; and (3) models that measured symptom change as a proportional function resulted in greater model fit and reduced measurement error ({\textless}30{\%}).

CONCLUSIONS

This study suggests that symptom scales, sharing an implicit feature of score bounding, are associated with a proportional function of change. Selecting statistics that overlook this proportional change (eg, Cohen d) is problematic and leads to (1) artificially increased estimates of change with higher baseline symptoms, (2) increased measurement error, and (3) confounded estimates of treatment efficacy and clinical change. Implications, limitations, and idiosyncrasies from these results are discussed.},
 author = {Karin, Eyal and Dear, Blake F. and Heller, Gillian Z. and Gandy, Milena and Titov, Nickolai},
 year = {2018},
 title = {Measurement of Symptom Change Following Web-Based Psychotherapy: Statistical Characteristics and Analytical Methods for Measuring and Interpreting Change},
 pages = {e10200},
 volume = {5},
 number = {3},
 issn = {2368-7959},
 journal = {JMIR mental health},
 doi = {10.2196/10200},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6062691},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30001999}
}


@article{Karyotaki.2018,
 abstract = {Little is known about clinically relevant changes in guided Internet-based interventions for depression. Moreover, methodological and power limitations preclude the identification of patients' groups that may benefit more from these interventions. This study aimed to investigate response rates, remission rates, and their moderators in randomized controlled trials (RCTs) comparing the effect of guided Internet-based interventions for adult depression to control groups using an individual patient data meta-analysis approach. Literature searches in PubMed, Embase, PsycINFO and Cochrane Library resulted in 13,384 abstracts from database inception to January 1, 2016. Twenty-four RCTs (4889 participants) comparing a guided Internet-based intervention with a control group contributed data to the analysis. Missing data were multiply imputed. To examine treatment outcome on response and remission, mixed-effects models with participants nested within studies were used. Response and remission rates were calculated using the Reliable Change Index. The intervention group obtained significantly higher response rates (OR = 2.49, 95{\%} CI 2.17-2.85) and remission rates compared to controls (OR = 2.41, 95{\%} CI 2.07-2.79). The moderator analysis indicated that older participants (OR = 1.01) and native-born participants (1.66) were more likely to respond to treatment compared to younger participants and ethnic minorities respectively. Age (OR = 1.01) and ethnicity (1.73) also moderated the effects of treatment on remission.Moreover, adults with more severe depressive symptoms at baseline were more likely to remit after receiving internet-based treatment (OR = 1.19). Guided Internet-based interventions lead to substantial positive treatment effects on treatment response and remission at post-treatment. Thus, such interventions may complement existing services for depression and potentially reduce the gap between the need and provision of evidence-based treatments.},
 author = {Karyotaki, Eirini and Ebert, David Daniel and Donkin, Liesje and Riper, Heleen and Twisk, Jos and Burger, Simone and Rozental, Alexander and Lange, Alfred and Williams, Alishia D. and Zarski, Anna Carlotta and Geraedts, Anna and {van Straten}, Annemieke and Kleiboer, Annet and Meyer, Bj{\"o}rn and {{\"U}nl{\"u} Ince}, Bur{\c{c}}in B. and Buntrock, Claudia and Lehr, Dirk and Snoek, Frank J. and Andrews, Gavin and Andersson, Gerhard and Choi, Isabella and Ruwaard, Jeroen and Klein, Jan Philipp and Newby, Jill M. and Schr{\"o}der, Johanna and Laferton, Johannes A. C. and {van Bastelaar}, Kim and Imamura, Kotaro and Vernmark, Kristofer and Bo{\ss}, Leif and Sheeber, Lisa B. and Kivi, Marie and Berking, Matthias and Titov, Nickolai and Carlbring, Per and Johansson, Robert and Kenter, Robin and Perini, Sarah and Moritz, Steffen and Nobis, Stephanie and Berger, Thomas and Kaldo, Viktor and Forsell, Yvonne and Lindefors, Nils and Kraepelien, Martin and Bj{\"o}rkelund, Cecilia and Kawakami, Norito and Cuijpers, Pim},
 year = {2018},
 title = {Do Guided Internet-Based Interventions Result in Clinically Relevant Changes for Patients With Depression? An Individual Participant Data Meta-Analysis},
 pages = {80--92},
 volume = {63},
 journal = {Clinical psychology review},
 doi = {10.1016/j.cpr.2018.06.007},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29940401}
}


@article{Kelcey.2020,
 abstract = {Objective: Analysis of the intermediate behaviors and mechanisms through which innovative therapies come to shape outcomes is a critical objective in many areas of psychotherapy research because it supports the iterative exploration, development and refinement of theories and therapies. Despite widespread interest in the intermediate behaviors and mechanisms that convey treatment effects, there is limited guidance on how to effectively and efficiently design studies to detect such mediated effects in the types of partially nested designs that commonly arise in psychotherapy research. In this study, we develop statistical power formulas to identify requisite sample sizes and guide the planning of studies probing mediation under two- and three-level partially nested designs. Method: We investigate multilevel mediation in partially nested structures and models for two- and three-level designs. Results: Well-powered studies probing mediation using partially nested designs will typically require moderate to large sample sizes or moderate to large effects. Discussion: We implement these formulas in the R package PowerUpR and a simple Shiny web application (https://poweruprshiny.shinyapps.io/PartiallyNestedMediationPower/) and demonstrate their use to plan studies using partially nested designs.},
 author = {Kelcey, Ben and Bai, Fangxing and Xie, Yanli},
 year = {2020},
 title = {Statistical Power in Partially Nested Designs Probing Multilevel Mediation},
 pages = {1061--1074},
 volume = {30},
 number = {8},
 journal = {Psychotherapy Research},
 doi = {10.1080/10503307.2020.1717012},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32036780}
}


@book{Kipman.2020,
 author = {Kipman, Ulrike and K{\"u}hberger, Christoph},
 year = {2020},
 title = {Einsatz und Nutzung des Geschichtsschulbuches: Eine Large-Scale-Untersuchung bei Sch{\"u}lern und Lehrern},
 price = {Festeinband : circa EUR 59.99 (DE), circa EUR 61.67 (AT), circa CHF 66.50 (freier Preis)},
 isbn = {978-3-658-24446-0},
 series = {Springer: Research},
 doi = {10.1007/978-3-658-24447-7},
 file = {http://deposit.dnb.de/cgi-bin/dokserv?id=39e91c72cb534dbd9343dc4d341a069d&prov=M&dok_var=1&dok_ext=htm},
 file = {http://www.springer.com/}
}


@misc{Klinke.2019,
 author = {Klinke, Sigbert},
 year = {2019},
 title = {plot.matrix: Visualizes a Matrix as Heatmap: R package version 1.4},
 url = {https://CRAN.R-project.org/package=plot.matrix}
}


@article{Holmes.2016,
 abstract = {Treatment innovation for bipolar disorder has been hampered by a lack of techniques to capture a hallmark symptom: ongoing mood instability. Mood swings persist during remission from acute mood episodes and impair daily functioning. The last significant treatment advance remains Lithium (in the 1970s), which aids only the minority of patients. There is no accepted way to establish proof of concept for a new mood-stabilizing treatment. We suggest that combining insights from mood measurement with applied mathematics may provide a step change: repeated daily mood measurement (depression) over a short time frame (1 month) can create individual bipolar mood instability profiles. A time-series approach allows comparison of mood instability pre- and post-treatment. We test a new imagery-focused cognitive therapy treatment approach (MAPP; Mood Action Psychology Programme) targeting a driver of mood instability, and apply these measurement methods in a non-concurrent multiple baseline design case series of 14 patients with bipolar disorder. Weekly mood monitoring and treatment target data improved for the whole sample combined. Time-series analyses of daily mood data, sampled remotely (mobile phone/Internet) for 28 days pre- and post-treatment, demonstrated improvements in individuals' mood stability for 11 of 14 patients. Thus the findings offer preliminary support for a new imagery-focused treatment approach. They also indicate a step in treatment innovation without the requirement for trials in illness episodes or relapse prevention. Importantly, daily measurement offers a description of mood instability at the individual patient level in a clinically meaningful time frame. This costly, chronic and disabling mental illness demands innovation in both treatment approaches (whether pharmacological or psychological) and measurement tool: this work indicates that daily measurements can be used to detect improvement in individual mood stability for treatment innovation (MAPP).},
 author = {Holmes, E. A. and Bonsall, M. B. and Hales, S. A. and Mitchell, H. and Renner, F. and Blackwell, S. E. and Watson, P. and Goodwin, G. M. and {Di Simplicio}, M.},
 year = {2016},
 title = {Applications of Time-Series Analysis to Mood Fluctuations in Bipolar Disorder to Promote Treatment Innovation: A Case Series},
 pages = {e720},
 volume = {6},
 journal = {Translational psychiatry},
 doi = {10.1038/tp.2015.207},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5068881},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26812041}
}


@article{HintonBayre.2016,
 abstract = {OBJECTIVE

Several reliable change indices (RCIs) exist to evaluate statistically significant individual change with repeated neuropsychological assessment. Yet there is little guidance on model selection and subsequent implications. Using existing test-retest norms, key parameters were systematically evaluated for influence on different RCI models.

METHOD

Normative test-retest data for selected Wechsler Memory Scale-IV subtests were chosen based on the direction and magnitude of differential practice (inequality of test and retest variance). The influence of individual relative position compared to the normative mean was systematically manipulated to evaluate for predictable differences in responsiveness for three RCI models.

RESULTS

With respect to negative change, RCI McSweeny was most responsive when individual baseline scores were below the normative mean, irrespective of differential practice. When an individual score was greater than the normative mean, RCI Chelune was most responsive with lower retest variance, and RCI Maassen most responsive with greater retest variance. This pattern of results can change when test-retest reliability is excellent and there is greater retest variability. Order of responsiveness is reversed if positive change is of interest.

CONCLUSION

RCI models tend to agree when the individual approximates the normative mean at baseline and test-retest variability is equal. However, no RCI model will be universally more or less responsive across all conditions, and model selection may influence subsequent interpretation of change. Given the systematic and predictable differences between models, a more rationale choice can now be made. While a consensus on RCI model preference does not exist, we prefer the regression-based model for several reasons outlined.},
 author = {Hinton-Bayre, Anton D.},
 year = {2016},
 title = {Clarifying Discrepancies in Responsiveness Between Reliable Change Indices},
 pages = {754--768},
 volume = {31},
 number = {7},
 journal = {Archives of Clinical Neuropsychology},
 doi = {10.1093/arclin/acw064},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/27590303}
}


@article{HintonBayre.2011,
 abstract = {There is an ongoing debate over the preferred method(s) for determining the reliable change (RC) in individual scores over time. In the present paper, specificity comparisons of several classic and contemporary RC models were made using a real data set. This included a more detailed review of a new RC model recently proposed in this journal, that used the within-subjects standard deviation (WSD) as the error term. It was suggested that the RC(WSD) was more sensitive to change and theoretically superior. The current paper demonstrated that even in the presence of mean practice effects, false-positive rates were comparable across models when reliability was good and initial and retest variances were equivalent. However, when variances differed, discrepancies in classification across models became evident. Notably, the RC using the WSD provided unacceptably high false-positive rates in this setting. It was considered that the WSD was never intended for measuring change in this manner. The WSD actually combines systematic and error variance. The systematic variance comes from measurable between-treatment differences, commonly referred to as practice effect. It was further demonstrated that removal of the systematic variance and appropriate modification of the residual error term for the purpose of testing individual change yielded an error term already published and criticized in the literature. A consensus on the RC approach is needed. To that end, further comparison of models under varied conditions is encouraged.},
 author = {Hinton-Bayre, Anton D.},
 year = {2011},
 title = {Specificity of Reliable Change Models and Review of the Within-Subjects Standard Deviation as an Error Term},
 pages = {67--75},
 volume = {26},
 number = {1},
 journal = {Archives of Clinical Neuropsychology},
 doi = {10.1093/arclin/acq087},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/21147862}
}


@article{HintonBayre.2011b,
 author = {Hinton-Bayre, Anton D.},
 year = {2011},
 title = {Calculating the Test-Retest Reliability Coefficient From Normative Retest Data for Determining Reliable Change},
 pages = {76--77},
 volume = {26},
 number = {1},
 journal = {Archives of Clinical Neuropsychology},
 doi = {10.1093/arclin/acq084},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/21084316}
}


@book{Fried.2018,
 author = {Fried, Eiko I.},
 year = {2018},
 title = {What are Psychological Constructs? On the Nature and Statistical Modeling of Emotions, Intelligence, Personality Traits and Mental Disorders},
 doi = {10.31234/osf.io/xpu4z}
}


@article{Friedl.2020,
 abstract = {A variety of effective psychotherapies for depression are available, but patients who suffer from depression vary in their treatment response. Combining face-to-face therapies with internet-based elements in the sense of blended treatment is a new approach to treatment for depression. The goal of this study was to answer the following research questions: (1) What are the most important predictors determining optimal treatment allocation to treatment as usual or blended treatment? and (2) Would model-determined treatment allocation using this predictive information and the personalized advantage index (PAI)-approach result in better treatment outcomes? Bayesian model averaging (BMA) was applied to the data of a randomized controlled trial (RCT) comparing the efficacy of treatment as usual and blended treatment in depressive outpatients. Pre-treatment symptomatology and treatment expectancy predicted outcomes irrespective of treatment condition, whereas different prescriptive predictors were found. A PAI of 2.33 PHQ-9 points was found, meaning that patients who would have received the treatment that is optimal for them would have had a post-treatment PHQ-9 score that is two points lower than if they had received the treatment that is suboptimal for them. For 29{\%} of the sample, the PAI was five or greater, which means that a substantial difference between the two treatments was predicted. The use of the PAI approach for clinical practice must be further confirmed in prospective research; the current study supports the identification of specific interventions favorable for specific patients.},
 author = {Friedl, Nadine and Krieger, Tobias and Chevreul, Karine and Hazo, Jean Baptiste and Holtzmann, J{\'e}r{\^o}me and Hoogendoorn, Mark and Kleiboer, Annet and Mathiasen, Kim and Urech, Antoine and Riper, Heleen and Berger, Thomas},
 year = {2020},
 title = {Using the Personalized Advantage Index for Individual Treatment Allocation to Blended Treatment or Treatment as Usual for Depression in Secondary Care},
 volume = {9},
 number = {2},
 issn = {2077-0383},
 journal = {Journal of clinical medicine},
 doi = {10.3390/jcm9020490},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7073663},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32054084}
}


@article{Gehlbach.2011,
 author = {Gehlbach, Hunter and Brinkworth, Maureen E.},
 year = {2011},
 title = {Measure Twice, Cut down Error: A Process for Enhancing the Validity of Survey Scales},
 pages = {380--387},
 volume = {15},
 number = {4},
 issn = {1089-2680},
 journal = {Review of General Psychology},
 doi = {10.1037/a0025704}
}


@article{Geiser.2000,
 abstract = {680 unselected outpatients of our Psychosomatic Clinic were divided into six diagnostic groups and examined by the SCL-90-R. By ANOVA we found differences of means between the diagnostic groups for the global severity score (GSI) and for the subscale scores. We describe the model of Jacobson et al. [1,2] for the determination of cut-off-points and reliable change indices for the assessment of changes over time, e.g. after psychotherapy. Our results lead to the conclusion that, although an overall reliable change index for the GSI can be applied to all psychosomatic patients, cut-off-points should be different following the diagnostic groups. Furthermore, information from the SCL-90-R-subscales as well as other specific symptom scales should be used for the interpretation of significant changes of the GSI.},
 author = {Geiser, F. and Imbierowicz, K. and Schilling, G. and Conrad, R. and Liedtke, R.},
 year = {2000},
 title = {Unterschiede zwischen Diagnosegruppen psychosomatischer Patienten in der Symptom-Checklist-90-R (SCL-90-R). Konsequenzen f{\"u}r den Gebrauch der SCL-90-R in der Verlaufsforschung},
 pages = {447--453},
 volume = {50},
 number = {12},
 issn = {0937-2032},
 journal = {Psychotherapie, Psychosomatik, medizinische Psychologie},
 doi = {10.1055/s-2000-9231},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/11199107}
}


@article{Giavarina.2015,
 abstract = {In a contemporary clinical laboratory it is very common to have to assess the agreement between two quantitative methods of measurement. The correct statistical approach to assess this degree of agreement is not obvious. Correlation and regression studies are frequently proposed. However, correlation studies the relationship between one variable and another, not the differences, and it is not recommended as a method for assessing the comparability between methods.  In 1983 Altman and Bland (B{\&}A) proposed an alternative analysis, based on the quantification of the agreement between two quantitative measurements by studying the mean difference and constructing limits of agreement.  The B{\&}A plot analysis is a simple way to evaluate a bias between the mean differences, and to estimate an agreement interval, within which 95{\%} of the differences of the second method, compared to the first one, fall. Data can be analyzed both as unit differences plot and as percentage differences plot.  The B{\&}A plot method only defines the intervals of agreements, it does not say whether those limits are acceptable or not. Acceptable limits must be defined a priori, based on clinical necessity, biological considerations or other goals.  The aim of this article is to provide guidance on the use and interpretation of Bland Altman analysis in method comparison studies.},
 author = {Giavarina, Davide},
 year = {2015},
 title = {Understanding Bland Altman Analysis},
 pages = {141--151},
 volume = {25},
 number = {2},
 issn = {1330-0962},
 journal = {Biochemia medica},
 doi = {10.11613/BM.2015.015},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4470095},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26110027}
}


@article{Hageman.1993,
 author = {Hageman, Willem JJM and Arrindell, Willem A.},
 year = {1993},
 title = {A Further Refinement of the Reliable Change (RC) Index by Improving the Pre-Post Difference Score: Introducing RCID},
 pages = {693--700},
 volume = {31},
 number = {7},
 journal = {Behaviour Research and Therapy}
}


@article{Hageman.1999,
 author = {Hageman, Willem JJM and Arrindell, Willem A.},
 year = {1999},
 title = {Establishing Clinically Significant Change: Increment of Precision and the Distinction Between Individual and Group Level of Analysis},
 journal = {Behaviour Research and Therapy}
}


@article{Haley.2006,
 author = {Haley, Stephen M. and Fragala-Pinkham, Maria A.},
 year = {2006},
 title = {Interpreting Change Scores of Tests and Measures Used in Physical Therapy},
 pages = {735--743},
 volume = {86},
 number = {5},
 issn = {0031-9023},
 journal = {Physical Therapy},
 doi = {10.1093/ptj/86.5.735}
}


@article{Fisher.2011,
 abstract = {OBJECTIVE

The present article aimed to demonstrate that the establishment of dynamic patterns during the course of psychotherapy can create attractor states for continued adaptive change following the conclusion of treatment.

METHOD

This study is a secondary analysis of T. D. Borkovec and E. Costello (1993). Of the 55 participants in the original study, 33 were retained for the present analysis due to the homogeneity of psychotherapy outcome among these participants. Of these 33, the majority were White (88{\%}) and female (70{\%}), and the average age was 35.44 years (SD = 14.46). Participants participated in 12 weeks of either cognitive behavioral therapy or applied relaxation. Daily diary entries from the treatment period were subjected to time series analyses in order to determine the degree of order versus disorder present within individual dynamic systems. These idiographic data were then aggregated for nomothetic analysis of treatment outcome via linear mixed effect models.

RESULTS

Spectral power due to daily to intradaily oscillations in thrice-daily diary data significantly moderated reliable change over posttreatment follow-up such that lesser power predicted increases in reliable change over the 1-year follow-up period. Additionally, residual variance for dynamic factor models significantly moderated the slope for change over the follow-up period, such that lesser variance--and thus greater order in dynamic systems--predicted increases in reliable change.

CONCLUSIONS

The degree of order in dynamic systems established during therapy acted as an adaptive attractor state, promoting continued positive gains 1 year after the conclusion of therapy. The present study represents an important innovation in the study of dynamic systems in psychotherapy.},
 author = {Fisher, Aaron J. and Newman, Michelle G. and Molenaar, Peter C. M.},
 year = {2011},
 title = {A Quantitative Method for the Analysis of Nomothetic Relationships Between Idiographic Structures: Dynamic Patterns Create Attractor States for Sustained Posttreatment Change},
 pages = {552--563},
 volume = {79},
 number = {4},
 issn = {0022-006X},
 journal = {Journal of consulting and clinical psychology},
 doi = {10.1037/a0024069},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3155821},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/21707138}
}


@book{Haslbeck.2020,
 author = {Haslbeck, Jonas M. B. and Ryan, Ois{\'i}n},
 year = {2020},
 title = {Recovering Within-Person Dynamics from Psychological Time Series},
 doi = {10.31234/osf.io/dymhw}
}


@article{Helmich.2020,
 author = {Helmich, Marieke A.},
 year = {2020},
 title = {Time-Weighted Reliable Change Index: Defining Clinically Relevant Transitions When the Time and Magnitude of Change are Unknown},
 url = {https://files.de-1.osf.io/v1/resources/9sxa4/providers/osfstorage/5e43d9c4a057ec00ebb05f45?action=download&direct&version=2}
}


@article{Hieronymus.2020,
 abstract = {In treatment trials, as well as in clinical practice, a number of individuals with depression fail to respond to medications with established antidepressant properties. The presence of such non-responders has been considered as indicative of inter-individual variability in treatment responsiveness, but disparate outcomes between individuals could also be due to other factors, such as inter-individual differences in disease severity and spontaneous improvement (1). To address this issue, recent studies (1,2) have compared variability (specifically standard deviations, SDs) between groups administered active treatment and placebo across trials. When no differences in variability have been found, this has been quoted as an argument against antidepressants being effective or against the value of precision medicine in depression (1).},
 author = {Hieronymus, Fredrik and Hieronymus, Magnus and Nilsson, Staffan and Eriksson, Elias and {\O}stergaard, S{\o}ren Dinesen},
 year = {2020},
 title = {Individual Variability in Treatment Response to Antidepressants in Major Depression: Comparing Trial-Level and Patient-Level Analyses},
 journal = {Acta psychiatrica Scandinavica},
 doi = {10.1111/acps.13205},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32940342}
}


@article{Hiller.2012,
 abstract = {There is no consensus as to how to define response and remission for mental disorder treatments. The Reliable Change Index (RCI) is most commonly used in psychotherapy research, whereas psychopharmacologists prefer to calculate percentage of improvement (PI). We compared both methods using the Beck Depression Inventory in 395 depressive outpatients. The overall pre-post effect size was d=1.18. The PI-50 ($\geq$ 50{\%} improvement from baseline) resulted in outcome estimates higher than the RCI: 66.3{\%} vs. 59.2{\%} for response and 50.6{\%} vs. 45.8{\%} for remission. We demonstrate that the PI approach is independent of arbitrarily chosen reliabilities and reference populations. Furthermore, it takes differences of pre-treatment severity into account. It is considered as a valuable extension of the established RCI in psychotherapy research.},
 author = {Hiller, Wolfgang and Schindler, Amrei C. and Lambert, Michael J.},
 year = {2012},
 title = {Defining Response and Remission in Psychotherapy Research: A Comparison of the RCI and the Method of Percent Improvement},
 pages = {1--11},
 volume = {22},
 number = {1},
 journal = {Psychotherapy Research},
 doi = {10.1080/10503307.2011.616237},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/21943215}
}


@article{HintonBayre.2000,
 author = {Hinton-Bayre, Anton},
 year = {2000},
 title = {Reliable Change formula query},
 pages = {362--363},
 volume = {6},
 number = {3},
 journal = {Journal of the International Neuropsychological Society}
}


@article{HintonBayre.2005,
 abstract = {The present study examined the comparability of 4 alternate forms of the Digit Symbol Substitution test and the Symbol Digit Modalities (written) test, including the original versions. Male contact-sport athletes (N = 112) were assessed on 1 of the 4 forms of each test. Reasonable alternate form comparability was demonstrated through establishing normality of form distributions and conducting pairwise form comparisons of means, variability, and intraclass correlations. Nonetheless, alternate forms are likely an insufficient means of controlling practice in speeded measures at brief (1-2 weeks) retest intervals. Reliable change indices demonstrated that practice must be accounted for in individual retesting.},
 author = {Hinton-Bayre, Anton and Geffen, Gina},
 year = {2005},
 title = {Comparability, Reliability, and Practice Effects on Alternate Forms of the Digit Symbol Substitution and Symbol Digit Modalities Tests},
 pages = {237--241},
 volume = {17},
 number = {2},
 journal = {Psychological assessment},
 doi = {10.1037/1040-3590.17.2.237},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/16029111}
}


@article{HintonBayre.2004,
 author = {Hinton-Bayre, Anton D.},
 year = {2004},
 title = {Holding Out for a Reliable Change From Confusion to a Solution: A Comment on Maassen's ``The Standard Error in the Jacobson and Truax Reliable Change Index.''},
 url = {https://core.ac.uk/download/pdf/83966172.pdf},
 pages = {894--898},
 volume = {10},
 number = {6},
 journal = {Journal of the International Neuropsychological Society}
}


@article{HintonBayre.2005b,
 author = {Hinton-Bayre, Anton D.},
 year = {2005},
 title = {Methodology is More Important Than Statistics When Determining Reliable Change},
 pages = {788--789},
 volume = {11},
 number = {6},
 journal = {Journal of the International Neuropsychological Society}
}


@article{HintonBayre.2010,
 abstract = {The use of reliable change (RC) statistics to determine whether an individual has significantly improved or deteriorated on retesting is growing rapidly in clinical neuropsychology. This paper demonstrates how with only basic test-retest data and a series of simple expressions, the clinician/researcher can implement the majority of contemporary RC model(s). Though sharing a fundamental structure, RC models vary in how they derive predicted retest scores and standard error terms. Published test-retest normative data and a simple case study are presented to demonstrate how to calculate several well-known RC scores. The paper highlights the circumstances under which models will diverge in the estimation of RC. Most importantly variations in individual's performance relative to controls at initial testing, practice effects, inequality of control variability from test to retest, and degree of reliability will see systematic and predictable disagreement among models. More generally, the limitations and opportunities of RC methodology were discussed. Although a consensus on preferred model continues to be debated, the comparison of RC models in clinical samples is encouraged.},
 author = {Hinton-Bayre, Anton D.},
 year = {2010},
 title = {Deriving Reliable Change Statistics From Test-Retest Normative Data: Comparison of Models and Mathematical Expressions},
 url = {https://www.researchgate.net/publication/41669116},
 pages = {244--256},
 volume = {25},
 number = {3},
 journal = {Archives of Clinical Neuropsychology},
 doi = {10.1093/arclin/acq008},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/20197293}
}


@article{Heino.2020,
 author = {Heino, Matti T. J. and Knittle, Keegan Phillip and Noone, Chris and Hasselman, Fred and Hankonen, Nelli},
 year = {2020},
 title = {Studying Behaviour Change Mechanisms Under Complexity},
 url = {https://psyarxiv.com/fxgw4/download?format=pdf}
}


@article{ZilchaMano.2020,
 abstract = {In recent years, innovative approaches have been implemented in counseling and psychotherapy research, creating new and exciting interdisciplinary subfields. The findings that emerged from the implementation of these approaches demonstrate their potential to deepen our understanding of therapeutic change. This article serves as an introduction to the {\textquotedbl}Innovative Approaches to Exploring Processes of Change in Counseling Psychology{\textquotedbl} special issue. The special issue includes articles representing several of the most promising approaches. Each article seeks to serve as a sourcebook for implementing a given approach in counseling research, in such areas as the assessment of coregulation processes, language processing, physiology, motion synchrony, event-related potentials, hormonal measures, and sociometric signals captured by a badge. The studies included in this special issue represent some of the most promising pathways for future studies and provide valuable resources for researchers, as well as clinicians interested in implementing such approaches and/or being educated consumers of empirical findings based on such approaches. This introduction synthesizes the articles in the special issue and proposes a list of guidelines for conducting and consuming research that implements new approaches for studying the process of therapeutic change. We believe that we are not far from the day when these approaches will be instrumental in everyday counseling practice, where they can assist therapists and patients in their collaborative efforts to reduce suffering and increase thriving. (PsycInfo Database Record (c) 2020 APA, all rights reserved).},
 author = {Zilcha-Mano, Sigal and Ramseyer, Fabian T.},
 year = {2020},
 title = {Innovative Approaches to Exploring Processes of Change in Counseling Psychology: Insights and Principles for Future Research},
 pages = {409--419},
 volume = {67},
 number = {4},
 issn = {0022-0167},
 journal = {Journal of counseling psychology},
 doi = {10.1037/cou0000426},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/32614223}
}


