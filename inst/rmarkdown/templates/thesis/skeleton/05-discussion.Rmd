
# Discussion

The present thesis aimed to investigate possible increases in precision of research designs in clinical trials through the use of ecological momentary assessment instead of single-point questionnaire assessments, as well through the use of psychometrically valid classification methods for determining meaningful symptom changes. An exploratory simulation study was conducted, in which a selection of clinical significance methods was compared for both a classical questionnaire format and an EMA format of the PHQ-9 scale for depressive symptoms. The following methods were evaluated: Percentage Change (PC), the Reliable Change Index RCI\textsubscript{JT} [@Jacobson.1984; @Jacobson.1991], and the Individualized Reliable Change Index RCI\textsubscript{ind,pre-SD} introduced in this thesis, as well as their Clinical Significance variants CSI, which included an additional cutoff criterion for determining significant changes.
\par
In this chapter, the results presented in the previous section will be summarized and interpreted. Furthermore, strengths and limitations of this study will be discussed and a final conclusion will be given.

<!------------------------------------------------------------------------------------------------------------->
## Discussion of Results

<!-- Increase in precision in % für 5.5>1.1 (v.a. impr. + det.) + Aufwand (5 Tage) vs. Benefit (accuracy), geeignet für Forschung + Praxis -->
The investigated increases in classification precision resulting from the implementation of multiple daily rather than single-point assessments were consistently found for both questionnaire and EMA formats, but were most pronounced in standard-questionnaire scenarios: By choosing 5-fold assessment intervals rather than two single questionnaire assessments, the accuracy of clinical significance methods was increased by between 17--22%. In particular, the sensitivity was increased by between 18--25% and the specificity by between 11--12%. All of the investigated methods especially benefited from assessment intervals in questionnaire scenarios, where they improved from moderate to high levels of accuracy in determining significantly improved and deteriorated symptom changes. For EMA scenarios, the classification accuracy was increased by between 4--6%, the sensitivity by between 0--7%, and the specificity by between 2--3%, through applying 5-fold intervals of subsequent days rather than 5-fold random-day assessments. Overall, these considerable increases in precision could justify the additional effort of implementing 5-fold assessment intervals over single-point assessments in clinical research and practice.
\par
<!-- Treatment Conditions: PP scenarios -->
In 30-fold, 5-fold Random Window, and single-point questionnaire scenarios under treatment conditions, the methods with the highest sensitivity and specificity were the CSI\textsubscript{PC}, the Mean PC, the CSI\textsubscript{RCI\textsubscript{ind,pre-SD}}, and the RCI\textsubscript{ind,pre-SD} methods. Comparing only 5-fold and 30-fold scenarios, all methods (i.e. Mean PC, RCI\textsubscript{ind,pre-SD}, and CSI\textsubscript{RCI\textsubscript{ind,pre-SD}}) reached similarly high sensitivity and specificity levels >.90.
\par
In a single-point standard-questionnaire scenario, specificity levels were consistently higher than sensitivity levels, indicating that all examined methods were better able to identify negative cases (i.e., correctly identifying when each of the clinical change categories \textit{is not} present) than positive cases (i.e., correctly identifying when each of the clinical change categories \textit{is} present).
\par
<!-- Treatment Conditions: EMA scenarios -->
In 30-fold, 5-fold Random Window, and 5-fold Random Days EMA scenarios under treatment conditions, the methods with the highest sensitivity and specificity were the CSI\textsubscript{PC}, the Mean PC, and the RCI\textsubscript{ind,pre-SD} methods, respectively. Contrary to expectations, there were no large differences between classifications calculated from 30 vs. 5 pre- and post-treatment assessments, except in the Mean PC method, which dropped in sensitivity and specificity from the 30-fold to both 5-fold scenarios. Across all methods and sampling frequencies, specificity levels were consistently higher than sensitivity levels. In both 5-fold Random Window and Random Days scenarios, considerably good performances were only achieved by the CSI\textsubscript{PC} and the Mean PC methods.
\par
<!-- Random Windows > Random Days -->
Classifications in an EMA scenario with randomly selected windows (i.e., comprised of 5 subsequent days pre- and post-treatment) were generally more similar to the \enquote{true} classifications from their full 30-fold assessment interval than the classifications calculated over 5-fold randomly selected sets of days. This result indicates an advantage of study designs with regular daily assessments over designs with randomly selected assessment days. This suggestion seems especially reasonable when a kind of treatment is provided, because its expected effects could presumably be captured most reliably when other systematic, time-sensitive influences are minimized. Similarly, when studying the effects of a therapy over time, random and treatment-independent influences could be detected and statistically controlled more easily in time series with evenly spaced assessments than with random intervals between them.
\par
<!-- Sensitivity + Specificity: CSI_PC > RCI_ind_pre-SD > ... -->
Regarding the examined clinical significance methods under treatment conditions in general, sensitivity and specificity analyses revealed the strongest performances throughout different sampling frequencies resulting from the Clinical Significance approach with a Percentage Change criterion (CSI\textsubscript{PC}), followed by the Clinical Significance approach with an Individualized Reliable Change Index (RCI\textsubscript{ind,pre-SD}).
\newline
\par
<!-- No-Treatment Conditions: FPR + Specificity -->
<!-- Specificity: EMA>PP, Spec. in EMA-No-Treat very good, in PP for EN very low -->
When considering the performance of classification methods, the category of _no significant change_ is equally as important as the directed change categories, because in some contexts, in which deterioration in symptoms is expected over time, the _no change_ category can serve as evidence of therapy effects [@Estrada.2018]. In order to specifically investigate the ability of different approaches to identify these cases, they were also implemented in \enquote{no-treatment} data sets, which simulated waitlist control groups.
\par
Within no-treatment questionnaire scenarios, the highest specificity levels were achieved by the CSI\textsubscript{PC} (in PP\textsubscript{1.1 No-Treat} even higher than in PP\textsubscript{1.1 Treat}), followed by the PC method and the RCI\textsubscript{JT} method. With the lowest specificity in no-treatment questionnaire scenarios, and therefore the highest number of patients falsely determined as significantly changed, the RCI\textsubscript{ind,pre-SD} method generally appeared to be too imprecise for standard-questionnaire studies.
\par
Under no-treatment conditions in EMA scenarios, the overall ability of methods to identify true-negative cases could be considered very good. In the 5-fold scenario, all methods achieved specificity levels >.90, while the CSI\textsubscript{PC} method also did in the single-point scenario. Calculated for only two assessments, the RCI\textsubscript{JT} method also reached acceptable specificity levels >.80. There was an interesting difference between specificity levels in questionnaire and EMA scenarios: Although both were separately investigated in 5-fold and single-point assessment frequencies, every method, except for the PC method, achieved higher specificity levels in EMA scenarios than in standard-questionnaire scenarios. This difference suggests that the PC method performed better (i.e. yielded less false-positive classifications) when calculated with the lower between-assessment intercorrelations present in EMA data.
\par
The (Mean) PC and RCI\textsubscript{ind,pre-SD} methods are proportional and therefore individualized estimates, which are characterized by not making implicit assumptions about the nature of individual change. The RCI\textsubscript{JT} method, on the other hand, has an inherent assumption of linear change in the sense that it judges on the basis of a fixed, sample-level score difference that has to be achieved by participants in order to be regarded as meaningfully improved or deteriorated. This approach treats all individuals equally in that it does not take into account the individual symptom severity expressed at their baseline assessment. Subjects with a low symptom severity at baseline may not be able to show a score reduction $\geq$ the pre-defined meaningful difference, and hence could not be regarded as meaningfully improved, while subjects with a high symptom severity at baseline could pass the required score improvement and be regarded as meaningfully improved, even though their post-treatment score could still be within the clinical range of scores. In contrast, if a method inherently assumes proportional change, it defines the absolute score difference to be regarded as meaningfully changed proportionally, in order to account for the influence of baseline severity. By setting proportional differences as cutoff criteria for classification categories, observed changes are evaluated individually in relation to baseline severity [@Karin.2018].
\par
These characteristics may explain why individualized methods generally resulted in higher sensitivity and specificity levels than the linearly calculated (i.e. more baseline-dependent) RCI\textsubscript{JT}, especially in repeated-assessment scenarios. This is in line with previous research showing that proportional estimates were better apt to model treatment effects, with much higher sensitivity and specificity levels and a lower baseline dependency than linear estimates [@Karin.2018].
\par
<!-- Precision: EMA > PP, R. Window > R. Days, Specificity: EMA > PP -->
Regarding the achieved precision, results of this study suggest a clear superiority of repeated-assessment over single-point assessment approaches, as well as an advantage of random-window over random-day assessment intervals in EMA scenarios. This resulting advantage of EMA over retrospective standard-questionnaire formats is in line with previous research [e.g., @Vork.2019]. Furthermore, specificity ratings were altogether better in EMA scenarios than in standard-questionnaire scenarios, suggesting that true-negative cases (i.e. individuals with no meaningful symptom changes) are generally better detected in EMA data than in questionnaire data.

<!------------------------------------------------------------------------------------------------------------->
## Strengths and Limitations

<!-- Strength -->
The biggest strength of the present study is the empirical basis of the simulated trial data. The generation of questionnaire and EMA data sets in scenarios with different effect sizes and sampling frequencies, which were prepared to comprise respectively identical samples of participants, allowed for comparing individual changes in depressive symptoms across these assessment designs. Questionnaire and EMA scenarios also showed sufficiently similar reliabilities, pre- and post-treatment levels of depressive symptoms, and effect sizes between different scenarios to be comparable without introducing significant systematic biases. A particular emphasis was put on within-subjects comparisons in order to maximize the validity and practicability of findings regarding the sensitivity and specificity of classification methods. In combination with the validated and externally anchored reference method for clinically significant change (i.e. Clinical Significance with Percentage Change criteria and external cutoff scores), these preconditions were created to ensure a high external validity of the results reported in this study.
\par
<!-- Limitation: low baseline levels of depression -->
One notable limitation of the study regards other characteristics of the simulated data sets. Although based on empirically gathered data from clinical samples, on average, the simulated baseline-interval scores were arguably low and mainly corresponded to mild and moderate levels of depression. The PHQ-9 scale has a maximum score of 27 points, but the data used for the analyses in this study did only reach a maximum of 25 points. Therefore, for instance, it would have been possible to add a constant score of 2 points to every single assessment, if the intention would have been to correct the data sets to represent more severe levels of depression. This overall correction would not have had any impact on the effect size (Cohen´s _d_), the underlying covariance matrix of assessments, test-retest reliabilities, the internal consistency Cronbach´s $\alpha$, or the proportional clinical change methods (Percentage Change and Individualized Reliable Change Index). It would only have affected the proportions of cases that were identified by the Clinical Significance method as moved from the clinical to the non-clinical population, or vice versa. This is because the standard definition of clinically significant change in PHQ-9 scores includes the 50 % change criterion, as well as passing the cutoff score of 9 points defining the border between the clinical and the non-clinical distribution [see @McMillan.2010]. However, following from the comparative approach in this study, a constant-value correction would not have altered the measures of interest in this methodological comparison, and neither the conclusions that are drawn from its results. This example is intended to emphasize the generalizability of conclusions regarding the sensitivity and specificity of the analyzed methods in comparison to each other: Assuming that the simulated data sets realistically represent empirically observed clinical trial data, the resulting differences in agreement between methods would be the same, regardless of the symptom-severity levels.
\par
<!-- Standardfehler-Problem:
ws-SD < bs-SD, dadurch weniger Veränderung nötig, um sich nach RCI_ind rel. zu verändern als nach RCI_JT (wegen größerem SE im Nenner beim RCI_JT) -->
Another notable limitation concerns the formula for the newly introduced RCI\textsubscript{ind,pre-SD}, which implements the within-subjects standard deviation to determine the standard error of measurement individually, rather than the between-subjects standard deviation. Although this adaptation is intended, it makes comparisons between both approaches difficult, because the within-subjects variability is often smaller than between-subjects variability. In this case, the RCI\textsubscript{ind,pre-SD} results in more liberal estimates than the RCI\textsubscript{JT} and therefore more subjects being classified as significantly changed. This should be noted when applying the RCI\textsubscript{ind,pre-SD}, especially when comparing it with other methods.
\par
<!-- Strength: PHQ-9 clear clinical interpretation categories + based on DSM-IV criteria
+ Limitation: PHQ-9 adaptation -->
A strength concerning the empirical basis of this study is the use of the PHQ-9 for simulated scenarios, as it can be considered a highly reliable and valid (i.e. its items correspond to the DSM-IV criteria of Major Depressive Disorder) scale, which is extensively studied and applied under both standard-questionnaire and repeated-measurement conditions. However, the use of a special variant of the PHQ-9, which was adapted to daily assessments in EMA scenarios, may confound the originally assessed frequency of depressive symptoms in the past two weeks with the now assessed severity of symptoms on the respective day. This limitation may affect the generalizability of the results in this study.
\par
<!-- Limitation: CSI_PC as Reference -->
Another important limitation concerns the reference standard that was used in parts of the analyses to evaluate the included calculation methods. The clinical significance criteria defined by @McMillan.2010, namely a percentage change of $\geq$ 50 % and passing a given cutoff score, were selected for their empirical basis and adaptation to the simulated questionnaire. Besides this recommended definition of the CS method for the PHQ-9, it would also have been reasonable to implement the @Jacobson.1991 definition, which would consist of the same cutoff score and a Reliable Change Index in the place of percentage change. In this study, the preference for the method including a PC criterion automatically led to higher sensitivity and specificity scores of the PC method, as it was, by definition, very closely related to the reference method of Clinically Significant Change. More importantly, despite the limitations that follow from this specific choice, imperfect reference classifications, unfortunately, are a general problem in almost every context where new assessment methods are evaluated against established ones.
\par
<!-- Future Research: Compare Methods by diff. Reliabilities -->
A potential next step for future research may be comparing the included classification methods for varying levels of reliability, as was done, e.g., by @Atkins.2005. In a simulation study with a similar data structure, pairwise agreement results between clinical significance methods could be computed for a range of empirically relevant reliabilities. Results from the study by @Atkins.2005 showed that for all pairwise comparisons, the agreement between methods increased with higher reliabilities, with Cohen´s Kappa >.90 for a theoretical reliability of _r_ = .95. This extreme result indicates a high dependence of methods on reliability scores: By using very reliable assessment methods, it seems to become less relevant which classification method is applied, as they produce increasingly similar results for high reliabilities. Although reliability estimates as high as $\geq$.90 are not common in many fields of research and also depend on which measure of reliability is used (e.g., Cronbach´s $\alpha$ or $r_{tt}$), high reliabilities are certainly often reported for outcome measures in psychotherapy research.
\par
<!-- Future Research: Compare Methods by diff. Effect Sizes -->
Analogously, the agreement between classification methods could be examined for a range of different overall effect sizes. This simulation would then give deeper insights into important questions such as whether methods also converge in their agreement for increasing effect sizes, or if there are methods which show high sensitivity and specificity over a wide range of effect sizes.

<!------------------------------------------------------------------------------------------------------------->
## Conclusion

<!-- why problem was important -->
The present thesis addressed two important methodological issues which have been prevalent in psychiatric trials for decades and arguably may have contributed to the replication crisis in subdivisions of psychology. The first problem is that the single-point assessment paradigm, which is predominantly used for measuring psychological symptoms over time, has a limited statistical precision to accurately measure the often highly dynamic expression of these symptoms. The second problem of interest is a lack of individual-level methods for evaluating clinically significant symptom changes in repeated-measurement studies. Various useful and widely used methods were introduced in the literature, but most of them were only formulated for either group-level comparisons or individual single-point pre-post assessment scenarios. The purpose of this explorative study was to describe and test ways to improve the psychometric quality of RCTs by conducting ecological momentary assessment studies and implementing individualized criteria for clinically meaningful change.
\par
<!-- ## Implications of Findings -->
On the basis of this research, characteristics like the intensity, frequency, and duration of psychological interventions could be closely tailored to each patient individually, according to their symptom severity and fluctuation over time. Digital mental health tools allow for the precise evaluation of treatment effects within subjects and between subjects in a variety of easily applicable ways [e.g., see @Bauer.2004b]. For instance, within-patient effect sizes and cutoff scores for significant changes may be reported in clinical studies and meta-analyses, in order to increase the transparency and practical benefits of methods. It would also be possible to monitor and analyze symptom changes in EMA apps by incorporating clinical significance methods in their algorithms, which could identify or even predict meaningful changes on the basis of markers for remission or deterioration. Some already widely used routine monitoring tools already offer patient feedback, therapy evaluation, and easy visualization of symptom changes for patients and practitioners. But in addition, individual symptom changes could also be relativized with internal and external factors (e.g., sleep, medication, occupational concerns), to distinguish random noise from treatment effects.
\par
<!-- Raphaels 6 Punkte: 
1. 5.5 > 1.1 in % increase for imp + det -> 5.5 should be used
2. for CSIs, use the baseline-independent, individualized RCI\textsubscript{ind,pre-SD}, or (Mean) PC method
3. highest performance can be expected from PC -> so no need for RCI(ind)?
4. Especially negative effects should be assessed by PC\textsubscript{5.5} or RCI\textsubscript{5.5} (here RCI\textsubscript{5.5} was most liberal, which can be helpful in research on negative events, when an unbiased indicator is desired)
5. accuracy of questionnaire < EMA, despite lower $r_{tt}$ -> indicates that EMA can serve as substitute for questionnaire or even outperform it (+ increase in %)
6. this also implies that changes in psychological states could be assessed more reliably by EMA than by questionnaires (even with equal assessment frequencies)
-->
<!-- Recommendations: EMA + CSI_PC/CSI_RCI_ind_pre-SD -->
There are many approaches to increase the sensitivity and specificity of assessment strategies. In particular, the results of the present study lead to the recommendation for treatment outcome research to implement repeated-measurement designs with short and reliable symptom scales via EMA and to evaluate treatment effects following the Clinical Significance approach with a validated external cutoff criterion and either (1) a Percentage Change criterion of PC $\geq$ 50% or (2) the proposed Individualized Reliable Change Index RCI\textsubscript{ind,pre-SD} (including the individual pre-treatment standard deviation).


