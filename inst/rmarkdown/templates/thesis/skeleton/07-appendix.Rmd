
`r if(knitr:::is_latex_output()) '\\appendix'`
`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'`

<!-- weil in References.Rmd ein Einzug eingestellt wurde -->
\setlength{\parindent}{0in}
\setlength{\leftskip}{0in}
\setlength{\parskip}{0pt}

# Appendix

\markboth{Appendix}{Appendix}

<!------------------------------------------------------------------------------------------------------------->
## Appendix A: Pairwise Correlations Between Assessments {#app-corrmats}

The first appendix includes tables displaying correlation coefficients for pairwise comparisons between single PHQ-9 assessments. Thus, the following correlation matrices give estimates of the retest-reliability $r_{tt}$ for all possible combinations of assessments in the respective dataset. For the sake of readability, they are only included for 5-fold questionnaire and EMA scenarios, but not for 30-fold scenarios. Pairwise correlations between consecutive assessments are displayed in bold font on the diagonals.

<!------------------------------------------------------------------------------------------------------------->
### Questionnaire Scenarios {#rel-corrmats-pp}

<!--
```{r results="asis", echo=FALSE, eval=FALSE}
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/PP_5.5_KorMat.RData")
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/PP_30.30_KorMat.RData")

PP_5.5_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Standard-Questionnaire Scenario",
    label = "cor-pp-55",
    #longtable = TRUE,
    placement = "htb")

PP_30.30_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 30-Fold Standard-Questionnaire Scenario",
    label = "cor-pp-3030",
    #longtable = TRUE,
    placement = "htb")
```
-->

\begin{table}[H]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Window Standard-Questionnaire Scenario}}
  \label{tab:cor-pp-55}
  \tiny
  \begin{tabular}{@{}ccccccccccc@{}}
  \toprule
  & \multicolumn{1}{c}{PRE\_1} & \multicolumn{1}{c}{PRE\_2} & \multicolumn{1}{c}{PRE\_3} & \multicolumn{1}{c}{PRE\_4} & \multicolumn{1}{c}{PRE\_5} & \multicolumn{1}{c}{POST\_1} & \multicolumn{1}{c}{POST\_2} & \multicolumn{1}{c}{POST\_3} & \multicolumn{1}{c}{POST\_4} & \multicolumn{1}{c}{POST\_5}\\
  \midrule
  PRE\_1  & 1 & \textbf{0.65} & 0.53 & 0.50 &	0.55 & 0.11 & 0.11 & 0.12 & 0.12 & 0.12\\
  PRE\_2  & 0.65 & 1 & \textbf{0.65} & 0.52 &	0.51 & 0.10 & 0.12 & 0.11 & 0.12 & 0.12\\
  PRE\_3  & 0.53 & 0.65 &	1 &	\textbf{0.64} & 0.54 & 0.10 & 0.10 & 0.12 & 0.12 & 0.11\\
  PRE\_4  & 0.50 & 0.52 &	0.64 & 1 & \textbf{0.64} & 0.09	& 0.09 & 0.11	& 0.10 & 0.10\\
  PRE\_5  & 0.55 & 0.51	& 0.54 & 0.64 & 1 &	\textbf{0.09} & 0.08 & 0.11 & 0.11 & 0.10\\
  POST\_1 & 0.11 & 0.10	& 0.10 & 0.09 & 0.09 & 1 & \textbf{0.65} & 0.53	& 0.51 & 0.53\\
  POST\_2 & 0.11 & 0.12	& 0.10 & 0.09 & 0.08 & 0.65 & 1 & \textbf{0.65} & 0.52 & 0.52\\
  POST\_3 & 0.12 & 0.11	& 0.12 & 0.11 & 0.11 & 0.53 & 0.65 & 1 & \textbf{0.63} & 0.52\\
  POST\_4 & 0.12 & 0.12	& 0.12 & 0.10 & 0.11 & 0.51 & 0.52 & 0.63 & 1 & \textbf{0.65}\\
  POST\_5 & 0.12 & 0.12	& 0.11 & 0.10 & 0.10 & 0.53 & 0.52 & 0.52 & 0.65 & 1\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}

<!------------------------------------------------------------------------------------------------------------->
### EMA Scenarios {#rel-corrmats-ema}

<!--
```{r results="asis", echo=FALSE, eval=FALSE}
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/EMA_5.5_KorMat.RData")
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/EMA_30.30_KorMat.RData")
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/EMA_5.5_Window_KorMat.RData")
load("C:/Users/steph/OneDrive/Desktop/MA_Clinical_Significance_and_Reliable_Change/MA_Clinical_Significance_and_Reliable_Change/inst/rmarkdown/templates/thesis/skeleton/data/EMA_5.5_Days_KorMat.RData")

EMA_5.5_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold EMA Scenario",
    label = "cor-ema-55",
    #longtable = TRUE,
    placement = "htb")

PP_30.30_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 30-Fold EMA Scenario",
    label = "cor-ema-3030",
    #longtable = TRUE,
    placement = "htb")

EMA_5.5_Window_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Window EMA Scenario",
    label = "cor-ema-55-win",
    #longtable = TRUE,
    placement = "htb")

EMA_5.5_Days_KorMat %>% 
  apa_table(
    caption = "Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Days EMA Scenario",
    label = "cor-ema-55-days",
    #longtable = TRUE,
    placement = "htb")
```
-->

\begin{table}[H]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Window EMA Scenario}}
  \label{tab:cor-ema-55-win}
  \tiny
  \begin{tabular}{@{}ccccccccccc@{}}
  \toprule
  & \multicolumn{1}{c}{PRE\_1} & \multicolumn{1}{c}{PRE\_2} & \multicolumn{1}{c}{PRE\_3} & \multicolumn{1}{c}{PRE\_4} & \multicolumn{1}{c}{PRE\_5} & \multicolumn{1}{c}{POST\_1} & \multicolumn{1}{c}{POST\_2} & \multicolumn{1}{c}{POST\_3} & \multicolumn{1}{c}{POST\_4} & \multicolumn{1}{c}{POST\_5}\\
  \midrule
  PRE\_1 & 1.00 & \textbf{0.32} & 0.18 & 0.24 & 0.30 & 0.02 & 0.00 & 0.02 & 0.03 & 0.01\\
  PRE\_2 & 0.32 & 1.00 & \textbf{0.30} & 0.19 & 0.22 & 0.03 & 0.02 & 0.01 & 0.02 & 0.02\\
  PRE\_3 & 0.18 & 0.30 & 1.00 & \textbf{0.31} & 0.17 & 0.02 & 0.02 & 0.02 & 0.02 & 0.02\\
  PRE\_4 & 0.24 & 0.19 & 0.31 & 1.00 & \textbf{0.31} & 0.01 & 0.03 & 0.03 & 0.01 & 0.02\\
  PRE\_5 & 0.30 & 0.22 & 0.17 & 0.31 & 1.00 & \textbf{0.03} & 0.01 & 0.02 & 0.01 & 0.02\\
  POST\_1 & 0.02 & 0.03 & 0.02 & 0.01 & 0.03 & 1.00 & \textbf{0.30} & 0.19 & 0.21 & 0.26\\
  POST\_2 & 0.00 & 0.02 & 0.02 & 0.03 & 0.01 & 0.30 & 1.00 & \textbf{0.30} & 0.19 & 0.19\\
  POST\_3 & 0.02 & 0.01 & 0.02 & 0.03 & 0.02 & 0.19 & 0.30 & 1.00 & \textbf{0.31} & 0.17\\
  POST\_4 & 0.03 & 0.02 & 0.02 & 0.01 & 0.01 & 0.21 & 0.19 & 0.31 & 1.00 & \textbf{0.32}\\
  POST\_5 & 0.01 & 0.02 & 0.02 & 0.02 & 0.02 & 0.26 & 0.19 & 0.17 & 0.32 & 1.00\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}

\begin{table}[H]
\vspace*{1.5em}
\begin{threeparttable}
  \caption{\textit{Matrix of Correlations Between Single Pre- and Post-Assessments in the 5-Fold Random-Days EMA Scenario}}
  \label{tab:cor-ema-55-days}
  \tiny
  \begin{tabular}{@{}ccccccccccc@{}}
  \toprule
  & \multicolumn{1}{c}{PRE\_1} & \multicolumn{1}{c}{PRE\_2} & \multicolumn{1}{c}{PRE\_3} & \multicolumn{1}{c}{PRE\_4} & \multicolumn{1}{c}{PRE\_5} & \multicolumn{1}{c}{POST\_1} & \multicolumn{1}{c}{POST\_2} & \multicolumn{1}{c}{POST\_3} & \multicolumn{1}{c}{POST\_4} & \multicolumn{1}{c}{POST\_5}\\
  \midrule
  PRE\_1 & 1.00 & \textbf{0.28} & 0.34 & 0.36 & 0.33 & 0.00 & 0.02 & 0.01 & 0.00 & 0.02\\
  PRE\_2 & 0.28 & 1.00 & \textbf{0.30} & 0.35 & 0.35 & 0.00 & 0.02 & 0.01 & 0.00 & 0.02\\
  PRE\_3 & 0.34 & 0.30 & 1.00 & \textbf{0.31} & 0.36 & 0.00 & 0.02 & 0.01 & 0.01 & 0.02\\
  PRE\_4 & 0.36 & 0.35 & 0.31 & 1.00 & \textbf{0.28} & -0.01 & 0.01 & 0.00 & -0.02 & -0.01\\
  PRE\_5 & 0.33 & 0.35 & 0.36 & 0.28 & 1.00 & \textbf{0.01} & 0.01 & 0.02 & 0.02 & 0.01\\
  POST\_1 & 0.00 & 0.00 & 0.00 & -0.01 & 0.01 & 1.00 & \textbf{0.28} & 0.34 & 0.35 & 0.33\\
  POST\_2 & 0.02 & 0.02 & 0.02 & 0.01 & 0.01 & 0.28 & 1.00 & \textbf{0.30} & 0.34 & 0.32\\
  POST\_3 & 0.01 & 0.01 & 0.01 & 0.00 & 0.02 & 0.34 & 0.30 & 1.00 & \textbf{0.31} & 0.32\\
  POST\_4 & 0.00 & 0.00 & 0.01 & -0.02 & 0.02 & 0.35 & 0.34 & 0.31 & 1.00 & \textbf{0.29}\\
  POST\_5 & 0.02 & 0.02 & 0.02 & -0.01 & 0.01 & 0.33 & 0.32 & 0.32 & 0.29 & 1.00\\
  \bottomrule
  \end{tabular}
\end{threeparttable}
\end{table}

\vspace*{\fill}

<!------------------------------------------------------------------------------------------------------------->
## Appendix B: Data Pre-Processing {#pre-pro}

The data sets originally generated for the study of @Schuster.2020 were further prepared to be apt for the particular questions of the present study. The strategy and process will be thoroughly described step by step in the following subsections, satisfying the required level of transparency needed for future reproduction and replication attempts. In the same regard, the R code utilized for these crucial steps of data preparation is included in Appendix \@ref(code)).

<!------------------------------------------------------------------------------------------------------------->
### Extension of Individual Assessments

#### K-Nearest-Neighbor Search

In order to investigate the sensitivity and specificity of estimates obtained through single and short-interval assessment formats in comparison to each subject´s respective _true symptom levels_ -- defined by the score fluctuation in their underlying structure of daily assessments -- it was necessary to extend the originally simulated assessment intervals. As both the questionnaire and EMA scenarios were first modeled for 5-fold intervals, they were extended for further analyses to obtain 30-fold pre- and post assessment intervals. This was achieved with the following approach.
\par
In both simulated data sets comprising _N_ = 100.000 participants each, subjects with equal interval means and standard deviations were matched using a k-nearest-neighbor (KNN) search algorithm. In particular, this was done using the k-dimensional tree algorithm within the function `get.knn()` from the R package _FNN_ [@Beygelzimer.2019]. This KNN-search method compares all cases to one another on one or more dimensions of interest by computing the Euclidian distances between them. For instance, to compare two participants $p = (p_{1},p_{2})$ and $q = (q_{1},q_{2})$ regarding the symptom severity and variability within their baseline assessments, with $p_{1}$ and $q_{1}$ denoting the mean scores and $p_{2}$ and $q_{2}$ denoting the standard deviations of their respective baseline intervals, the Euclidian distance _d_ between them is given by Equation \@ref(eq:eucl-dist):

\begin{equation}
d(p,q) = \sqrt{(q_{1} - p_{1})^2 + (q_{2} - p_{2})^2} (\#eq:eucl-dist)
\end{equation}

Cases were matched separately by pre- and post-treatment intervals to ensure an appropriate balance between (1) within-interval similarity and (2) individual between-interval changes:

1. within-interval similarity between matched cases: cases need to have an exactly equal mean score and fluctuation within the respective interval
2. individual between-interval changes: intervals are matched and concatenated separately: matching pre-treatment intervals are concatenated case-wise and matching post-treatment intervals are concatenated case-wise in another step.

\par \noindent
By specifying the KNN-search function with _k_ = 5, it calculates the similarity of all cases to each other and matches each case with its 5 nearest neighbors, resulting in lists of 6 matched case IDs for each specific (observed) combination of interval means and standard deviations. In this way, cases with similar average symptom scores and similar pre- and post standard deviations were matched inside each data set (questionnaire and EMA). Thereby, only participants with both similar average score changes from pre to post and similar intra-individual variability were matched together.

<!------------------------------------------------------------------------------------------------------------->
#### Generation of 30-fold Individual Assessment Intervals

The individual assessment intervals of these similar cases were then concatenated after one another in order to extend the number of simulated assessments from 5-fold to 30-fold intervals for each participant. In detail, for each combination of 6 perfect neighbors regarding the pre-treatment interval, the IDs of these neighbors were used to bind their 5-fold pre-treatment intervals together to obtain a table of cases with 30-fold pre-treatment intervals. Within this data set of matched pre-case IDs, cases were first sorted within each set of 6 matched IDs (i.e., within rows), then sorted by rows (i.e., by the lowest ID in each row), and then filtered to contain only unique combinations of matched case IDs. This was also done for all cases that were matched by their post-treatment intervals.
\par
Finally, pre- and post-KNN lists were joined by the first, and therefore lowest, ID in each case row. Hence, the number of cases was further filtered to comprise only cases which contained both 6-fold pre- and 6-fold post-case IDs, i.e. only cases with 6 pre-nearest neighbors and 6 post-nearest neighbors, which could be linked together by their lowest ID. Using this KNN-search information, the final 30-fold assessment intervals were created by concatenating the assessments of matched cases from the originally simulated questionnaire- and EMA-like data sets. Both the questionnaire-like and EMA-like samples were reduced by the extension process by about 92 %, resulting in sample sizes of _N_ = 8.240 (questionnaire) and _N_ = 8.087 (EMA). R code for this procedure is provided in the Appendices \@ref(r-knn-search) and \@ref(r-extension).
\par
It should be noted that this strategy to extend assessment intervals, i.e. by stringing together 5-fold intervals from multiple different cases, was only considered appropriate because the originally simulated data presented no signs of autoregressive effects within individual intervals, i.e. neither systematic longitudinal effects between consecutive assessments (i.e., overall improvement or deterioration of symptoms within an interval) nor systematic variability (i.e., regression towards the mean or regression towards the tail). These assumptions can be confirmed, for instance, from the correlation matrices given in Appendix \@ref(app-corrmats).

<!------------------------------------------------------------------------------------------------------------->
### Random Sampling of Assessments From the Intense-Assessment Intervals

In order to realistically simulate drawing arbitrary 5-fold (EMA-like) samples of assessments from each subject´s 30-fold intervals, the following approach was taken within both questionnaire and EMA data sets (see the R code in Appendix \@ref(r-random-sampling)).
\par
For each subject individually, 5-fold windows of pre-treatment and post-treatment assessments were randomly drawn from their respective 30-fold intervals in order to create the scenarios PP\textsubscript{5.5-Window} and EMA\textsubscript{5.5-Window}. This scenario simulates a study design in which participants are monitored via questionnaires or EMA on 5 consecutive days before and after receiving a treatment. Furthermore, only within EMA data, for each subject individually, 5 single pre-treatment and post-treatment assessments were randomly drawn from their respective 30-fold intervals in order to create the scenario EMA\textsubscript{5.5-Days}. This scenario simulates a study design in which participants are monitored via EMA on 5 arbitrary and not necessarily consecutive days before and after receiving a treatment. This was included to analyze potential systematic differences between implementing a daily vs. a non-daily EMA routine.

<!------------------------------------------------------------------------------------------------------------->
### Exclusion of Cases Without Variance

A small number of cases with no symptom variability (i.e. with perfectly constant scores) throughout one or both of their assessment intervals were excluded from all analyses. This criterion for exclusion was formulated because it was deemed improbable for participants to show no fluctuation in PHQ-9 scores over 5-fold or, even more improbable, over 30-fold assessments. Including these cases would also have affected the outcome of the RCI\textsubscript{ind, pre SD}, which incorporates individual standard deviations as estimates of within-subject fluctuations Calculating this estimate of reliable change (see Equation \@ref(eq:rci-ind-presd-se)) with an individual interval standard deviation of 0 would result in an infinite value for the CSI\textsubscript{RCI\textsubscript{ind,pre-SD}}.
\par
The exclusion of these cases was the last step of pre-processing and resulted in the final structure of data sets, as displayed in Table \@ref(tab:data-structure), with the following sample sizes: Among __treatment condition__ trials, _n_ = 60 cases were excluded from questionnaire scenarios and _n_ = 47 cases were excluded from EMA scenarios, resulting in final samples comprising _N_ = 8.180 participants with questionnaire assessments and _N_ = 8.040 participants with EMA assessments. Among __no-treatment condition__ trials, _n_ = 190 cases were excluded from questionnaire scenarios and _n_ = 36 cases were excluded from EMA scenarios, resulting in final samples comprising _N_ = 99.810 participants with questionnaire assessments and _N_ = 99.964 participants with EMA assessments. No-treatment scenarios had larger final sample sizes than treatment scenarios, as the pre-processing steps described above (knn-search, interval extension, and random sampling of assessments) were not applied on them. Within those scenarios, analyses of false-positive rates and specificity levels were only conducted in 5-fold and single-assessment frequencies, therefore not requiring the generation of 30-fold assessment intervals.


<!------------------------------------------------------------------------------------------------------------->
## Appendix C: Distributions of Individual Symptom Changes {#distr-plots}

<!------------------------------------------------------------------------------------------------------------->
### Questionnaire Scenarios {#pp-distr-plot}

Figure \@ref(fig:k20-pp-55-3030-11-pre-post-plot) gives an complete overview over within-subjects treatment effects observed in the three questionnaire scenarios, with individual score changes depicted by thin gray lines, the overall average pre-post effect given by the bold black line, and pre- and post-treatment score distributions depicted as density and box plots. The top-left plot in Figure \@ref(fig:k20-pp-55-3030-11-pre-post-plot) displays individual changes for participants in the 5-fold questionnaire scenario between pre- and post-treatment intervals.^[Repeated-measures box- and violin plots were created following an open-visualizations tutorial available on \url{https://jorvlan.github.io/publications/repmes_tutorial_R.pdf}, which is now also available in the R package \textit{raincloudplots}.] The top-right plot in Figure \@ref(fig:k20-pp-55-3030-11-pre-post-plot) displays individual changes for participants in the 30-fold questionnaire scenario between pre- and post-treatment intervals. The bottom-left plot in Figure \@ref(fig:k20-pp-55-3030-11-pre-post-plot) displays individual changes for participants in the single-assessment questionnaire scenario between their pre- and post-treatment assessments.

\begin{figure}[H]
\caption{\textit{PHQ-9 Score Distributions of (1) 5-Fold and (2) 30-Fold Individual Pre- and Post-Treatment Interval Mean Scores and (3) Single Individual Pre- and Post-Treatment Scores in a Simulated Standard-Questionnaire Scenario}}\label{fig:k20-pp-55-3030-11-pre-post-plot}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_PP_5.5_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_PP_30.30_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_PP_1.1_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\end{figure}

<!------------------------------------------------------------------------------------------------------------->
### EMA Scenarios {#ema-distr-plot}

Figure \@ref(fig:k20-ema-3030-55w-55d-pre-post-plot) gives an overview over within-subjects treatment effects observed in the three EMA scenarios, with individual score changes depicted by thin gray lines, the overall average pre-post effect given by the bold black line, and pre- and post-treatment score distributions depicted as density and box plots. The top-left plot in Figure \@ref(fig:k20-ema-3030-55w-55d-pre-post-plot) displays individual changes for participants in the 30-fold EMA scenario between pre- and post-treatment intervals. The top-right plot in Figure \@ref(fig:k20-ema-3030-55w-55d-pre-post-plot) displays individual changes for participants in the 5-fold Random Window EMA scenario between pre- and post-treatment intervals. The bottom-left plot in Figure \@ref(fig:k20-ema-3030-55w-55d-pre-post-plot) displays individual changes for participants in the 5-fold Random Days EMA scenario between pre- and post-treatment intervals.

\begin{figure}[H]
\caption{\textit{Individual Mean Differences in PHQ-9 Scores Between (1) 30-Fold, (2) 5-Fold Random Window, and (3) 5-Fold Random Days Pre-Treatment and Post-Treatment Intervals in a Simulated EMA Scenario}}\label{fig:k20-ema-3030-55w-55d-pre-post-plot}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_EMA_30.30_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_EMA_5.5_Window_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\includegraphics[width=0.5\linewidth]{data/Time Series Dataframes/k20_EMA_5.5_Days_Pre-Post_Box_Violin_Mean+CI} \hfill{}
\end{figure}


<!------------------------------------------------------------------------------------------------------------->
## Appendix D: R Code {#code}

The third appendix includes information about the R version and packages that were used to prepare and process data, as well as R code for the most important pre-processing steps and the computation of clinical change methods.

<!------------------------------------------------------------------------------------------------------------->
### R Session Information and Used Packages {#session-info}

```{r label="session-info", results="asis", echo=TRUE}
toLatex(sessionInfo())
```

<!------------------------------------------------------------------------------------------------------------->
### K-Nearest-Neighbor Search {#r-knn-search}

K-Nearest-Neighbor Search (using `get.knn()` from the package _FNN_) for the questionnaire data set $PP_{5.5}$ as an example (similar procedure for both the EMA and the questionnaire data set).

```{r label="r-knn-search", eval=FALSE}
pacman::p_load(dplyr, FNN)

# opening the originally simulated data set (N = 100.000) and 
# calculating interval means and standard deviations
PP_5.5 = read.delim("cor_07_k20/cor_07_dataset_k20.txt", 
                    row.names=NULL) %>%
  select(PRE1_1:POST1_5) %>%
  add_column(., .before = "PRE1_1", ID = 1:nrow(.)) %>%
  as_tibble()

pre_5mzp = c("PRE1_1","PRE1_2","PRE1_3","PRE1_4","PRE1_5")
post_5mzp = c("POST1_1","POST1_2","POST1_3","POST1_4","POST1_5")

PP_5.5$PRE_Mean = apply(PP_5.5[pre_5mzp], 1, mean)
PP_5.5$POST_Mean = apply(PP_5.5[post_5mzp], 1, mean)
PP_5.5$MeanDiff = PP_5.5$PRE_Mean - PP_5.5$POST_Mean
PP_5.5$ind.pretestSD = apply(PP_5.5[pre_5mzp], 1, sd)
PP_5.5$ind.posttestSD = apply(PP_5.5[post_5mzp], 1, sd)
save(PP_5.5, file = "cor_07_k20/PP_5.5.RData")

# PRE interval: finding the k=5 nearest neighbors regarding their 
# mean score and standard deviation (with distance == 0)
pre_data = PP_5.5 %>% select(PRE_Mean, ind.pretestSD)
PP_PRE_KNN_df = FNN::get.knn(pre_data, k=5, algorithm = "kd_tree")

x = as_tibble(PP_PRE_KNN_df[[1]], .name_repair = "minimal")
colnames(x) = c("neighbor1", "neighbor2", "neighbor3", 
                "neighbor4", "neighbor5")
y = as_tibble(PP_PRE_KNN_df[[2]], .name_repair = "minimal")
colnames(y) = c("distance1", "distance2", "distance3", 
                "distance4", "distance5")

PP_PRE_KNN_df = bind_cols(x, y) %>%
  add_column(., .before = "neighbor1", ID = 1:nrow(.)) %>%
  filter(distance1 == 0 & distance2 == 0 & distance3 == 0 & 
           distance4 == 0 & distance5 == 0)

# POST interval: finding the k=5 nearest neighbors regarding their 
# mean score and standard deviation (with distance == 0)
post_data = PP_5.5 %>% select(POST_Mean, ind.posttestSD)
PP_POST_KNN_df = FNN::get.knn(post_data, k=5, algorithm = "kd_tree")

x = as_tibble(PP_POST_KNN_df[[1]], .name_repair = "minimal")
colnames(x) = c("neighbor1", "neighbor2", "neighbor3", 
                "neighbor4", "neighbor5")
y = as_tibble(PP_POST_KNN_df[[2]], .name_repair = "minimal")
colnames(y) = c("distance1", "distance2", "distance3", 
                "distance4", "distance5")

PP_POST_KNN_df = bind_cols(x, y) %>%
  add_column(., .before = "neighbor1", ID = 1:nrow(.)) %>%
  filter(distance1 == 0 & distance2 == 0 & distance3 == 0 & 
           distance4 == 0 & distance5 == 0)

# filtering the resulting knn combinations to keep only unique 
# rows of 6 perfectly matching neighbors
# PRE interval
PP_PRE_KNN_df = PP_PRE_KNN_df %>%
  select(ID, neighbor1, neighbor2, neighbor3, neighbor4, neighbor5) %>%
  apply(., 1, sort) %>%
  t() %>%
  as_tibble() %>%
  arrange(., V1, V2, V3, V4, V5, V6) %>%
  distinct() %>%
  filter(V1 != V2 & V2 != V3 & V3 != V4 & V4 != V5 & V5 != V6) %>%
  group_by(V1) %>%
  filter(row_number() == 1) %>%
  ungroup()

colnames(PP_PRE_KNN_df) = c("ID1_PRE", "ID2_PRE", "ID3_PRE", 
                            "ID4_PRE", "ID5_PRE", "ID6_PRE")

# POST interval
PP_POST_KNN_df = PP_POST_KNN_df %>%
  select(ID, neighbor1, neighbor2, neighbor3, neighbor4, neighbor5) %>%
  apply(., 1, sort) %>%
  t() %>%
  as_tibble() %>%
  arrange(., V1, V2, V3, V4, V5, V6) %>%
  distinct() %>%
  filter(V1 != V2 & V2 != V3 & V3 != V4 & V4 != V5 & V5 != V6) %>%
  group_by(V1) %>%
  filter(row_number() == 1) %>%
  ungroup()

colnames(PP_POST_KNN_df) = c("ID1_POST", "ID2_POST", "ID3_POST", 
                             "ID4_POST", "ID5_POST", "ID6_POST")

# joining the matched IDs of pre- and post-neighbors in a data frame
PP_KNNs = inner_join(PP_PRE_KNN_df, PP_POST_KNN_df, 
              by = c("ID1_PRE" = "ID1_POST"))
PP_KNNs = PP_KNNs %>%
  add_column(., .before = "ID2_POST", ID1_POST = PP_KNNs$ID1_PRE)
save(PP_KNNs, file = "cor_07_k20/PP_KNNs.RData")
```

<!------------------------------------------------------------------------------------------------------------->
### Extension of Assessment Intervals {#r-extension}

Extension of assessment intervals for the questionnaire data set $PP_{5.5}$ as an example (similar procedure for both the EMA and the questionnaire data set).

```{r label="r-extension", eval=FALSE}
pacman::p_load(dplyr)
load("cor_07_k20/PP_KNNs.RData")
load("cor_07_k20/PP_5.5.RData")
PP_KNNs = PP_KNNs %>% as.data.frame()

PP_30.30 = data.frame(
  ID1_PRE = c(), ID2_PRE = c(), ID3_PRE = c(), 
  ID4_PRE = c(), ID5_PRE = c(), ID6_PRE = c(),
  ID1_POST = c(), ID2_POST = c(), ID3_POST = c(), 
  ID4_POST = c(), ID5_POST = c(), ID6_POST = c(),

  PRE1_1 = c(), PRE1_2 = c(), PRE1_3 = c(), PRE1_4 = c(), 
  PRE1_5 = c(), PRE1_6 = c(), PRE1_7 = c(), PRE1_8 = c(), 
  PRE1_9 = c(), PRE1_10 = c(), PRE1_11 = c(), PRE1_12 = c(), 
  PRE1_13 = c(), PRE1_14 = c(), PRE1_15 = c(), PRE1_16 = c(), 
  PRE1_17 = c(), PRE1_18 = c(), PRE1_19 = c(), PRE1_20 = c(),
  PRE1_21 = c(), PRE1_22 = c(), PRE1_23 = c(), PRE1_24 = c(), 
  PRE1_25 = c(), PRE1_26 = c(), PRE1_27 = c(), PRE1_28 = c(), 
  PRE1_29 = c(), PRE1_30 = c(),

  POST1_1 = c(), POST1_2 = c(), POST1_3 = c(), POST1_4 = c(), 
  POST1_5 = c(), POST1_6 = c(), POST1_7 = c(), POST1_8 = c(), 
  POST1_9 = c(), POST1_10 = c(), POST1_11 = c(), POST1_12 = c(), 
  POST1_13 = c(), POST1_14 = c(), POST1_15 = c(), POST1_16 = c(), 
  POST1_17 = c(), POST1_18 = c(), POST1_19 = c(), POST1_20 = c(),
  POST1_21 = c(), POST1_22 = c(), POST1_23 = c(), POST1_24 = c(), 
  POST1_25 = c(), POST1_26 = c(), POST1_27 = c(), POST1_28 = c(), 
  POST1_29 = c(), POST1_30 = c())

for (i in 1:length(PP_KNNs$ID1_PRE)) {
  PP_30.30[i,"ID1_PRE"] = PP_KNNs[i,"ID1_PRE"]
  PP_30.30[i,"ID2_PRE"] = PP_KNNs[i,"ID2_PRE"]
  PP_30.30[i,"ID3_PRE"] = PP_KNNs[i,"ID3_PRE"]
  PP_30.30[i,"ID4_PRE"] = PP_KNNs[i,"ID4_PRE"]
  PP_30.30[i,"ID5_PRE"] = PP_KNNs[i,"ID5_PRE"]
  PP_30.30[i,"ID6_PRE"] = PP_KNNs[i,"ID6_PRE"]
  PP_30.30[i,"ID1_POST"] = PP_KNNs[i,"ID1_POST"]
  PP_30.30[i,"ID2_POST"] = PP_KNNs[i,"ID2_POST"]
  PP_30.30[i,"ID3_POST"] = PP_KNNs[i,"ID3_POST"]
  PP_30.30[i,"ID4_POST"] = PP_KNNs[i,"ID4_POST"]
  PP_30.30[i,"ID5_POST"] = PP_KNNs[i,"ID5_POST"]
  PP_30.30[i,"ID6_POST"] = PP_KNNs[i,"ID6_POST"]

  PP_30.30[i,"PRE1_1"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_2"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_3"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_4"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_5"] = PP_5.5[PP_KNNs[i,"ID1_PRE"],"PRE1_5"]
  PP_30.30[i,"PRE1_6"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_7"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_8"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_9"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_10"] = PP_5.5[PP_KNNs[i,"ID2_PRE"],"PRE1_5"]

  PP_30.30[i,"PRE1_11"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_12"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_13"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_14"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_15"] = PP_5.5[PP_KNNs[i,"ID3_PRE"],"PRE1_5"]
  PP_30.30[i,"PRE1_16"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_17"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_18"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_19"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_20"] = PP_5.5[PP_KNNs[i,"ID4_PRE"],"PRE1_5"]

  PP_30.30[i,"PRE1_21"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_22"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_23"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_24"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_25"] = PP_5.5[PP_KNNs[i,"ID5_PRE"],"PRE1_5"]
  PP_30.30[i,"PRE1_26"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_1"]
  PP_30.30[i,"PRE1_27"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_2"]
  PP_30.30[i,"PRE1_28"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_3"]
  PP_30.30[i,"PRE1_29"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_4"]
  PP_30.30[i,"PRE1_30"] = PP_5.5[PP_KNNs[i,"ID6_PRE"],"PRE1_5"]


  PP_30.30[i,"POST1_1"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_1"]
  PP_30.30[i,"POST1_2"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_2"]
  PP_30.30[i,"POST1_3"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_3"]
  PP_30.30[i,"POST1_4"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_4"]
  PP_30.30[i,"POST1_5"] = PP_5.5[PP_KNNs[i,"ID1_POST"],"POST1_5"]
  PP_30.30[i,"POST1_6"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_1"]
  PP_30.30[i,"POST1_7"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_2"]
  PP_30.30[i,"POST1_8"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_3"]
  PP_30.30[i,"POST1_9"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_4"]
  PP_30.30[i,"POST1_10"] = PP_5.5[PP_KNNs[i,"ID2_POST"],"POST1_5"]

  PP_30.30[i,"POST1_11"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_1"]
  PP_30.30[i,"POST1_12"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_2"]
  PP_30.30[i,"POST1_13"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_3"]
  PP_30.30[i,"POST1_14"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_4"]
  PP_30.30[i,"POST1_15"] = PP_5.5[PP_KNNs[i,"ID3_POST"],"POST1_5"]
  PP_30.30[i,"POST1_16"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_1"]
  PP_30.30[i,"POST1_17"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_2"]
  PP_30.30[i,"POST1_18"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_3"]
  PP_30.30[i,"POST1_19"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_4"]
  PP_30.30[i,"POST1_20"] = PP_5.5[PP_KNNs[i,"ID4_POST"],"POST1_5"]

  PP_30.30[i,"POST1_21"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_1"]
  PP_30.30[i,"POST1_22"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_2"]
  PP_30.30[i,"POST1_23"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_3"]
  PP_30.30[i,"POST1_24"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_4"]
  PP_30.30[i,"POST1_25"] = PP_5.5[PP_KNNs[i,"ID5_POST"],"POST1_5"]
  PP_30.30[i,"POST1_26"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_1"]
  PP_30.30[i,"POST1_27"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_2"]
  PP_30.30[i,"POST1_28"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_3"]
  PP_30.30[i,"POST1_29"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_4"]
  PP_30.30[i,"POST1_30"] = PP_5.5[PP_KNNs[i,"ID6_POST"],"POST1_5"]}
```

<!------------------------------------------------------------------------------------------------------------->
### Random Sampling of 5-fold EMA Windows and Days {#r-random-sampling}

Random sampling of 5-fold EMA assessments from 30-fold intervals for the generation of individual (1) 5-fold windows (similar process for the 5-fold Random-Window standard-questionnaire data set) and (2) 5-fold single assessment days.

```{r label="r-random-sampling", eval=FALSE}
pacman::p_load(dplyr)
set.seed(42)

pre_5mzp = c("PRE1_1","PRE1_2","PRE1_3","PRE1_4","PRE1_5")
post_5mzp = c("POST1_1","POST1_2","POST1_3","POST1_4","POST1_5")

pre_30mzp = c("PRE1_1","PRE1_2","PRE1_3","PRE1_4","PRE1_5",
            "PRE1_6","PRE1_7","PRE1_8","PRE1_9","PRE1_10",
            "PRE1_11","PRE1_12","PRE1_13","PRE1_14","PRE1_15",
            "PRE1_16","PRE1_17","PRE1_18","PRE1_19","PRE1_20",
            "PRE1_21","PRE1_22","PRE1_23","PRE1_24","PRE1_25",
            "PRE1_26","PRE1_27","PRE1_28","PRE1_29","PRE1_30")
post_30mzp = c("POST1_1","POST1_2","POST1_3","POST1_4","POST1_5",
             "POST1_6","POST1_7","POST1_8","POST1_9","POST1_10",
             "POST1_11","POST1_12","POST1_13","POST1_14","POST1_15",
             "POST1_16","POST1_17","POST1_18","POST1_19","POST1_20",
             "POST1_21","POST1_22","POST1_23","POST1_24","POST1_25",
             "POST1_26","POST1_27","POST1_28","POST1_29","POST1_30")

# (1) 5-fold windows (EMA_5.5_Window)
# random sampling of 5-fold pre- and post-assessment windows (5 days 
# in a row) from individual 30-fold intervals (EMA_30.30)
EMA_5.5_Window = data.frame(ID = c(), 
  Pre_MZP1 = c(), Pre_MZP2 = c(), Pre_MZP3 = c(), Pre_MZP4 = c(),
  Pre_MZP5 = c(), Post_MZP1 = c(), Post_MZP2 = c(), Post_MZP3 = c(),
  Post_MZP4 = c(), Post_MZP5 = c(), PRE1_1 = c(), PRE1_2 = c(),
  PRE1_3 = c(), PRE1_4 = c(), PRE1_5 = c(), POST1_1 = c(),
  POST1_2 = c(), POST1_3 = c(), POST1_4 = c(), POST1_5 = c())

for (i in EMA_30.30$ID) {
  a = sample(1:26, 1)
  EMA_5.5_pre_Window = pre_30mzp[seq(from = a, to = a+4)]
  b = sample(1:26, 1)
  EMA_5.5_post_Window = post_30mzp[seq(from = b, to = b+4)]
  
  EMA_5.5_Window[i,"ID"] = i
  EMA_5.5_Window[i,"Pre_MZP1"] = EMA_5.5_pre_Window[1]
  EMA_5.5_Window[i,"Pre_MZP2"] = EMA_5.5_pre_Window[2]
  EMA_5.5_Window[i,"Pre_MZP3"] = EMA_5.5_pre_Window[3]
  EMA_5.5_Window[i,"Pre_MZP4"] = EMA_5.5_pre_Window[4]
  EMA_5.5_Window[i,"Pre_MZP5"] = EMA_5.5_pre_Window[5]
  EMA_5.5_Window[i,"Post_MZP1"] = EMA_5.5_post_Window[1]
  EMA_5.5_Window[i,"Post_MZP2"] = EMA_5.5_post_Window[2]
  EMA_5.5_Window[i,"Post_MZP3"] = EMA_5.5_post_Window[3]
  EMA_5.5_Window[i,"Post_MZP4"] = EMA_5.5_post_Window[4]
  EMA_5.5_Window[i,"Post_MZP5"] = EMA_5.5_post_Window[5]
  
  EMA_5.5_Window[i,"PRE1_1"] = EMA_30.30[i,EMA_5.5_pre_Window[1]]
  EMA_5.5_Window[i,"PRE1_2"] = EMA_30.30[i,EMA_5.5_pre_Window[2]]
  EMA_5.5_Window[i,"PRE1_3"] = EMA_30.30[i,EMA_5.5_pre_Window[3]]
  EMA_5.5_Window[i,"PRE1_4"] = EMA_30.30[i,EMA_5.5_pre_Window[4]]
  EMA_5.5_Window[i,"PRE1_5"] = EMA_30.30[i,EMA_5.5_pre_Window[5]]
  EMA_5.5_Window[i,"POST1_1"] = EMA_30.30[i,EMA_5.5_post_Window[1]]
  EMA_5.5_Window[i,"POST1_2"] = EMA_30.30[i,EMA_5.5_post_Window[2]]
  EMA_5.5_Window[i,"POST1_3"] = EMA_30.30[i,EMA_5.5_post_Window[3]]
  EMA_5.5_Window[i,"POST1_4"] = EMA_30.30[i,EMA_5.5_post_Window[4]]
  EMA_5.5_Window[i,"POST1_5"] = EMA_30.30[i,EMA_5.5_post_Window[5]]}

# (2) 5-fold single assessment days (EMA_5.5_Days)
# random sampling of 5-fold pre- and post-assessments (not necessarily
# days in a row) from individual 30-fold intervals (EMA_30.30)
EMA_5.5_Days = data.frame(ID = c(), 
  Pre_MZP1 = c(), Pre_MZP2 = c(), Pre_MZP3 = c(), Pre_MZP4 = c(),
  Pre_MZP5 = c(), Post_MZP1 = c(), Post_MZP2 = c(), Post_MZP3 = c(),
  Post_MZP4 = c(), Post_MZP5 = c(), PRE1_1 = c(), PRE1_2 = c(),
  PRE1_3 = c(), PRE1_4 = c(), PRE1_5 = c(), POST1_1 = c(),
  POST1_2 = c(), POST1_3 = c(), POST1_4 = c(), POST1_5 = c())

for (i in EMA_30.30$ID) {
  EMA_5.5_pre_Days = pre_30mzp[sort(sample(1:30, 5))]
  EMA_5.5_post_Days = post_30mzp[sort(sample(1:30, 5))]
  
  EMA_5.5_Days[i,"ID"] = i
  EMA_5.5_Days[i,"Pre_MZP1"] = EMA_5.5_pre_Days[1]
  EMA_5.5_Days[i,"Pre_MZP2"] = EMA_5.5_pre_Days[2]
  EMA_5.5_Days[i,"Pre_MZP3"] = EMA_5.5_pre_Days[3]
  EMA_5.5_Days[i,"Pre_MZP4"] = EMA_5.5_pre_Days[4]
  EMA_5.5_Days[i,"Pre_MZP5"] = EMA_5.5_pre_Days[5]
  EMA_5.5_Days[i,"Post_MZP1"] = EMA_5.5_post_Days[1]
  EMA_5.5_Days[i,"Post_MZP2"] = EMA_5.5_post_Days[2]
  EMA_5.5_Days[i,"Post_MZP3"] = EMA_5.5_post_Days[3]
  EMA_5.5_Days[i,"Post_MZP4"] = EMA_5.5_post_Days[4]
  EMA_5.5_Days[i,"Post_MZP5"] = EMA_5.5_post_Days[5]
  
  EMA_5.5_Days[i,"PRE1_1"] = EMA_30.30[i,EMA_5.5_pre_Days[1]]
  EMA_5.5_Days[i,"PRE1_2"] = EMA_30.30[i,EMA_5.5_pre_Days[2]]
  EMA_5.5_Days[i,"PRE1_3"] = EMA_30.30[i,EMA_5.5_pre_Days[3]]
  EMA_5.5_Days[i,"PRE1_4"] = EMA_30.30[i,EMA_5.5_pre_Days[4]]
  EMA_5.5_Days[i,"PRE1_5"] = EMA_30.30[i,EMA_5.5_pre_Days[5]]
  EMA_5.5_Days[i,"POST1_1"] = EMA_30.30[i,EMA_5.5_post_Days[1]]
  EMA_5.5_Days[i,"POST1_2"] = EMA_30.30[i,EMA_5.5_post_Days[2]]
  EMA_5.5_Days[i,"POST1_3"] = EMA_30.30[i,EMA_5.5_post_Days[3]]
  EMA_5.5_Days[i,"POST1_4"] = EMA_30.30[i,EMA_5.5_post_Days[4]]
  EMA_5.5_Days[i,"POST1_5"] = EMA_30.30[i,EMA_5.5_post_Days[5]]}
```

<!------------------------------------------------------------------------------------------------------------->
### R Code for the Calculation of Clinical Change Methods

#### Percentage Change {#r-pc}

Calculation of the Percentage Change method PC for interpreting the score difference between two assessment intervals (i.e. Mean Percentage Change), as well as between two single assessments for the questionnaire data set as an example (similar process for both the EMA and the questionnaire data set).

```{r label="r-pc", eval=FALSE}
pacman::p_load(dplyr)

### PP_5.5:
PP_5.5$Mean_PC = (1-(PP_5.5$POST_Mean / PP_5.5$PRE_Mean)) * 100

# creating the interpretation categories for Percentage Change,
# ranging from -2 (strong deterioration) to 2 (strong improvement):
PP_5.5 = PP_5.5 %>% 
  mutate(Mean_PC_klass = case_when(
    Mean_PC <= -50 ~ -2,
    Mean_PC > -50 & Mean_PC <= -25 ~ -1,
    Mean_PC > -25 & Mean_PC < 25 ~ 0,
    Mean_PC >= 25 & Mean_PC < 50 ~ 1,
    Mean_PC >= 50 ~ 2,
    TRUE ~ Mean_PC))

### PP_30.30:
PP_30.30$Mean_PC = (1-(PP_30.30$POST_Mean / PP_30.30$PRE_Mean)) * 100

# creating the interpretation categories for Percentage Change,
# ranging from -2 (strong deterioration) to 2 (strong improvement):
PP_30.30 = PP_30.30 %>% 
  mutate(Mean_PC_klass = case_when(
    Mean_PC <= -50 ~ -2,
    Mean_PC > -50 & Mean_PC <= -25 ~ -1,
    Mean_PC > -25 & Mean_PC < 25 ~ 0,
    Mean_PC >= 25 & Mean_PC < 50 ~ 1,
    Mean_PC >= 50 ~ 2,
    TRUE ~ Mean_PC))

### PP_1.1:
PP_1.1$PC = (1 - (PP_1.1$POST / PP_1.1$PRE)) * 100

# creating the interpretation categories for Percentage Change,
# ranging from -2 (strong deterioration) to 2 (strong improvement):
PP_1.1 = PP_1.1 %>% 
  mutate(PC_klass = case_when(
    PC <= -50 ~ -2,
    PC > -50 & PC <= -25 ~ -1,
    PC > -25 & PC < 25 ~ 0,
    PC >= 25 & PC < 50 ~ 1,
    PC >= 50 ~ 2,
    TRUE ~ as.numeric(PC)))
```

#### Clinical Significance {#r-csi}

Implementation of the Clinical Significance method CSI [see @McMillan.2010] for interpreting the score difference between two assessment intervals, as well as between two single assessments for the questionnaire data set as an example (similar process for both the EMA and the questionnaire data set).

```{r label="r-csi", eval=FALSE}
# creating the interpretation categories for Clinically Sig. Change,
# ranging from -1 (improvement) to 1 (deterioration):
pacman::p_load(dplyr)

### PP_5.5:
PP_5.5 = PP_5.5 %>% 
   mutate(CSI_klass = case_when(
     PRE_Mean >= 10 & POST_Mean <= 9 & Mean_PC >= 50 ~ -1,
     PRE_Mean <= 9 & POST_Mean >= 10 & Mean_PC <= -50 ~ 1,
     TRUE ~ 0))

### PP_30.30:
PP_30.30 = PP_30.30 %>% 
   mutate(CSI_klass = case_when(
     PRE_Mean >= 10 & POST_Mean <= 9 & Mean_PC >= 50 ~ -1,
     PRE_Mean <= 9 & POST_Mean >= 10 & Mean_PC <= -50 ~ 1,
     TRUE ~ 0))

### PP_1.1:
PP_1.1 = PP_1.1 %>% 
   mutate(CSI_klass = case_when(
     PRE >= 10 & POST <= 9 & PC >= 50 ~ -1,
     PRE <= 9 & POST >= 10 & PC <= -50 ~ 1,
     TRUE ~ 0))
```

#### Average Internal Consistency {#r-alpha}

Calculation of the average internal consistency Cronbach´s $\alpha$ as the estimate of reliability to be used to compute Reliable Change Indices. The population´s internal consistency of PHQ-9 assessments was first calculated within each 5-fold interval (pre and post). Then, $\alpha_{pre}$ and $\alpha_{post}$ were Fisher-Z transformed to take the average of both estimates, and finally this value was transformed back to obtain a pooled Cronbach´s $\alpha$. The same calculation method was used for questionnaire and EMA data sets.

```{r label="r-alpha", eval=FALSE}
pacman::p_load(DescTools)
PRE_alpha = CronbachAlpha(PP_5.5[pre_5mzp])
POST_alpha = CronbachAlpha(PP_5.5[post_5mzp])
PP_5.5_Alpha = FisherZInv(mean(c(FisherZ(PRE_alpha), 
                                 FisherZ(POST_alpha))))
```

#### Reliable Change Index [@Jacobson.1984; @Jacobson.1991] {#r-rci-jt}

Calculation of the Reliable Change Index \textit{RCI\textsubscript{JT}} and its population-level significance cutoff sensu @Jacobson.1984 and @Jacobson.1991 for the difference between two single assessments for the questionnaire data set as an example (similar process for both the EMA and the questionnaire data set).

```{r label="r-rci-jt", eval=FALSE}
pacman::p_load(dplyr)

PP_1.1$RCI_JT = (PP_1.1$POST - PP_1.1$PRE) / 
    sqrt(2 * (sd(PP_1.1$PRE) * sqrt(1 - PP_5.5_Alpha)) ^ 2)
RCI_JT_Cutoff = 1.96 * sqrt(2 * (sd(PP_1.1$PRE) * 
    sqrt(1 - PP_5.5_Alpha)) ^ 2)

# creating the interpretation categories for the RCI(JT), ranging 
# from -1 (reliable improvement) to 1 (reliable deterioration):
PP_1.1 = PP_1.1 %>% 
  mutate(RCI_JT_klass = case_when(
    PRE >= 10 & POST <= 9 & RCI_JT < -1.96 ~ -1,
    PRE <= 9 & POST >= 10 & RCI_JT > 1.96 ~ 1,
    TRUE ~ 0))
```

#### Individualized Reliable Change Index (Pre-SD) {#r-rci-ind-pre}

Calculation of a proposed Individualized Reliable Change Index \textit{RCI\textsubscript{ind,pre-SD}} and its corresponding individual significance cutoff for the difference between two assessment intervals, including the subject´s standard deviation from the baseline interval as a measure of individual variability. The same calculation method was used for both questionnaire and EMA data sets.

```{r label="r-rci-ind-pre", eval=FALSE}
pacman::p_load(dplyr)

### PP_5.5:
PP_5.5$SEd_pre = sqrt(2 * (PP_5.5$ind.pretestSD * 
                            sqrt(1 - PP_5.5_Alpha)) ^ 2)
PP_5.5$RCI_ind_preSD = (PP_5.5$POST_Mean - PP_5.5$PRE_Mean) / 
                            PP_5.5$SEd_pre
PP_5.5$RCI_ind_preSD_Cutoff =  1.96 * PP_5.5$SEd_pre

# creating the interpretation categories for the RCI(JT), ranging 
# from -1 (reliable improvement) to 1 (reliable deterioration):
PP_5.5 = PP_5.5 %>% 
  mutate(RCI_ind_preSD_klass = case_when(
    PRE_Mean >= 10 & POST_Mean <= 9 & RCI_ind_preSD < -1.96 ~ -1,
    PRE_Mean <= 9 & POST_Mean >= 10 & RCI_ind_preSD > 1.96 ~ 1,
    TRUE ~ 0))

### PP_30.30:
PP_30.30$SEd_pre = sqrt(2 * (PP_30.30$ind.pretestSD * 
                            sqrt(1 - PP_5.5_Alpha)) ^ 2)
PP_30.30$RCI_ind_preSD = (PP_30.30$POST_Mean - PP_30.30$PRE_Mean) / 
                            PP_30.30$SEd_pre
PP_30.30$RCI_ind_preSD_Cutoff =  1.96 * PP_30.30$SEd_pre

# creating the interpretation categories for the RCI(JT), ranging 
# from -1 (reliable improvement) to 1 (reliable deterioration):
PP_30.30 = PP_30.30 %>% 
  mutate(RCI_ind_preSD_klass = case_when(
    PRE_Mean >= 10 & POST_Mean <= 9 & RCI_ind_preSD < -1.96 ~ -1,
    PRE_Mean <= 9 & POST_Mean >= 10 & RCI_ind_preSD > 1.96 ~ 1,
    TRUE ~ 0))
```
